<p class="caption"><span class="caption-text">First steps</span></p>
<p class="caption"><span class="caption-text">Basic concepts</span></p>
<p class="caption"><span class="caption-text">Built-in services</span></p>
<p class="caption"><span class="caption-text">Solving specific problems</span></p>
<p class="caption"><span class="caption-text">Extending Scrapy</span></p>
<p class="caption"><span class="caption-text">All the rest</span></p>
<p>Scrapy is an application framework for crawling web sites and extracting
structured data which can be used for a wide range of useful applications, like
data mining, information processing or historical archival.</p>
<p>Even though Scrapy was originally designed for <a class="reference external" href="https://en.wikipedia.org/wiki/Web_scraping">web scraping</a>, it can also be
used to extract data using APIs (such as <a class="reference external" href="https://affiliate-program.amazon.com/gp/advertising/api/detail/main.html">Amazon Associates Web Services</a>) or
as a general purpose web crawler.</p>
<p>In order to show you what Scrapy brings to the table, we’ll walk you through an
example of a Scrapy Spider using the simplest way to run a spider.</p>
<p>Here’s the code for a spider that scrapes famous quotes from website
<a class="reference external" href="http://quotes.toscrape.com">http://quotes.toscrape.com</a>, following the pagination:</p>
<p>Put this in a text file, name it to something like <code class="docutils literal"><span class="pre">quotes_spider.py</span></code>
and run the spider using the <a class="reference internal" href="../topics/commands.html#std:command-runspider"><code class="xref std std-command docutils literal"><span class="pre">runspider</span></code></a> command:</p>
<p>When this finishes you will have in the <code class="docutils literal"><span class="pre">quotes.json</span></code> file a list of the
quotes in JSON format, containing text and author, looking like this (reformatted
here for better readability):</p>
<p>When you ran the command <code class="docutils literal"><span class="pre">scrapy</span> <span class="pre">runspider</span> <span class="pre">quotes_spider.py</span></code>, Scrapy looked for a
Spider definition inside it and ran it through its crawler engine.</p>
<p>The crawl started by making requests to the URLs defined in the <code class="docutils literal"><span class="pre">start_urls</span></code>
attribute (in this case, only the URL for quotes in <em>humor</em> category)
and called the default callback method <code class="docutils literal"><span class="pre">parse</span></code>, passing the response object as
an argument. In the <code class="docutils literal"><span class="pre">parse</span></code> callback, we loop through the quote elements
using a CSS Selector, yield a Python dict with the extracted quote text and author,
look for a link to the next page and schedule another request using the same
<code class="docutils literal"><span class="pre">parse</span></code> method as callback.</p>
<p>Here you notice one of the main advantages about Scrapy: requests are
<a class="reference internal" href="../topics/architecture.html#topics-architecture"><span class="std std-ref">scheduled and processed asynchronously</span></a>.  This
means that Scrapy doesn’t need to wait for a request to be finished and
processed, it can send another request or do other things in the meantime. This
also means that other requests can keep going even if some request fails or an
error happens while handling it.</p>
<p>While this enables you to do very fast crawls (sending multiple concurrent
requests at the same time, in a fault-tolerant way) Scrapy also gives you
control over the politeness of the crawl through <a class="reference internal" href="../topics/settings.html#topics-settings-ref"><span class="std std-ref">a few settings</span></a>. You can do things like setting a download delay between
each request, limiting amount of concurrent requests per domain or per IP, and
even <a class="reference internal" href="../topics/autothrottle.html#topics-autothrottle"><span class="std std-ref">using an auto-throttling extension</span></a> that tries
to figure out these automatically.</p>
<p class="first admonition-title">Note</p>
<p class="last">This is using <a class="reference internal" href="../topics/feed-exports.html#topics-feed-exports"><span class="std std-ref">feed exports</span></a> to generate the
JSON file, you can easily change the export format (XML or CSV, for example) or the
storage backend (FTP or <a class="reference external" href="https://aws.amazon.com/s3/">Amazon S3</a>, for example).  You can also write an
<a class="reference internal" href="../topics/item-pipeline.html#topics-item-pipeline"><span class="std std-ref">item pipeline</span></a> to store the items in a database.</p>
<p>You’ve seen how to extract and store items from a website using Scrapy, but
this is just the surface. Scrapy provides a lot of powerful features for making
scraping easy and efficient, such as:</p>
<p>The next steps for you are to <a class="reference internal" href="install.html#intro-install"><span class="std std-ref">install Scrapy</span></a>,
<a class="reference internal" href="tutorial.html#intro-tutorial"><span class="std std-ref">follow through the tutorial</span></a> to learn how to create
a full-blown Scrapy project and <a class="reference external" href="http://scrapy.org/community/">join the community</a>. Thanks for your
interest!</p>
<p>
        © Copyright 2008-2016, Scrapy developers.
      
        <span class="commit">
          Revision <code>5aebdac4</code>.
        </span>
</p>
<p class="caption"><span class="caption-text">First steps</span></p>
<p class="caption"><span class="caption-text">Basic concepts</span></p>
<p class="caption"><span class="caption-text">Built-in services</span></p>
<p class="caption"><span class="caption-text">Solving specific problems</span></p>
<p class="caption"><span class="caption-text">Extending Scrapy</span></p>
<p class="caption"><span class="caption-text">All the rest</span></p>
<p>Scrapy runs on Python 2.7 and Python 3.3 or above.</p>
<p>If you’re using <a class="reference external" href="http://docs.continuum.io/anaconda/index">Anaconda</a> or <a class="reference external" href="http://conda.pydata.org/docs/install/quick.html">Miniconda</a>, you can install the package from
the <a class="reference external" href="https://conda-forge.github.io/">conda-forge</a> channel, which has up-to-date packages for Linux, Windows
and OS X.</p>
<p>To install Scrapy using <code class="docutils literal"><span class="pre">conda</span></code>, run:</p>
<p>Alternatively, if you’re already familiar with installation of Python packages,
you can install Scrapy and its dependencies from PyPI with:</p>
<p>Note that sometimes this may require solving compilation issues for some Scrapy
dependencies depending on your operating system, so be sure to check the
<a class="reference internal" href="#intro-install-platform-notes"><span class="std std-ref">Platform specific installation notes</span></a>.</p>
<p>We strongly recommend that you install Scrapy in <a class="reference internal" href="#intro-using-virtualenv"><span class="std std-ref">a dedicated virtualenv</span></a>,
to avoid conflicting with your system packages.</p>
<p>For more detailed and platform specifics instructions, read on.</p>
<p>Scrapy is written in pure Python and depends on a few key Python packages (among others):</p>
<p>The minimal versions which Scrapy is tested against are:</p>
<p>Scrapy may work with older versions of these packages
but it is not guaranteed it will continue working
because it’s not being tested against them.</p>
<p>Some of these packages themselves depends on non-Python packages
that might require additional installation steps depending on your platform.
Please check <a class="reference internal" href="#intro-install-platform-notes"><span class="std std-ref">platform-specific guides below</span></a>.</p>
<p>In case of any trouble related to these dependencies,
please refer to their respective installation instructions:</p>
<p>TL;DR: We recommend installing Scrapy inside a virtual environment
on all platforms.</p>
<p>Python packages can be installed either globally (a.k.a system wide),
or in user-space. We do not recommend installing scrapy system wide.</p>
<p>Instead, we recommend that you install scrapy within a so-called
“virtual environment” (<a class="reference external" href="https://virtualenv.pypa.io">virtualenv</a>).
Virtualenvs allow you to not conflict with already-installed Python
system packages (which could break some of your system tools and scripts),
and still install packages normally with <code class="docutils literal"><span class="pre">pip</span></code> (without <code class="docutils literal"><span class="pre">sudo</span></code> and the likes).</p>
<p>To get started with virtual environments, see <a class="reference external" href="https://virtualenv.pypa.io/en/stable/installation/">virtualenv installation instructions</a>.
To install it globally (having it globally installed actually helps here),
it should be a matter of running:</p>
<p>Check this <a class="reference external" href="https://virtualenv.pypa.io/en/stable/userguide/">user guide</a> on how to create your virtualenv.</p>
<p class="first admonition-title">Note</p>
<p class="last">If you use Linux or OS X, <a class="reference external" href="https://virtualenvwrapper.readthedocs.io/en/latest/install.html">virtualenvwrapper</a> is a handy tool to create virtualenvs.</p>
<p>Once you have created a virtualenv, you can install scrapy inside it with <code class="docutils literal"><span class="pre">pip</span></code>,
just like any other Python package.
(See <a class="reference internal" href="#intro-install-platform-notes"><span class="std std-ref">platform-specific guides</span></a>
below for non-Python dependencies that you may need to install beforehand).</p>
<p>Python virtualenvs can be created to use Python 2 by default, or Python 3 by default.</p>
<p>Though it’s possible to install Scrapy on Windows using pip, we recommend you
to install <a class="reference external" href="http://docs.continuum.io/anaconda/index">Anaconda</a> or <a class="reference external" href="http://conda.pydata.org/docs/install/quick.html">Miniconda</a> and use the package from the
<a class="reference external" href="https://conda-forge.github.io/">conda-forge</a> channel, which will avoid most installation issues.</p>
<p>Once you’ve installed <a class="reference external" href="http://docs.continuum.io/anaconda/index">Anaconda</a> or <a class="reference external" href="http://conda.pydata.org/docs/install/quick.html">Miniconda</a>, install Scrapy with:</p>
<p>Scrapy is currently tested with recent-enough versions of lxml,
twisted and pyOpenSSL, and is compatible with recent Ubuntu distributions.
But it should support older versions of Ubuntu too, like Ubuntu 12.04,
albeit with potential issues with TLS connections.</p>
<p><strong>Don’t</strong> use the <code class="docutils literal"><span class="pre">python-scrapy</span></code> package provided by Ubuntu, they are
typically too old and slow to catch up with latest Scrapy.</p>
<p>To install scrapy on Ubuntu (or Ubuntu-based) systems, you need to install
these dependencies:</p>
<p>If you want to install scrapy on Python 3, you’ll also need Python 3 development headers:</p>
<p>Inside a <a class="reference internal" href="#intro-using-virtualenv"><span class="std std-ref">virtualenv</span></a>,
you can install Scrapy with <code class="docutils literal"><span class="pre">pip</span></code> after that:</p>
<p class="first admonition-title">Note</p>
<p class="last">The same non-python dependencies can be used to install Scrapy in Debian
Wheezy (7.0) and above.</p>
<p>Building Scrapy’s dependencies requires the presence of a C compiler and
development headers. On OS X this is typically provided by Apple’s Xcode
development tools. To install the Xcode command line tools open a terminal
window and run:</p>
<p>There’s a <a class="reference external" href="https://github.com/pypa/pip/issues/2468">known issue</a> that
prevents <code class="docutils literal"><span class="pre">pip</span></code> from updating system packages. This has to be addressed to
successfully install Scrapy and its dependencies. Here are some proposed
solutions:</p>
<p class="first"><em>(Recommended)</em> <strong>Don’t</strong> use system python, install a new, updated version
that doesn’t conflict with the rest of your system. Here’s how to do it using
the <a class="reference external" href="http://brew.sh/">homebrew</a> package manager:</p>
<p class="first">Install <a class="reference external" href="http://brew.sh/">homebrew</a> following the instructions in <a class="reference external" href="http://brew.sh/">http://brew.sh/</a></p>
<p class="first">Update your <code class="docutils literal"><span class="pre">PATH</span></code> variable to state that homebrew packages should be
used before system packages (Change <code class="docutils literal"><span class="pre">.bashrc</span></code> to <code class="docutils literal"><span class="pre">.zshrc</span></code> accordantly
if you’re using <a class="reference external" href="http://www.zsh.org/">zsh</a> as default shell):</p>
<p class="first">Reload <code class="docutils literal"><span class="pre">.bashrc</span></code> to ensure the changes have taken place:</p>
<p class="first">Install python:</p>
<p class="first">Latest versions of python have <code class="docutils literal"><span class="pre">pip</span></code> bundled with them so you won’t need
to install it separately. If this is not the case, upgrade python:</p>
<p class="first"><em>(Optional)</em> Install Scrapy inside an isolated python environment.</p>
<p>This method is a workaround for the above OS X issue, but it’s an overall
good practice for managing dependencies and can complement the first method.</p>
<p><a class="reference external" href="https://virtualenv.pypa.io">virtualenv</a> is a tool you can use to create virtual environments in python.
We recommended reading a tutorial like
<a class="reference external" href="http://docs.python-guide.org/en/latest/dev/virtualenvs/">http://docs.python-guide.org/en/latest/dev/virtualenvs/</a> to get started.</p>
<p>After any of these workarounds you should be able to install Scrapy:</p>
<p>
        © Copyright 2008-2016, Scrapy developers.
      
        <span class="commit">
          Revision <code>5aebdac4</code>.
        </span>
</p>
<p class="caption"><span class="caption-text">First steps</span></p>
<p class="caption"><span class="caption-text">Basic concepts</span></p>
<p class="caption"><span class="caption-text">Built-in services</span></p>
<p class="caption"><span class="caption-text">Solving specific problems</span></p>
<p class="caption"><span class="caption-text">Extending Scrapy</span></p>
<p class="caption"><span class="caption-text">All the rest</span></p>
<p>In this tutorial, we’ll assume that Scrapy is already installed on your system.
If that’s not the case, see <a class="reference internal" href="install.html#intro-install"><span class="std std-ref">Installation guide</span></a>.</p>
<p>We are going to scrape <a class="reference external" href="http://quotes.toscrape.com/">quotes.toscrape.com</a>, a website
that lists quotes from famous authors.</p>
<p>This tutorial will walk you through these tasks:</p>
<p>Scrapy is written in <a class="reference external" href="https://www.python.org/">Python</a>. If you’re new to the language you might want to
start by getting an idea of what the language is like, to get the most out of
Scrapy.</p>
<p>If you’re already familiar with other languages, and want to learn Python
quickly, we recommend reading through <a class="reference external" href="http://www.diveintopython3.net">Dive Into Python 3</a>.  Alternatively,
you can follow the <a class="reference external" href="https://docs.python.org/3/tutorial">Python Tutorial</a>.</p>
<p>If you’re new to programming and want to start with Python, you may find useful
the online book <a class="reference external" href="http://learnpythonthehardway.org/book/">Learn Python The Hard Way</a>. You can also take a look at <a class="reference external" href="https://wiki.python.org/moin/BeginnersGuide/NonProgrammers">this
list of Python resources for non-programmers</a>.</p>
<p>Before you start scraping, you will have to set up a new Scrapy project. Enter a
directory where you’d like to store your code and run:</p>
<p>This will create a <code class="docutils literal"><span class="pre">tutorial</span></code> directory with the following contents:</p>
<p>Spiders are classes that you define and that Scrapy uses to scrape information
from a website (or a group of websites). They must subclass
<code class="xref py py-class docutils literal"><span class="pre">scrapy.Spider</span></code> and define the initial requests to make, optionally how
to follow links in the pages, and how to parse the downloaded page content to
extract data.</p>
<p>This is the code for our first Spider. Save it in a file named
<code class="docutils literal"><span class="pre">quotes_spider.py</span></code> under the <code class="docutils literal"><span class="pre">tutorial/spiders</span></code> directory in your project:</p>
<p>As you can see, our Spider subclasses <a class="reference internal" href="../topics/spiders.html#scrapy.spiders.Spider" title="scrapy.spiders.Spider"><code class="xref py py-class docutils literal"><span class="pre">scrapy.Spider</span></code></a>
and defines some attributes and methods:</p>
<p class="first"><a class="reference internal" href="../topics/spiders.html#scrapy.spiders.Spider.name" title="scrapy.spiders.Spider.name"><code class="xref py py-attr docutils literal"><span class="pre">name</span></code></a>: identifies the Spider. It must be
unique within a project, that is, you can’t set the same name for different
Spiders.</p>
<p class="first"><a class="reference internal" href="../topics/spiders.html#scrapy.spiders.Spider.start_requests" title="scrapy.spiders.Spider.start_requests"><code class="xref py py-meth docutils literal"><span class="pre">start_requests()</span></code></a>: must return an iterable of
Requests (you can return a list of requests or write a generator function)
which the Spider will begin to crawl from. Subsequent requests will be
generated successively from these initial requests.</p>
<p class="first"><a class="reference internal" href="../topics/spiders.html#scrapy.spiders.Spider.parse" title="scrapy.spiders.Spider.parse"><code class="xref py py-meth docutils literal"><span class="pre">parse()</span></code></a>: a method that will be called to handle
the response downloaded for each of the requests made. The response parameter
is an instance of <a class="reference internal" href="../topics/request-response.html#scrapy.http.TextResponse" title="scrapy.http.TextResponse"><code class="xref py py-class docutils literal"><span class="pre">TextResponse</span></code></a> that holds
the page content and has further helpful methods to handle it.</p>
<p>The <a class="reference internal" href="../topics/spiders.html#scrapy.spiders.Spider.parse" title="scrapy.spiders.Spider.parse"><code class="xref py py-meth docutils literal"><span class="pre">parse()</span></code></a> method usually parses the response, extracting
the scraped data as dicts and also finding new URLs to
follow and creating new requests (<a class="reference internal" href="../topics/request-response.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal"><span class="pre">Request</span></code></a>) from them.</p>
<p>To put our spider to work, go to the project’s top level directory and run:</p>
<p>This command runs the spider with name <code class="docutils literal"><span class="pre">quotes</span></code> that we’ve just added, that
will send some requests for the <code class="docutils literal"><span class="pre">quotes.toscrape.com</span></code> domain. You will get an output
similar to this:</p>
<p>Now, check the files in the current directory. You should notice that two new
files have been created: <em>quotes-1.html</em> and <em>quotes-2.html</em>, with the content
for the respective URLs, as our <code class="docutils literal"><span class="pre">parse</span></code> method instructs.</p>
<p class="first admonition-title">Note</p>
<p class="last">If you are wondering why we haven’t parsed the HTML yet, hold
on, we will cover that soon.</p>
<p>Scrapy schedules the <a class="reference internal" href="../topics/request-response.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal"><span class="pre">scrapy.Request</span></code></a> objects
returned by the <code class="docutils literal"><span class="pre">start_requests</span></code> method of the Spider. Upon receiving a
response for each one, it instantiates <a class="reference internal" href="../topics/request-response.html#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal"><span class="pre">Response</span></code></a> objects
and calls the callback method associated with the request (in this case, the
<code class="docutils literal"><span class="pre">parse</span></code> method) passing the response as argument.</p>
<p>Instead of implementing a <a class="reference internal" href="../topics/spiders.html#scrapy.spiders.Spider.start_requests" title="scrapy.spiders.Spider.start_requests"><code class="xref py py-meth docutils literal"><span class="pre">start_requests()</span></code></a> method
that generates <a class="reference internal" href="../topics/request-response.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal"><span class="pre">scrapy.Request</span></code></a> objects from URLs,
you can just define a <a class="reference internal" href="../topics/spiders.html#scrapy.spiders.Spider.start_urls" title="scrapy.spiders.Spider.start_urls"><code class="xref py py-attr docutils literal"><span class="pre">start_urls</span></code></a> class attribute
with a list of URLs. This list will then be used by the default implementation
of <a class="reference internal" href="../topics/spiders.html#scrapy.spiders.Spider.start_requests" title="scrapy.spiders.Spider.start_requests"><code class="xref py py-meth docutils literal"><span class="pre">start_requests()</span></code></a> to create the initial requests
for your spider:</p>
<p>The <a class="reference internal" href="../topics/spiders.html#scrapy.spiders.Spider.parse" title="scrapy.spiders.Spider.parse"><code class="xref py py-meth docutils literal"><span class="pre">parse()</span></code></a> method will be called to handle each
of the requests for those URLs, even though we haven’t explicitly told Scrapy
to do so. This happens because <a class="reference internal" href="../topics/spiders.html#scrapy.spiders.Spider.parse" title="scrapy.spiders.Spider.parse"><code class="xref py py-meth docutils literal"><span class="pre">parse()</span></code></a> is Scrapy’s
default callback method, which is called for requests without an explicitly
assigned callback.</p>
<p>The best way to learn how to extract data with Scrapy is trying selectors
using the shell <a class="reference internal" href="../topics/shell.html#topics-shell"><span class="std std-ref">Scrapy shell</span></a>. Run:</p>
<p class="first admonition-title">Note</p>
<p>Remember to always enclose urls in quotes when running Scrapy shell from
command-line, otherwise urls containing arguments (ie. <code class="docutils literal"><span class="pre">&amp;</span></code> character)
will not work.</p>
<p>On Windows, use double quotes instead:</p>
<p>You will see something like:</p>
<p>Using the shell, you can try selecting elements using <a class="reference external" href="https://www.w3.org/TR/selectors">CSS</a> with the response
object:</p>
<p>The result of running <code class="docutils literal"><span class="pre">response.css('title')</span></code> is a list-like object called
<a class="reference internal" href="../topics/selectors.html#scrapy.selector.SelectorList" title="scrapy.selector.SelectorList"><code class="xref py py-class docutils literal"><span class="pre">SelectorList</span></code></a>, which represents a list of
<a class="reference internal" href="../topics/selectors.html#scrapy.selector.Selector" title="scrapy.selector.Selector"><code class="xref py py-class docutils literal"><span class="pre">Selector</span></code></a> objects that wrap around XML/HTML elements
and allow you to run further queries to fine-grain the selection or extract the
data.</p>
<p>To extract the text from the title above, you can do:</p>
<p>There are two things to note here: one is that we’ve added <code class="docutils literal"><span class="pre">::text</span></code> to the
CSS query, to mean we want to select only the text elements directly inside
<code class="docutils literal"><span class="pre">&lt;title&gt;</span></code> element.  If we don’t specify <code class="docutils literal"><span class="pre">::text</span></code>, we’d get the full title
element, including its tags:</p>
<p>The other thing is that the result of calling <code class="docutils literal"><span class="pre">.extract()</span></code> is a list, because
we’re dealing with an instance of <a class="reference internal" href="../topics/selectors.html#scrapy.selector.SelectorList" title="scrapy.selector.SelectorList"><code class="xref py py-class docutils literal"><span class="pre">SelectorList</span></code></a>.  When
you know you just want the first result, as in this case, you can do:</p>
<p>As an alternative, you could’ve written:</p>
<p>However, using <code class="docutils literal"><span class="pre">.extract_first()</span></code> avoids an <code class="docutils literal"><span class="pre">IndexError</span></code> and returns
<code class="docutils literal"><span class="pre">None</span></code> when it doesn’t find any element matching the selection.</p>
<p>There’s a lesson here: for most scraping code, you want it to be resilient to
errors due to things not being found on a page, so that even if some parts fail
to be scraped, you can at least get <strong>some</strong> data.</p>
<p>Besides the <a class="reference internal" href="../topics/selectors.html#scrapy.selector.Selector.extract" title="scrapy.selector.Selector.extract"><code class="xref py py-meth docutils literal"><span class="pre">extract()</span></code></a> and
<code class="xref py py-meth docutils literal"><span class="pre">extract_first()</span></code> methods, you can also use
the <a class="reference internal" href="../topics/selectors.html#scrapy.selector.Selector.re" title="scrapy.selector.Selector.re"><code class="xref py py-meth docutils literal"><span class="pre">re()</span></code></a> method to extract using <cite>regular
expressions</cite>:</p>
<p>In order to find the proper CSS selectors to use, you might find useful opening
the response page from the shell in your web browser using <code class="docutils literal"><span class="pre">view(response)</span></code>.
You can use your browser developer tools or extensions like Firebug (see
sections about <a class="reference internal" href="../topics/firebug.html#topics-firebug"><span class="std std-ref">Using Firebug for scraping</span></a> and <a class="reference internal" href="../topics/firefox.html#topics-firefox"><span class="std std-ref">Using Firefox for scraping</span></a>).</p>
<p><a class="reference external" href="http://selectorgadget.com/">Selector Gadget</a> is also a nice tool to quickly find CSS selector for
visually selected elements, which works in many browsers.</p>
<p>Besides <a class="reference external" href="https://www.w3.org/TR/selectors">CSS</a>, Scrapy selectors also support using <a class="reference external" href="https://www.w3.org/TR/xpath">XPath</a> expressions:</p>
<p>XPath expressions are very powerful, and are the foundation of Scrapy
Selectors. In fact, CSS selectors are converted to XPath under-the-hood. You
can see that if you read closely the text representation of the selector
objects in the shell.</p>
<p>While perhaps not as popular as CSS selectors, XPath expressions offer more
power because besides navigating the structure, it can also look at the
content. Using XPath, you’re able to select things like: <em>select the link
that contains the text “Next Page”</em>. This makes XPath very fitting to the task
of scraping, and we encourage you to learn XPath even if you already know how to
construct CSS selectors, it will make scraping much easier.</p>
<p>We won’t cover much of XPath here, but you can read more about <a class="reference internal" href="../topics/selectors.html#topics-selectors"><span class="std std-ref">using XPath
with Scrapy Selectors here</span></a>. To learn more about XPath, we
recommend <a class="reference external" href="http://zvon.org/comp/r/tut-XPath_1.html">this tutorial to learn XPath through examples</a>, and <a class="reference external" href="http://plasmasturm.org/log/xpath101/">this tutorial to learn “how
to think in XPath”</a>.</p>
<p>Now that you know a bit about selection and extraction, let’s complete our
spider by writing the code to extract the quotes from the web page.</p>
<p>Each quote in <a class="reference external" href="http://quotes.toscrape.com">http://quotes.toscrape.com</a> is represented by HTML elements that look
like this:</p>
<p>Let’s open up scrapy shell and play a bit to find out how to extract the data
we want:</p>
<p>We get a list of selectors for the quote HTML elements with:</p>
<p>Each of the selectors returned by the query above allows us to run further
queries over their sub-elements. Let’s assign the first selector to a
variable, so that we can run our CSS selectors directly on a particular quote:</p>
<p>Now, let’s extract <code class="docutils literal"><span class="pre">title</span></code>, <code class="docutils literal"><span class="pre">author</span></code> and the <code class="docutils literal"><span class="pre">tags</span></code> from that quote
using the <code class="docutils literal"><span class="pre">quote</span></code> object we just created:</p>
<p>Given that the tags are a list of strings, we can use the <code class="docutils literal"><span class="pre">.extract()</span></code> method
to get all of them:</p>
<p>Having figured out how to extract each bit, we can now iterate over all the
quotes elements and put them together into a Python dictionary:</p>
<p>Let’s get back to our spider. Until now, it doesn’t extract any data in
particular, just saves the whole HTML page to a local file. Let’s integrate the
extraction logic above into our spider.</p>
<p>A Scrapy spider typically generates many dictionaries containing the data
extracted from the page. To do that, we use the <code class="docutils literal"><span class="pre">yield</span></code> Python keyword
in the callback, as you can see below:</p>
<p>If you run this spider, it will output the extracted data with the log:</p>
<p>The simplest way to store the scraped data is by using <a class="reference internal" href="../topics/feed-exports.html#topics-feed-exports"><span class="std std-ref">Feed exports</span></a>, with the following command:</p>
<p>That will generate an <code class="docutils literal"><span class="pre">quotes.json</span></code> file containing all scraped items,
serialized in <a class="reference external" href="https://en.wikipedia.org/wiki/JSON">JSON</a>.</p>
<p>For historic reasons, Scrapy appends to a given file instead of overwriting
its contents. If you run this command twice without removing the file
before the second time, you’ll end up with a broken JSON file.</p>
<p>You can also used other formats, like <a class="reference external" href="http://jsonlines.org">JSON Lines</a>:</p>
<p>The <a class="reference external" href="http://jsonlines.org">JSON Lines</a> format is useful because it’s stream-like, you can easily
append new records to it. It doesn’t have the same problem of JSON when you run
twice. Also, as each record is a separate line, you can process big files
without having to fit everything in memory, there are tools like <a class="reference external" href="https://stedolan.github.io/jq">JQ</a> to help
doing that at the command-line.</p>
<p>In small projects (like the one in this tutorial), that should be enough.
However, if you want to perform more complex things with the scraped items, you
can write an <a class="reference internal" href="../topics/item-pipeline.html#topics-item-pipeline"><span class="std std-ref">Item Pipeline</span></a>. A placeholder file
for Item Pipelines has been set up for you when the project is created, in
<code class="docutils literal"><span class="pre">tutorial/pipelines.py</span></code>. Though you don’t need to implement any item
pipelines if you just want to store the scraped items.</p>
<p>Let’s say, instead of just scraping the stuff from the first two pages
from <a class="reference external" href="http://quotes.toscrape.com">http://quotes.toscrape.com</a>, you want quotes from all the pages in the website.</p>
<p>Now that you know how to extract data from pages, let’s see how to follow links
from them.</p>
<p>First thing is to extract the link to the page we want to follow.  Examining
our page, we can see there is a link to the next page with the following
markup:</p>
<p>We can try extracting it in the shell:</p>
<p>This gets the anchor element, but we want the attribute <code class="docutils literal"><span class="pre">href</span></code>. For that,
Scrapy supports a CSS extension that let’s you select the attribute contents,
like this:</p>
<p>Let’s see now our spider modified to recursively follow the link to the next
page, extracting data from it:</p>
<p>Now, after extracting the data, the <code class="docutils literal"><span class="pre">parse()</span></code> method looks for the link to
the next page, builds a full absolute URL using the
<a class="reference internal" href="../topics/request-response.html#scrapy.http.Response.urljoin" title="scrapy.http.Response.urljoin"><code class="xref py py-meth docutils literal"><span class="pre">urljoin()</span></code></a> method (since the links can be
relative) and yields a new request to the next page, registering itself as
callback to handle the data extraction for the next page and to keep the
crawling going through all the pages.</p>
<p>What you see here is Scrapy’s mechanism of following links: when you yield
a Request in a callback method, Scrapy will schedule that request to be sent
and register a callback method to be executed when that request finishes.</p>
<p>Using this, you can build complex crawlers that follow links according to rules
you define, and extract different kinds of data depending on the page it’s
visiting.</p>
<p>In our example, it creates a sort of loop, following all the links to the next page
until it doesn’t find one – handy for crawling blogs, forums and other sites with
pagination.</p>
<p>As a shortcut for creating Request objects you can use
<a class="reference internal" href="../topics/request-response.html#scrapy.http.TextResponse.follow" title="scrapy.http.TextResponse.follow"><code class="xref py py-meth docutils literal"><span class="pre">response.follow</span></code></a>:</p>
<p>Unlike scrapy.Request, <code class="docutils literal"><span class="pre">response.follow</span></code> supports relative URLs directly - no
need to call urljoin. Note that <code class="docutils literal"><span class="pre">response.follow</span></code> just returns a Request
instance; you still have to yield this Request.</p>
<p>You can also pass a selector to <code class="docutils literal"><span class="pre">response.follow</span></code> instead of a string;
this selector should extract necessary attributes:</p>
<p>For <code class="docutils literal"><span class="pre">&lt;a&gt;</span></code> elements there is a shortcut: <code class="docutils literal"><span class="pre">response.follow</span></code> uses their href
attribute automatically. So the code can be shortened further:</p>
<p class="first admonition-title">Note</p>
<p class="last"><code class="docutils literal"><span class="pre">response.follow(response.css('li.next</span> <span class="pre">a'))</span></code> is not valid because
<code class="docutils literal"><span class="pre">response.css</span></code> returns a list-like object with selectors for all results,
not a single selector. A <code class="docutils literal"><span class="pre">for</span></code> loop like in the example above, or
<code class="docutils literal"><span class="pre">response.follow(response.css('li.next</span> <span class="pre">a')[0])</span></code> is fine.</p>
<p>Here is another spider that illustrates callbacks and following links,
this time for scraping author information:</p>
<p>This spider will start from the main page, it will follow all the links to the
authors pages calling the <code class="docutils literal"><span class="pre">parse_author</span></code> callback for each of them, and also
the pagination links with the <code class="docutils literal"><span class="pre">parse</span></code> callback as we saw before.</p>
<p>Here we’re passing callbacks to <code class="docutils literal"><span class="pre">response.follow</span></code> as positional arguments
to make the code shorter; it also works for <code class="docutils literal"><span class="pre">scrapy.Request</span></code>.</p>
<p>The <code class="docutils literal"><span class="pre">parse_author</span></code> callback defines a helper function to extract and cleanup the
data from a CSS query and yields the Python dict with the author data.</p>
<p>Another interesting thing this spider demonstrates is that, even if there are
many quotes from the same author, we don’t need to worry about visiting the
same author page multiple times. By default, Scrapy filters out duplicated
requests to URLs already visited, avoiding the problem of hitting servers too
much because of a programming mistake. This can be configured by the setting
<a class="reference internal" href="../topics/settings.html#std:setting-DUPEFILTER_CLASS"><code class="xref std std-setting docutils literal"><span class="pre">DUPEFILTER_CLASS</span></code></a>.</p>
<p>Hopefully by now you have a good understanding of how to use the mechanism
of following links and callbacks with Scrapy.</p>
<p>As yet another example spider that leverages the mechanism of following links,
check out the <a class="reference internal" href="../topics/spiders.html#scrapy.spiders.CrawlSpider" title="scrapy.spiders.CrawlSpider"><code class="xref py py-class docutils literal"><span class="pre">CrawlSpider</span></code></a> class for a generic
spider that implements a small rules engine that you can use to write your
crawlers on top of it.</p>
<p>Also, a common pattern is to build an item with data from more than one page,
using a <a class="reference internal" href="../topics/request-response.html#topics-request-response-ref-request-callback-arguments"><span class="std std-ref">trick to pass additional data to the callbacks</span></a>.</p>
<p>You can provide command line arguments to your spiders by using the <code class="docutils literal"><span class="pre">-a</span></code>
option when running them:</p>
<p>These arguments are passed to the Spider’s <code class="docutils literal"><span class="pre">__init__</span></code> method and become
spider attributes by default.</p>
<p>In this example, the value provided for the <code class="docutils literal"><span class="pre">tag</span></code> argument will be available
via <code class="docutils literal"><span class="pre">self.tag</span></code>. You can use this to make your spider fetch only quotes
with a specific tag, building the URL based on the argument:</p>
<p>If you pass the <code class="docutils literal"><span class="pre">tag=humor</span></code> argument to this spider, you’ll notice that it
will only visit URLs from the <code class="docutils literal"><span class="pre">humor</span></code> tag, such as
<code class="docutils literal"><span class="pre">http://quotes.toscrape.com/tag/humor</span></code>.</p>
<p>You can <a class="reference internal" href="../topics/spiders.html#spiderargs"><span class="std std-ref">learn more about handling spider arguments here</span></a>.</p>
<p>This tutorial covered only the basics of Scrapy, but there’s a lot of other
features not mentioned here. Check the <a class="reference internal" href="overview.html#topics-whatelse"><span class="std std-ref">What else?</span></a> section in
<a class="reference internal" href="overview.html#intro-overview"><span class="std std-ref">Scrapy at a glance</span></a> chapter for a quick overview of the most important ones.</p>
<p>You can continue from the section <a class="reference internal" href="../index.html#section-basics"><span class="std std-ref">Basic concepts</span></a> to know more about the
command-line tool, spiders, selectors and other things the tutorial hasn’t covered like
modeling the scraped data. If you prefer to play with an example project, check
the <a class="reference internal" href="examples.html#intro-examples"><span class="std std-ref">Examples</span></a> section.</p>
<p>
        © Copyright 2008-2016, Scrapy developers.
      
        <span class="commit">
          Revision <code>5aebdac4</code>.
        </span>
</p>
<p class="caption"><span class="caption-text">First steps</span></p>
<p class="caption"><span class="caption-text">Basic concepts</span></p>
<p class="caption"><span class="caption-text">Built-in services</span></p>
<p class="caption"><span class="caption-text">Solving specific problems</span></p>
<p class="caption"><span class="caption-text">Extending Scrapy</span></p>
<p class="caption"><span class="caption-text">All the rest</span></p>
<p>The best way to learn is with examples, and Scrapy is no exception. For this
reason, there is an example Scrapy project named <a class="reference external" href="https://github.com/scrapy/quotesbot">quotesbot</a>, that you can use to
play and learn more about Scrapy. It contains two spiders for
<a class="reference external" href="http://quotes.toscrape.com">http://quotes.toscrape.com</a>, one using CSS selectors and another one using XPath
expressions.</p>
<p>The <a class="reference external" href="https://github.com/scrapy/quotesbot">quotesbot</a> project is available at: <a class="reference external" href="https://github.com/scrapy/quotesbot">https://github.com/scrapy/quotesbot</a>.
You can find more information about it in the project’s README.</p>
<p>If you’re familiar with git, you can checkout the code. Otherwise you can
download the project as a zip file by clicking
<a class="reference external" href="https://github.com/scrapy/quotesbot/archive/master.zip">here</a>.</p>
<p>
        © Copyright 2008-2016, Scrapy developers.
      
        <span class="commit">
          Revision <code>5aebdac4</code>.
        </span>
</p>
<p class="caption"><span class="caption-text">First steps</span></p>
<p class="caption"><span class="caption-text">Basic concepts</span></p>
<p class="caption"><span class="caption-text">Built-in services</span></p>
<p class="caption"><span class="caption-text">Solving specific problems</span></p>
<p class="caption"><span class="caption-text">Extending Scrapy</span></p>
<p class="caption"><span class="caption-text">All the rest</span></p>
<p><span class="versionmodified">New in version 0.10.</span></p>
<p>Scrapy is controlled through the <code class="docutils literal"><span class="pre">scrapy</span></code> command-line tool, to be referred
here as the “Scrapy tool” to differentiate it from the sub-commands, which we
just call “commands” or “Scrapy commands”.</p>
<p>The Scrapy tool provides several commands, for multiple purposes, and each one
accepts a different set of arguments and options.</p>
<p>(The <code class="docutils literal"><span class="pre">scrapy</span> <span class="pre">deploy</span></code> command has been removed in 1.0 in favor of the
standalone <code class="docutils literal"><span class="pre">scrapyd-deploy</span></code>. See <a class="reference external" href="https://scrapyd.readthedocs.io/en/latest/deploy.html">Deploying your project</a>.)</p>
<p>Scrapy will look for configuration parameters in ini-style <code class="docutils literal"><span class="pre">scrapy.cfg</span></code> files
in standard locations:</p>
<p>Settings from these files are merged in the listed order of preference:
user-defined values have higher priority than system-wide defaults
and project-wide settings will override all others, when defined.</p>
<p>Scrapy also understands, and can be configured through, a number of environment
variables. Currently these are:</p>
<p>Before delving into the command-line tool and its sub-commands, let’s first
understand the directory structure of a Scrapy project.</p>
<p>Though it can be modified, all Scrapy projects have the same file
structure by default, similar to this:</p>
<p>The directory where the <code class="docutils literal"><span class="pre">scrapy.cfg</span></code> file resides is known as the <em>project
root directory</em>. That file contains the name of the python module that defines
the project settings. Here is an example:</p>
<p>You can start by running the Scrapy tool with no arguments and it will print
some usage help and the available commands:</p>
<p>The first line will print the currently active project if you’re inside a
Scrapy project. In this example it was run from outside a project. If run from inside
a project it would have printed something like this:</p>
<p>The first thing you typically do with the <code class="docutils literal"><span class="pre">scrapy</span></code> tool is create your Scrapy
project:</p>
<p>That will create a Scrapy project under the <code class="docutils literal"><span class="pre">project_dir</span></code> directory.
If <code class="docutils literal"><span class="pre">project_dir</span></code> wasn’t specified, <code class="docutils literal"><span class="pre">project_dir</span></code> will be the same as <code class="docutils literal"><span class="pre">myproject</span></code>.</p>
<p>Next, you go inside the new project directory:</p>
<p>And you’re ready to use the <code class="docutils literal"><span class="pre">scrapy</span></code> command to manage and control your
project from there.</p>
<p>You use the <code class="docutils literal"><span class="pre">scrapy</span></code> tool from inside your projects to control and manage
them.</p>
<p>For example, to create a new spider:</p>
<p>Some Scrapy commands (like <a class="reference internal" href="#std:command-crawl"><code class="xref std std-command docutils literal"><span class="pre">crawl</span></code></a>) must be run from inside a Scrapy
project. See the <a class="reference internal" href="#topics-commands-ref"><span class="std std-ref">commands reference</span></a> below for more
information on which commands must be run from inside projects, and which not.</p>
<p>Also keep in mind that some commands may have slightly different behaviours
when running them from inside projects. For example, the fetch command will use
spider-overridden behaviours (such as the <code class="docutils literal"><span class="pre">user_agent</span></code> attribute to override
the user-agent) if the url being fetched is associated with some specific
spider. This is intentional, as the <code class="docutils literal"><span class="pre">fetch</span></code> command is meant to be used to
check how spiders are downloading pages.</p>
<p>This section contains a list of the available built-in commands with a
description and some usage examples. Remember, you can always get more info
about each command by running:</p>
<p>And you can see all available commands with:</p>
<p>There are two kinds of commands, those that only work from inside a Scrapy
project (Project-specific commands) and those that also work without an active
Scrapy project (Global commands), though they may behave slightly different
when running from inside a project (as they would use the project overridden
settings).</p>
<p>Global commands:</p>
<p>Project-only commands:</p>
<p>Creates a new Scrapy project named <code class="docutils literal"><span class="pre">project_name</span></code>, under the <code class="docutils literal"><span class="pre">project_dir</span></code>
directory.
If <code class="docutils literal"><span class="pre">project_dir</span></code> wasn’t specified, <code class="docutils literal"><span class="pre">project_dir</span></code> will be the same as <code class="docutils literal"><span class="pre">myproject</span></code>.</p>
<p>Usage example:</p>
<p>Create a new spider in the current folder or in the current project’s <code class="docutils literal"><span class="pre">spiders</span></code> folder, if called from inside a project. The <code class="docutils literal"><span class="pre">&lt;name&gt;</span></code> parameter is set as the spider’s <code class="docutils literal"><span class="pre">name</span></code>, while <code class="docutils literal"><span class="pre">&lt;domain&gt;</span></code> is used to generate the <code class="docutils literal"><span class="pre">allowed_domains</span></code> and <code class="docutils literal"><span class="pre">start_urls</span></code> spider’s attributes.</p>
<p>Usage example:</p>
<p>This is just a convenience shortcut command for creating spiders based on
pre-defined templates, but certainly not the only way to create spiders. You
can just create the spider source code files yourself, instead of using this
command.</p>
<p>Start crawling using a spider.</p>
<p>Usage examples:</p>
<p>Run contract checks.</p>
<p>Usage examples:</p>
<p>List all available spiders in the current project. The output is one spider per
line.</p>
<p>Usage example:</p>
<p>Edit the given spider using the editor defined in the <code class="docutils literal"><span class="pre">EDITOR</span></code> environment
variable or (if unset) the <a class="reference internal" href="settings.html#std:setting-EDITOR"><code class="xref std std-setting docutils literal"><span class="pre">EDITOR</span></code></a> setting.</p>
<p>This command is provided only as a convenience shortcut for the most common
case, the developer is of course free to choose any tool or IDE to write and
debug spiders.</p>
<p>Usage example:</p>
<p>Downloads the given URL using the Scrapy downloader and writes the contents to
standard output.</p>
<p>The interesting thing about this command is that it fetches the page how the
spider would download it. For example, if the spider has a <code class="docutils literal"><span class="pre">USER_AGENT</span></code>
attribute which overrides the User Agent, it will use that one.</p>
<p>So this command can be used to “see” how your spider would fetch a certain page.</p>
<p>If used outside a project, no particular per-spider behaviour would be applied
and it will just use the default Scrapy downloader settings.</p>
<p>Supported options:</p>
<p>Usage examples:</p>
<p>Opens the given URL in a browser, as your Scrapy spider would “see” it.
Sometimes spiders see pages differently from regular users, so this can be used
to check what the spider “sees” and confirm it’s what you expect.</p>
<p>Supported options:</p>
<p>Usage example:</p>
<p>Starts the Scrapy shell for the given URL (if given) or empty if no URL is
given. Also supports UNIX-style local file paths, either relative with
<code class="docutils literal"><span class="pre">./</span></code> or <code class="docutils literal"><span class="pre">../</span></code> prefixes or absolute file paths.
See <a class="reference internal" href="shell.html#topics-shell"><span class="std std-ref">Scrapy shell</span></a> for more info.</p>
<p>Supported options:</p>
<p>Usage example:</p>
<p>Fetches the given URL and parses it with the spider that handles it, using the
method passed with the <code class="docutils literal"><span class="pre">--callback</span></code> option, or <code class="docutils literal"><span class="pre">parse</span></code> if not given.</p>
<p>Supported options:</p>
<p>Usage example:</p>
<p>Get the value of a Scrapy setting.</p>
<p>If used inside a project it’ll show the project setting value, otherwise it’ll
show the default Scrapy value for that setting.</p>
<p>Example usage:</p>
<p>Run a spider self-contained in a Python file, without having to create a
project.</p>
<p>Example usage:</p>
<p>Prints the Scrapy version. If used with <code class="docutils literal"><span class="pre">-v</span></code> it also prints Python, Twisted
and Platform info, which is useful for bug reports.</p>
<p><span class="versionmodified">New in version 0.17.</span></p>
<p>Run a quick benchmark test. <a class="reference internal" href="benchmarking.html#benchmarking"><span class="std std-ref">Benchmarking</span></a>.</p>
<p>You can also add your custom project commands by using the
<a class="reference internal" href="#std:setting-COMMANDS_MODULE"><code class="xref std std-setting docutils literal"><span class="pre">COMMANDS_MODULE</span></code></a> setting. See the Scrapy commands in
<a class="reference external" href="https://github.com/scrapy/scrapy/tree/master/scrapy/commands">scrapy/commands</a> for examples on how to implement your commands.</p>
<p>Default: <code class="docutils literal"><span class="pre">''</span></code> (empty string)</p>
<p>A module to use for looking up custom Scrapy commands. This is used to add custom
commands for your Scrapy project.</p>
<p>Example:</p>
<p class="first admonition-title">Note</p>
<p class="last">This is an experimental feature, use with caution.</p>
<p>You can also add Scrapy commands from an external library by adding a
<code class="docutils literal"><span class="pre">scrapy.commands</span></code> section in the entry points of the library <code class="docutils literal"><span class="pre">setup.py</span></code>
file.</p>
<p>The following example adds <code class="docutils literal"><span class="pre">my_command</span></code> command:</p>
<p>
        © Copyright 2008-2016, Scrapy developers.
      
        <span class="commit">
          Revision <code>5aebdac4</code>.
        </span>
</p>
<p class="caption"><span class="caption-text">First steps</span></p>
<p class="caption"><span class="caption-text">Basic concepts</span></p>
<p class="caption"><span class="caption-text">Built-in services</span></p>
<p class="caption"><span class="caption-text">Solving specific problems</span></p>
<p class="caption"><span class="caption-text">Extending Scrapy</span></p>
<p class="caption"><span class="caption-text">All the rest</span></p>
<p>Spiders are classes which define how a certain site (or a group of sites) will be
scraped, including how to perform the crawl (i.e. follow links) and how to
extract structured data from their pages (i.e. scraping items). In other words,
Spiders are the place where you define the custom behaviour for crawling and
parsing pages for a particular site (or, in some cases, a group of sites).</p>
<p>For spiders, the scraping cycle goes through something like this:</p>
<p class="first">You start by generating the initial Requests to crawl the first URLs, and
specify a callback function to be called with the response downloaded from
those requests.</p>
<p>The first requests to perform are obtained by calling the
<a class="reference internal" href="#scrapy.spiders.Spider.start_requests" title="scrapy.spiders.Spider.start_requests"><code class="xref py py-meth docutils literal"><span class="pre">start_requests()</span></code></a> method which (by default)
generates <a class="reference internal" href="request-response.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal"><span class="pre">Request</span></code></a> for the URLs specified in the
<a class="reference internal" href="#scrapy.spiders.Spider.start_urls" title="scrapy.spiders.Spider.start_urls"><code class="xref py py-attr docutils literal"><span class="pre">start_urls</span></code></a> and the
<a class="reference internal" href="#scrapy.spiders.Spider.parse" title="scrapy.spiders.Spider.parse"><code class="xref py py-attr docutils literal"><span class="pre">parse</span></code></a> method as callback function for the
Requests.</p>
<p class="first">In the callback function, you parse the response (web page) and return either
dicts with extracted data, <a class="reference internal" href="items.html#scrapy.item.Item" title="scrapy.item.Item"><code class="xref py py-class docutils literal"><span class="pre">Item</span></code></a> objects,
<a class="reference internal" href="request-response.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal"><span class="pre">Request</span></code></a> objects, or an iterable of these objects.
Those Requests will also contain a callback (maybe
the same) and will then be downloaded by Scrapy and then their
response handled by the specified callback.</p>
<p class="first">In callback functions, you parse the page contents, typically using
<a class="reference internal" href="selectors.html#topics-selectors"><span class="std std-ref">Selectors</span></a> (but you can also use BeautifulSoup, lxml or whatever
mechanism you prefer) and generate items with the parsed data.</p>
<p class="first">Finally, the items returned from the spider will be typically persisted to a
database (in some <a class="reference internal" href="item-pipeline.html#topics-item-pipeline"><span class="std std-ref">Item Pipeline</span></a>) or written to
a file using <a class="reference internal" href="feed-exports.html#topics-feed-exports"><span class="std std-ref">Feed exports</span></a>.</p>
<p>Even though this cycle applies (more or less) to any kind of spider, there are
different kinds of default spiders bundled into Scrapy for different purposes.
We will talk about those types here.</p>
<p>This is the simplest spider, and the one from which every other spider
must inherit (including spiders that come bundled with Scrapy, as well as spiders
that you write yourself). It doesn’t provide any special functionality. It just
provides a default <a class="reference internal" href="#scrapy.spiders.Spider.start_requests" title="scrapy.spiders.Spider.start_requests"><code class="xref py py-meth docutils literal"><span class="pre">start_requests()</span></code></a> implementation which sends requests from
the <a class="reference internal" href="#scrapy.spiders.Spider.start_urls" title="scrapy.spiders.Spider.start_urls"><code class="xref py py-attr docutils literal"><span class="pre">start_urls</span></code></a> spider attribute and calls the spider’s method <code class="docutils literal"><span class="pre">parse</span></code>
for each of the resulting responses.</p>
<p>A string which defines the name for this spider. The spider name is how
the spider is located (and instantiated) by Scrapy, so it must be
unique. However, nothing prevents you from instantiating more than one
instance of the same spider. This is the most important spider attribute
and it’s required.</p>
<p>If the spider scrapes a single domain, a common practice is to name the
spider after the domain, with or without the <a class="reference external" href="https://en.wikipedia.org/wiki/Top-level_domain">TLD</a>. So, for example, a
spider that crawls <code class="docutils literal"><span class="pre">mywebsite.com</span></code> would often be called
<code class="docutils literal"><span class="pre">mywebsite</span></code>.</p>
<p class="first admonition-title">Note</p>
<p class="last">In Python 2 this must be ASCII only.</p>
<p>An optional list of strings containing domains that this spider is
allowed to crawl. Requests for URLs not belonging to the domain names
specified in this list (or their subdomains) won’t be followed if
<a class="reference internal" href="spider-middleware.html#scrapy.spidermiddlewares.offsite.OffsiteMiddleware" title="scrapy.spidermiddlewares.offsite.OffsiteMiddleware"><code class="xref py py-class docutils literal"><span class="pre">OffsiteMiddleware</span></code></a> is enabled.</p>
<p>Let’s say your target url is <code class="docutils literal"><span class="pre">https://www.example.com/1.html</span></code>,
then add <code class="docutils literal"><span class="pre">'example.com'</span></code> to the list.</p>
<p>A list of URLs where the spider will begin to crawl from, when no
particular URLs are specified. So, the first pages downloaded will be those
listed here. The subsequent URLs will be generated successively from data
contained in the start URLs.</p>
<p>A dictionary of settings that will be overridden from the project wide
configuration when running this spider. It must be defined as a class
attribute since the settings are updated before instantiation.</p>
<p>For a list of available built-in settings see:
<a class="reference internal" href="settings.html#topics-settings-ref"><span class="std std-ref">Built-in settings reference</span></a>.</p>
<p>This attribute is set by the <a class="reference internal" href="item-pipeline.html#from_crawler" title="from_crawler"><code class="xref py py-meth docutils literal"><span class="pre">from_crawler()</span></code></a> class method after
initializating the class, and links to the
<a class="reference internal" href="api.html#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler"><code class="xref py py-class docutils literal"><span class="pre">Crawler</span></code></a> object to which this spider instance is
bound.</p>
<p>Crawlers encapsulate a lot of components in the project for their single
entry access (such as extensions, middlewares, signals managers, etc).
See <a class="reference internal" href="api.html#topics-api-crawler"><span class="std std-ref">Crawler API</span></a> to know more about them.</p>
<p>Configuration for running this spider. This is a
<a class="reference internal" href="api.html#scrapy.settings.Settings" title="scrapy.settings.Settings"><code class="xref py py-class docutils literal"><span class="pre">Settings</span></code></a> instance, see the
<a class="reference internal" href="settings.html#topics-settings"><span class="std std-ref">Settings</span></a> topic for a detailed introduction on this subject.</p>
<p>Python logger created with the Spider’s <a class="reference internal" href="#scrapy.spiders.Spider.name" title="scrapy.spiders.Spider.name"><code class="xref py py-attr docutils literal"><span class="pre">name</span></code></a>. You can use it to
send log messages through it as described on
<a class="reference internal" href="logging.html#topics-logging-from-spiders"><span class="std std-ref">Logging from Spiders</span></a>.</p>
<p>This is the class method used by Scrapy to create your spiders.</p>
<p>You probably won’t need to override this directly because the default
implementation acts as a proxy to the <code class="xref py py-meth docutils literal"><span class="pre">__init__()</span></code> method, calling
it with the given arguments <cite>args</cite> and named arguments <cite>kwargs</cite>.</p>
<p>Nonetheless, this method sets the <a class="reference internal" href="#scrapy.spiders.Spider.crawler" title="scrapy.spiders.Spider.crawler"><code class="xref py py-attr docutils literal"><span class="pre">crawler</span></code></a> and <a class="reference internal" href="#scrapy.spiders.Spider.settings" title="scrapy.spiders.Spider.settings"><code class="xref py py-attr docutils literal"><span class="pre">settings</span></code></a>
attributes in the new instance so they can be accessed later inside the
spider’s code.</p>
<p>This method must return an iterable with the first Requests to crawl for
this spider. It is called by Scrapy when the spider is opened for
scraping. Scrapy calls it only once, so it is safe to implement
<a class="reference internal" href="#scrapy.spiders.Spider.start_requests" title="scrapy.spiders.Spider.start_requests"><code class="xref py py-meth docutils literal"><span class="pre">start_requests()</span></code></a> as a generator.</p>
<p>The default implementation generates <code class="docutils literal"><span class="pre">Request(url,</span> <span class="pre">dont_filter=True)</span></code>
for each url in <a class="reference internal" href="#scrapy.spiders.Spider.start_urls" title="scrapy.spiders.Spider.start_urls"><code class="xref py py-attr docutils literal"><span class="pre">start_urls</span></code></a>.</p>
<p>If you want to change the Requests used to start scraping a domain, this is
the method to override. For example, if you need to start by logging in using
a POST request, you could do:</p>
<p>This is the default callback used by Scrapy to process downloaded
responses, when their requests don’t specify a callback.</p>
<p>The <code class="docutils literal"><span class="pre">parse</span></code> method is in charge of processing the response and returning
scraped data and/or more URLs to follow. Other Requests callbacks have
the same requirements as the <a class="reference internal" href="#scrapy.spiders.Spider" title="scrapy.spiders.Spider"><code class="xref py py-class docutils literal"><span class="pre">Spider</span></code></a> class.</p>
<p>This method, as well as any other Request callback, must return an
iterable of <a class="reference internal" href="request-response.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal"><span class="pre">Request</span></code></a> and/or
dicts or <a class="reference internal" href="items.html#scrapy.item.Item" title="scrapy.item.Item"><code class="xref py py-class docutils literal"><span class="pre">Item</span></code></a> objects.</p>
<p>Wrapper that sends a log message through the Spider’s <a class="reference internal" href="#scrapy.spiders.Spider.logger" title="scrapy.spiders.Spider.logger"><code class="xref py py-attr docutils literal"><span class="pre">logger</span></code></a>,
kept for backwards compatibility. For more information see
<a class="reference internal" href="logging.html#topics-logging-from-spiders"><span class="std std-ref">Logging from Spiders</span></a>.</p>
<p>Called when the spider closes. This method provides a shortcut to
signals.connect() for the <a class="reference internal" href="signals.html#std:signal-spider_closed"><code class="xref std std-signal docutils literal"><span class="pre">spider_closed</span></code></a> signal.</p>
<p>Let’s see an example:</p>
<p>Return multiple Requests and items from a single callback:</p>
<p>Instead of <a class="reference internal" href="#scrapy.spiders.Spider.start_urls" title="scrapy.spiders.Spider.start_urls"><code class="xref py py-attr docutils literal"><span class="pre">start_urls</span></code></a> you can use <a class="reference internal" href="#scrapy.spiders.Spider.start_requests" title="scrapy.spiders.Spider.start_requests"><code class="xref py py-meth docutils literal"><span class="pre">start_requests()</span></code></a> directly;
to give data more structure you can use <a class="reference internal" href="items.html#topics-items"><span class="std std-ref">Items</span></a>:</p>
<p>Spiders can receive arguments that modify their behaviour. Some common uses for
spider arguments are to define the start URLs or to restrict the crawl to
certain sections of the site, but they can be used to configure any
functionality of the spider.</p>
<p>Spider arguments are passed through the <a class="reference internal" href="commands.html#std:command-crawl"><code class="xref std std-command docutils literal"><span class="pre">crawl</span></code></a> command using the
<code class="docutils literal"><span class="pre">-a</span></code> option. For example:</p>
<p>Spiders can access arguments in their <cite>__init__</cite> methods:</p>
<p>The default <cite>__init__</cite> method will take any spider arguments
and copy them to the spider as attributes.
The above example can also be written as follows:</p>
<p>Keep in mind that spider arguments are only strings.
The spider will not do any parsing on its own.
If you were to set the <cite>start_urls</cite> attribute from the command line,
you would have to parse it on your own into a list
using something like
<a class="reference external" href="https://docs.python.org/library/ast.html#ast.literal_eval">ast.literal_eval</a>
or <a class="reference external" href="https://docs.python.org/library/json.html#json.loads">json.loads</a>
and then set it as an attribute.
Otherwise, you would cause iteration over a <cite>start_urls</cite> string
(a very common python pitfall)
resulting in each character being seen as a separate url.</p>
<p>A valid use case is to set the http auth credentials
used by <a class="reference internal" href="downloader-middleware.html#scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware" title="scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware"><code class="xref py py-class docutils literal"><span class="pre">HttpAuthMiddleware</span></code></a>
or the user agent
used by <a class="reference internal" href="downloader-middleware.html#scrapy.downloadermiddlewares.useragent.UserAgentMiddleware" title="scrapy.downloadermiddlewares.useragent.UserAgentMiddleware"><code class="xref py py-class docutils literal"><span class="pre">UserAgentMiddleware</span></code></a>:</p>
<p>Spider arguments can also be passed through the Scrapyd <code class="docutils literal"><span class="pre">schedule.json</span></code> API.
See <a class="reference external" href="https://scrapyd.readthedocs.io/en/latest/">Scrapyd documentation</a>.</p>
<p>Scrapy comes with some useful generic spiders that you can use to subclass
your spiders from. Their aim is to provide convenient functionality for a few
common scraping cases, like following all links on a site based on certain
rules, crawling from <a class="reference external" href="http://www.sitemaps.org">Sitemaps</a>, or parsing an XML/CSV feed.</p>
<p>For the examples used in the following spiders, we’ll assume you have a project
with a <code class="docutils literal"><span class="pre">TestItem</span></code> declared in a <code class="docutils literal"><span class="pre">myproject.items</span></code> module:</p>
<p>This is the most commonly used spider for crawling regular websites, as it
provides a convenient mechanism for following links by defining a set of rules.
It may not be the best suited for your particular web sites or project, but
it’s generic enough for several cases, so you can start from it and override it
as needed for more custom functionality, or just implement your own spider.</p>
<p>Apart from the attributes inherited from Spider (that you must
specify), this class supports a new attribute:</p>
<p>Which is a list of one (or more) <a class="reference internal" href="#scrapy.spiders.Rule" title="scrapy.spiders.Rule"><code class="xref py py-class docutils literal"><span class="pre">Rule</span></code></a> objects.  Each <a class="reference internal" href="#scrapy.spiders.Rule" title="scrapy.spiders.Rule"><code class="xref py py-class docutils literal"><span class="pre">Rule</span></code></a>
defines a certain behaviour for crawling the site. Rules objects are
described below. If multiple rules match the same link, the first one
will be used, according to the order they’re defined in this attribute.</p>
<p>This spider also exposes an overrideable method:</p>
<p>This method is called for the start_urls responses. It allows to parse
the initial responses and must return either an
<a class="reference internal" href="items.html#scrapy.item.Item" title="scrapy.item.Item"><code class="xref py py-class docutils literal"><span class="pre">Item</span></code></a> object, a <a class="reference internal" href="request-response.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal"><span class="pre">Request</span></code></a>
object, or an iterable containing any of them.</p>
<p><code class="docutils literal"><span class="pre">link_extractor</span></code> is a <a class="reference internal" href="link-extractors.html#topics-link-extractors"><span class="std std-ref">Link Extractor</span></a> object which
defines how links will be extracted from each crawled page.</p>
<p><code class="docutils literal"><span class="pre">callback</span></code> is a callable or a string (in which case a method from the spider
object with that name will be used) to be called for each link extracted with
the specified link_extractor. This callback receives a response as its first
argument and must return a list containing <a class="reference internal" href="items.html#scrapy.item.Item" title="scrapy.item.Item"><code class="xref py py-class docutils literal"><span class="pre">Item</span></code></a> and/or
<a class="reference internal" href="request-response.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal"><span class="pre">Request</span></code></a> objects (or any subclass of them).</p>
<p class="first admonition-title">Warning</p>
<p class="last">When writing crawl spider rules, avoid using <code class="docutils literal"><span class="pre">parse</span></code> as
callback, since the <a class="reference internal" href="#scrapy.spiders.CrawlSpider" title="scrapy.spiders.CrawlSpider"><code class="xref py py-class docutils literal"><span class="pre">CrawlSpider</span></code></a> uses the <code class="docutils literal"><span class="pre">parse</span></code> method
itself to implement its logic. So if you override the <code class="docutils literal"><span class="pre">parse</span></code> method,
the crawl spider will no longer work.</p>
<p><code class="docutils literal"><span class="pre">cb_kwargs</span></code> is a dict containing the keyword arguments to be passed to the
callback function.</p>
<p><code class="docutils literal"><span class="pre">follow</span></code> is a boolean which specifies if links should be followed from each
response extracted with this rule. If <code class="docutils literal"><span class="pre">callback</span></code> is None <code class="docutils literal"><span class="pre">follow</span></code> defaults
to <code class="docutils literal"><span class="pre">True</span></code>, otherwise it defaults to <code class="docutils literal"><span class="pre">False</span></code>.</p>
<p><code class="docutils literal"><span class="pre">process_links</span></code> is a callable, or a string (in which case a method from the
spider object with that name will be used) which will be called for each list
of links extracted from each response using the specified <code class="docutils literal"><span class="pre">link_extractor</span></code>.
This is mainly used for filtering purposes.</p>
<p><code class="docutils literal"><span class="pre">process_request</span></code> is a callable, or a string (in which case a method from
the spider object with that name will be used) which will be called with
every request extracted by this rule, and must return a request or None (to
filter out the request).</p>
<p>Let’s now take a look at an example CrawlSpider with rules:</p>
<p>This spider would start crawling example.com’s home page, collecting category
links, and item links, parsing the latter with the <code class="docutils literal"><span class="pre">parse_item</span></code> method. For
each item response, some data will be extracted from the HTML using XPath, and
an <a class="reference internal" href="items.html#scrapy.item.Item" title="scrapy.item.Item"><code class="xref py py-class docutils literal"><span class="pre">Item</span></code></a> will be filled with it.</p>
<p>XMLFeedSpider is designed for parsing XML feeds by iterating through them by a
certain node name.  The iterator can be chosen from: <code class="docutils literal"><span class="pre">iternodes</span></code>, <code class="docutils literal"><span class="pre">xml</span></code>,
and <code class="docutils literal"><span class="pre">html</span></code>.  It’s recommended to use the <code class="docutils literal"><span class="pre">iternodes</span></code> iterator for
performance reasons, since the <code class="docutils literal"><span class="pre">xml</span></code> and <code class="docutils literal"><span class="pre">html</span></code> iterators generate the
whole DOM at once in order to parse it.  However, using <code class="docutils literal"><span class="pre">html</span></code> as the
iterator may be useful when parsing XML with bad markup.</p>
<p>To set the iterator and the tag name, you must define the following class
attributes:</p>
<p>A string which defines the iterator to use. It can be either:</p>
<p>It defaults to: <code class="docutils literal"><span class="pre">'iternodes'</span></code>.</p>
<p>A string with the name of the node (or element) to iterate in. Example:</p>
<p>A list of <code class="docutils literal"><span class="pre">(prefix,</span> <span class="pre">uri)</span></code> tuples which define the namespaces
available in that document that will be processed with this spider. The
<code class="docutils literal"><span class="pre">prefix</span></code> and <code class="docutils literal"><span class="pre">uri</span></code> will be used to automatically register
namespaces using the
<a class="reference internal" href="selectors.html#scrapy.selector.Selector.register_namespace" title="scrapy.selector.Selector.register_namespace"><code class="xref py py-meth docutils literal"><span class="pre">register_namespace()</span></code></a> method.</p>
<p>You can then specify nodes with namespaces in the <a class="reference internal" href="#scrapy.spiders.XMLFeedSpider.itertag" title="scrapy.spiders.XMLFeedSpider.itertag"><code class="xref py py-attr docutils literal"><span class="pre">itertag</span></code></a>
attribute.</p>
<p>Example:</p>
<p>Apart from these new attributes, this spider has the following overrideable
methods too:</p>
<p>A method that receives the response as soon as it arrives from the spider
middleware, before the spider starts parsing it. It can be used to modify
the response body before parsing it. This method receives a response and
also returns a response (it could be the same or another one).</p>
<p>This method is called for the nodes matching the provided tag name
(<code class="docutils literal"><span class="pre">itertag</span></code>).  Receives the response and an
<a class="reference internal" href="selectors.html#scrapy.selector.Selector" title="scrapy.selector.Selector"><code class="xref py py-class docutils literal"><span class="pre">Selector</span></code></a> for each node.  Overriding this
method is mandatory. Otherwise, you spider won’t work.  This method
must return either a <a class="reference internal" href="items.html#scrapy.item.Item" title="scrapy.item.Item"><code class="xref py py-class docutils literal"><span class="pre">Item</span></code></a> object, a
<a class="reference internal" href="request-response.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal"><span class="pre">Request</span></code></a> object, or an iterable containing any of
them.</p>
<p>This method is called for each result (item or request) returned by the
spider, and it’s intended to perform any last time processing required
before returning the results to the framework core, for example setting the
item IDs. It receives a list of results and the response which originated
those results. It must return a list of results (Items or Requests).</p>
<p>These spiders are pretty easy to use, let’s have a look at one example:</p>
<p>Basically what we did up there was to create a spider that downloads a feed from
the given <code class="docutils literal"><span class="pre">start_urls</span></code>, and then iterates through each of its <code class="docutils literal"><span class="pre">item</span></code> tags,
prints them out, and stores some random data in an <a class="reference internal" href="items.html#scrapy.item.Item" title="scrapy.item.Item"><code class="xref py py-class docutils literal"><span class="pre">Item</span></code></a>.</p>
<p>This spider is very similar to the XMLFeedSpider, except that it iterates
over rows, instead of nodes. The method that gets called in each iteration
is <a class="reference internal" href="#scrapy.spiders.CSVFeedSpider.parse_row" title="scrapy.spiders.CSVFeedSpider.parse_row"><code class="xref py py-meth docutils literal"><span class="pre">parse_row()</span></code></a>.</p>
<p>A string with the separator character for each field in the CSV file
Defaults to <code class="docutils literal"><span class="pre">','</span></code> (comma).</p>
<p>A string with the enclosure character for each field in the CSV file
Defaults to <code class="docutils literal"><span class="pre">'"'</span></code> (quotation mark).</p>
<p>A list of the rows contained in the file CSV feed which will be used to
extract fields from it.</p>
<p>Receives a response and a dict (representing each row) with a key for each
provided (or detected) header of the CSV file.  This spider also gives the
opportunity to override <code class="docutils literal"><span class="pre">adapt_response</span></code> and <code class="docutils literal"><span class="pre">process_results</span></code> methods
for pre- and post-processing purposes.</p>
<p>Let’s see an example similar to the previous one, but using a
<a class="reference internal" href="#scrapy.spiders.CSVFeedSpider" title="scrapy.spiders.CSVFeedSpider"><code class="xref py py-class docutils literal"><span class="pre">CSVFeedSpider</span></code></a>:</p>
<p>SitemapSpider allows you to crawl a site by discovering the URLs using
<a class="reference external" href="http://www.sitemaps.org">Sitemaps</a>.</p>
<p>It supports nested sitemaps and discovering sitemap urls from
<a class="reference external" href="http://www.robotstxt.org/">robots.txt</a>.</p>
<p>A list of urls pointing to the sitemaps whose urls you want to crawl.</p>
<p>You can also point to a <a class="reference external" href="http://www.robotstxt.org/">robots.txt</a> and it will be parsed to extract
sitemap urls from it.</p>
<p>A list of tuples <code class="docutils literal"><span class="pre">(regex,</span> <span class="pre">callback)</span></code> where:</p>
<p>For example:</p>
<p>Rules are applied in order, and only the first one that matches will be
used.</p>
<p>If you omit this attribute, all urls found in sitemaps will be
processed with the <code class="docutils literal"><span class="pre">parse</span></code> callback.</p>
<p>A list of regexes of sitemap that should be followed. This is is only
for sites that use <a class="reference external" href="http://www.sitemaps.org/protocol.html#index">Sitemap index files</a> that point to other sitemap
files.</p>
<p>By default, all sitemaps are followed.</p>
<p>Specifies if alternate links for one <code class="docutils literal"><span class="pre">url</span></code> should be followed. These
are links for the same website in another language passed within
the same <code class="docutils literal"><span class="pre">url</span></code> block.</p>
<p>For example:</p>
<p>With <code class="docutils literal"><span class="pre">sitemap_alternate_links</span></code> set, this would retrieve both URLs. With
<code class="docutils literal"><span class="pre">sitemap_alternate_links</span></code> disabled, only <code class="docutils literal"><span class="pre">http://example.com/</span></code> would be
retrieved.</p>
<p>Default is <code class="docutils literal"><span class="pre">sitemap_alternate_links</span></code> disabled.</p>
<p>Simplest example: process all urls discovered through sitemaps using the
<code class="docutils literal"><span class="pre">parse</span></code> callback:</p>
<p>Process some urls with certain callback and other urls with a different
callback:</p>
<p>Follow sitemaps defined in the <a class="reference external" href="http://www.robotstxt.org/">robots.txt</a> file and only follow sitemaps
whose url contains <code class="docutils literal"><span class="pre">/sitemap_shop</span></code>:</p>
<p>Combine SitemapSpider with other sources of urls:</p>
<p>
        © Copyright 2008-2016, Scrapy developers.
      
        <span class="commit">
          Revision <code>5aebdac4</code>.
        </span>
</p>
<p class="caption"><span class="caption-text">First steps</span></p>
<p class="caption"><span class="caption-text">Basic concepts</span></p>
<p class="caption"><span class="caption-text">Built-in services</span></p>
<p class="caption"><span class="caption-text">Solving specific problems</span></p>
<p class="caption"><span class="caption-text">Extending Scrapy</span></p>
<p class="caption"><span class="caption-text">All the rest</span></p>
<p>When you’re scraping web pages, the most common task you need to perform is
to extract data from the HTML source. There are several libraries available to
achieve this:</p>
<p>Scrapy comes with its own mechanism for extracting data. They’re called
selectors because they “select” certain parts of the HTML document specified
either by <a class="reference external" href="https://www.w3.org/TR/xpath">XPath</a> or <a class="reference external" href="https://www.w3.org/TR/selectors">CSS</a> expressions.</p>
<p><a class="reference external" href="https://www.w3.org/TR/xpath">XPath</a> is a language for selecting nodes in XML documents, which can also be
used with HTML. <a class="reference external" href="https://www.w3.org/TR/selectors">CSS</a> is a language for applying styles to HTML documents. It
defines selectors to associate those styles with specific HTML elements.</p>
<p>Scrapy selectors are built over the <a class="reference external" href="http://lxml.de/">lxml</a> library, which means they’re very
similar in speed and parsing accuracy.</p>
<p>This page explains how selectors work and describes their API which is very
small and simple, unlike the <a class="reference external" href="http://lxml.de/">lxml</a> API which is much bigger because the
<a class="reference external" href="http://lxml.de/">lxml</a> library can be used for many other tasks, besides selecting markup
documents.</p>
<p>For a complete reference of the selectors API see
<a class="reference internal" href="#topics-selectors-ref"><span class="std std-ref">Selector reference</span></a></p>
<p>Scrapy selectors are instances of <a class="reference internal" href="#scrapy.selector.Selector" title="scrapy.selector.Selector"><code class="xref py py-class docutils literal"><span class="pre">Selector</span></code></a> class
constructed by passing <strong>text</strong> or <a class="reference internal" href="request-response.html#scrapy.http.TextResponse" title="scrapy.http.TextResponse"><code class="xref py py-class docutils literal"><span class="pre">TextResponse</span></code></a>
object. It automatically chooses the best parsing rules (XML vs HTML) based on
input type:</p>
<p>Constructing from text:</p>
<p>Constructing from response:</p>
<p>For convenience, response objects expose a selector on <cite>.selector</cite> attribute,
it’s totally OK to use this shortcut when possible:</p>
<p>To explain how to use the selectors we’ll use the <cite>Scrapy shell</cite> (which
provides interactive testing) and an example page located in the Scrapy
documentation server:</p>
<p id="topics-selectors-htmlcode">Here’s its HTML code:</p>
<p>First, let’s open the shell:</p>
<p>Then, after the shell loads, you’ll have the response available as <code class="docutils literal"><span class="pre">response</span></code>
shell variable, and its attached selector in <code class="docutils literal"><span class="pre">response.selector</span></code> attribute.</p>
<p>Since we’re dealing with HTML, the selector will automatically use an HTML parser.</p>
<p>So, by looking at the <a class="reference internal" href="#topics-selectors-htmlcode"><span class="std std-ref">HTML code</span></a> of that
page, let’s construct an XPath for selecting the text inside the title tag:</p>
<p>Querying responses using XPath and CSS is so common that responses include two
convenience shortcuts: <code class="docutils literal"><span class="pre">response.xpath()</span></code> and <code class="docutils literal"><span class="pre">response.css()</span></code>:</p>
<p>As you can see, <code class="docutils literal"><span class="pre">.xpath()</span></code> and <code class="docutils literal"><span class="pre">.css()</span></code> methods return a
<a class="reference internal" href="#scrapy.selector.SelectorList" title="scrapy.selector.SelectorList"><code class="xref py py-class docutils literal"><span class="pre">SelectorList</span></code></a> instance, which is a list of new
selectors. This API can be used for quickly selecting nested data:</p>
<p>To actually extract the textual data, you must call the selector <code class="docutils literal"><span class="pre">.extract()</span></code>
method, as follows:</p>
<p>If you want to extract only first matched element, you can call the selector <code class="docutils literal"><span class="pre">.extract_first()</span></code></p>
<p>It returns <code class="docutils literal"><span class="pre">None</span></code> if no element was found:</p>
<p>A default return value can be provided as an argument, to be used instead of <code class="docutils literal"><span class="pre">None</span></code>:</p>
<p>Notice that CSS selectors can select text or attribute nodes using CSS3
pseudo-elements:</p>
<p>Now we’re going to get the base URL and some image links:</p>
<p>The selection methods (<code class="docutils literal"><span class="pre">.xpath()</span></code> or <code class="docutils literal"><span class="pre">.css()</span></code>) return a list of selectors
of the same type, so you can call the selection methods for those selectors
too. Here’s an example:</p>
<p><a class="reference internal" href="#scrapy.selector.Selector" title="scrapy.selector.Selector"><code class="xref py py-class docutils literal"><span class="pre">Selector</span></code></a> also has a <code class="docutils literal"><span class="pre">.re()</span></code> method for extracting
data using regular expressions. However, unlike using <code class="docutils literal"><span class="pre">.xpath()</span></code> or
<code class="docutils literal"><span class="pre">.css()</span></code> methods, <code class="docutils literal"><span class="pre">.re()</span></code> returns a list of unicode strings. So you
can’t construct nested <code class="docutils literal"><span class="pre">.re()</span></code> calls.</p>
<p>Here’s an example used to extract image names from the <a class="reference internal" href="#topics-selectors-htmlcode"><span class="std std-ref">HTML code</span></a> above:</p>
<p>There’s an additional helper reciprocating <code class="docutils literal"><span class="pre">.extract_first()</span></code> for <code class="docutils literal"><span class="pre">.re()</span></code>,
named <code class="docutils literal"><span class="pre">.re_first()</span></code>. Use it to extract just the first matching string:</p>
<p>Keep in mind that if you are nesting selectors and use an XPath that starts
with <code class="docutils literal"><span class="pre">/</span></code>, that XPath will be absolute to the document and not relative to the
<code class="docutils literal"><span class="pre">Selector</span></code> you’re calling it from.</p>
<p>For example, suppose you want to extract all <code class="docutils literal"><span class="pre">&lt;p&gt;</span></code> elements inside <code class="docutils literal"><span class="pre">&lt;div&gt;</span></code>
elements. First, you would get all <code class="docutils literal"><span class="pre">&lt;div&gt;</span></code> elements:</p>
<p>At first, you may be tempted to use the following approach, which is wrong, as
it actually extracts all <code class="docutils literal"><span class="pre">&lt;p&gt;</span></code> elements from the document, not only those
inside <code class="docutils literal"><span class="pre">&lt;div&gt;</span></code> elements:</p>
<p>This is the proper way to do it (note the dot prefixing the <code class="docutils literal"><span class="pre">.//p</span></code> XPath):</p>
<p>Another common case would be to extract all direct <code class="docutils literal"><span class="pre">&lt;p&gt;</span></code> children:</p>
<p>For more details about relative XPaths see the <a class="reference external" href="https://www.w3.org/TR/xpath#location-paths">Location Paths</a> section in the
XPath specification.</p>
<p>XPath allows you to reference variables in your XPath expressions, using
the <code class="docutils literal"><span class="pre">$somevariable</span></code> syntax. This is somewhat similar to parameterized
queries or prepared statements in the SQL world where you replace
some arguments in your queries with placeholders like <code class="docutils literal"><span class="pre">?</span></code>,
which are then substituted with values passed with the query.</p>
<p>Here’s an example to match an element based on its “id” attribute value,
without hard-coding it (that was shown previously):</p>
<p>Here’s another example, to find the “id” attribute of a <code class="docutils literal"><span class="pre">&lt;div&gt;</span></code> tag containing
five <code class="docutils literal"><span class="pre">&lt;a&gt;</span></code> children (here we pass the value <code class="docutils literal"><span class="pre">5</span></code> as an integer):</p>
<p>All variable references must have a binding value when calling <code class="docutils literal"><span class="pre">.xpath()</span></code>
(otherwise you’ll get a <code class="docutils literal"><span class="pre">ValueError:</span> <span class="pre">XPath</span> <span class="pre">error:</span></code> exception).
This is done by passing as many named arguments as necessary.</p>
<p><a class="reference external" href="https://parsel.readthedocs.io/">parsel</a>, the library powering Scrapy selectors, has more details and examples
on <a class="reference external" href="https://parsel.readthedocs.io/en/latest/usage.html#variables-in-xpath-expressions">XPath variables</a>.</p>
<p>Being built atop <a class="reference external" href="http://lxml.de/">lxml</a>, Scrapy selectors also support some <a class="reference external" href="http://exslt.org/">EXSLT</a> extensions
and come with these pre-registered namespaces to use in XPath expressions:</p>
<p>The <code class="docutils literal"><span class="pre">test()</span></code> function, for example, can prove quite useful when XPath’s
<code class="docutils literal"><span class="pre">starts-with()</span></code> or <code class="docutils literal"><span class="pre">contains()</span></code> are not sufficient.</p>
<p>Example selecting links in list item with a “class” attribute ending with a digit:</p>
<p class="first admonition-title">Warning</p>
<p class="last">C library <code class="docutils literal"><span class="pre">libxslt</span></code> doesn’t natively support EXSLT regular
expressions so <a class="reference external" href="http://lxml.de/">lxml</a>‘s implementation uses hooks to Python’s <code class="docutils literal"><span class="pre">re</span></code> module.
Thus, using regexp functions in your XPath expressions may add a small
performance penalty.</p>
<p>These can be handy for excluding parts of a document tree before
extracting text elements for example.</p>
<p>Example extracting microdata (sample content taken from <a class="reference external" href="http://schema.org/Product">http://schema.org/Product</a>)
with groups of itemscopes and corresponding itemprops:</p>
<p>Here we first iterate over <code class="docutils literal"><span class="pre">itemscope</span></code> elements, and for each one,
we look for all <code class="docutils literal"><span class="pre">itemprops</span></code> elements and exclude those that are themselves
inside another <code class="docutils literal"><span class="pre">itemscope</span></code>.</p>
<p>Here are some tips that you may find useful when using XPath
with Scrapy selectors, based on <a class="reference external" href="https://blog.scrapinghub.com/2014/07/17/xpath-tips-from-the-web-scraping-trenches/">this post from ScrapingHub’s blog</a>.
If you are not much familiar with XPath yet,
you may want to take a look first at this <a class="reference external" href="http://www.zvon.org/comp/r/tut-XPath_1.html">XPath tutorial</a>.</p>
<p>When you need to use the text content as argument to an <a class="reference external" href="https://www.w3.org/TR/xpath/#section-String-Functions">XPath string function</a>,
avoid using <code class="docutils literal"><span class="pre">.//text()</span></code> and use just <code class="docutils literal"><span class="pre">.</span></code> instead.</p>
<p>This is because the expression <code class="docutils literal"><span class="pre">.//text()</span></code> yields a collection of text elements – a <em>node-set</em>.
And when a node-set is converted to a string, which happens when it is passed as argument to
a string function like <code class="docutils literal"><span class="pre">contains()</span></code> or <code class="docutils literal"><span class="pre">starts-with()</span></code>, it results in the text for the first element only.</p>
<p>Example:</p>
<p>Converting a <em>node-set</em> to string:</p>
<p>A <em>node</em> converted to a string, however, puts together the text of itself plus of all its descendants:</p>
<p>So, using the <code class="docutils literal"><span class="pre">.//text()</span></code> node-set won’t select anything in this case:</p>
<p>But using the <code class="docutils literal"><span class="pre">.</span></code> to mean the node, works:</p>
<p><code class="docutils literal"><span class="pre">//node[1]</span></code> selects all the nodes occurring first under their respective parents.</p>
<p><code class="docutils literal"><span class="pre">(//node)[1]</span></code> selects all the nodes in the document, and then gets only the first of them.</p>
<p>Example:</p>
<p>This gets all first <code class="docutils literal"><span class="pre">&lt;li&gt;</span></code>  elements under whatever it is its parent:</p>
<p>And this gets the first <code class="docutils literal"><span class="pre">&lt;li&gt;</span></code>  element in the whole document:</p>
<p>This gets all first <code class="docutils literal"><span class="pre">&lt;li&gt;</span></code>  elements under an <code class="docutils literal"><span class="pre">&lt;ul&gt;</span></code>  parent:</p>
<p>And this gets the first <code class="docutils literal"><span class="pre">&lt;li&gt;</span></code>  element under an <code class="docutils literal"><span class="pre">&lt;ul&gt;</span></code>  parent in the whole document:</p>
<p>Because an element can contain multiple CSS classes, the XPath way to select elements
by class is the rather verbose:</p>
<p>If you use <code class="docutils literal"><span class="pre">@class='someclass'</span></code> you may end up missing elements that have
other classes, and if you just use <code class="docutils literal"><span class="pre">contains(@class,</span> <span class="pre">'someclass')</span></code> to make up
for that you may end up with more elements that you want, if they have a different
class name that shares the string <code class="docutils literal"><span class="pre">someclass</span></code>.</p>
<p>As it turns out, Scrapy selectors allow you to chain selectors, so most of the time
you can just select by class using CSS and then switch to XPath when needed:</p>
<p>This is cleaner than using the verbose XPath trick shown above. Just remember
to use the <code class="docutils literal"><span class="pre">.</span></code> in the XPath expressions that will follow.</p>
<p>An instance of <a class="reference internal" href="#scrapy.selector.Selector" title="scrapy.selector.Selector"><code class="xref py py-class docutils literal"><span class="pre">Selector</span></code></a> is a wrapper over response to select
certain parts of its content.</p>
<p><code class="docutils literal"><span class="pre">response</span></code> is an <a class="reference internal" href="request-response.html#scrapy.http.HtmlResponse" title="scrapy.http.HtmlResponse"><code class="xref py py-class docutils literal"><span class="pre">HtmlResponse</span></code></a> or an
<a class="reference internal" href="request-response.html#scrapy.http.XmlResponse" title="scrapy.http.XmlResponse"><code class="xref py py-class docutils literal"><span class="pre">XmlResponse</span></code></a> object that will be used for selecting and
extracting data.</p>
<p><code class="docutils literal"><span class="pre">text</span></code> is a unicode string or utf-8 encoded text for cases when a
<code class="docutils literal"><span class="pre">response</span></code> isn’t available. Using <code class="docutils literal"><span class="pre">text</span></code> and <code class="docutils literal"><span class="pre">response</span></code> together is
undefined behavior.</p>
<p><code class="docutils literal"><span class="pre">type</span></code> defines the selector type, it can be <code class="docutils literal"><span class="pre">"html"</span></code>, <code class="docutils literal"><span class="pre">"xml"</span></code> or <code class="docutils literal"><span class="pre">None</span></code> (default).</p>
<p>If <code class="docutils literal"><span class="pre">type</span></code> is <code class="docutils literal"><span class="pre">None</span></code>, the selector automatically chooses the best type
based on <code class="docutils literal"><span class="pre">response</span></code> type (see below), or defaults to <code class="docutils literal"><span class="pre">"html"</span></code> in case it
is used together with <code class="docutils literal"><span class="pre">text</span></code>.</p>
<p>If <code class="docutils literal"><span class="pre">type</span></code> is <code class="docutils literal"><span class="pre">None</span></code> and a <code class="docutils literal"><span class="pre">response</span></code> is passed, the selector type is
inferred from the response type as follows:</p>
<p>Otherwise, if <code class="docutils literal"><span class="pre">type</span></code> is set, the selector type will be forced and no
detection will occur.</p>
<p>Find nodes matching the xpath <code class="docutils literal"><span class="pre">query</span></code> and return the result as a
<a class="reference internal" href="#scrapy.selector.SelectorList" title="scrapy.selector.SelectorList"><code class="xref py py-class docutils literal"><span class="pre">SelectorList</span></code></a> instance with all elements flattened. List
elements implement <a class="reference internal" href="#scrapy.selector.Selector" title="scrapy.selector.Selector"><code class="xref py py-class docutils literal"><span class="pre">Selector</span></code></a> interface too.</p>
<p><code class="docutils literal"><span class="pre">query</span></code> is a string containing the XPATH query to apply.</p>
<p class="first admonition-title">Note</p>
<p class="last">For convenience, this method can be called as <code class="docutils literal"><span class="pre">response.xpath()</span></code></p>
<p>Apply the given CSS selector and return a <a class="reference internal" href="#scrapy.selector.SelectorList" title="scrapy.selector.SelectorList"><code class="xref py py-class docutils literal"><span class="pre">SelectorList</span></code></a> instance.</p>
<p><code class="docutils literal"><span class="pre">query</span></code> is a string containing the CSS selector to apply.</p>
<p>In the background, CSS queries are translated into XPath queries using
<a class="reference external" href="https://pypi.python.org/pypi/cssselect/">cssselect</a> library and run <code class="docutils literal"><span class="pre">.xpath()</span></code> method.</p>
<p class="first admonition-title">Note</p>
<p class="last">For convenience this method can be called as <code class="docutils literal"><span class="pre">response.css()</span></code></p>
<p>Serialize and return the matched nodes as a list of unicode strings.
Percent encoded content is unquoted.</p>
<p>Apply the given regex and return a list of unicode strings with the
matches.</p>
<p><code class="docutils literal"><span class="pre">regex</span></code> can be either a compiled regular expression or a string which
will be compiled to a regular expression using <code class="docutils literal"><span class="pre">re.compile(regex)</span></code></p>
<p class="first admonition-title">Note</p>
<p class="last">Note that <code class="docutils literal"><span class="pre">re()</span></code> and <code class="docutils literal"><span class="pre">re_first()</span></code> both decode HTML entities (except <code class="docutils literal"><span class="pre">&amp;lt;</span></code> and <code class="docutils literal"><span class="pre">&amp;amp;</span></code>).</p>
<p>Register the given namespace to be used in this <a class="reference internal" href="#scrapy.selector.Selector" title="scrapy.selector.Selector"><code class="xref py py-class docutils literal"><span class="pre">Selector</span></code></a>.
Without registering namespaces you can’t select or extract data from
non-standard namespaces. See examples below.</p>
<p>Remove all namespaces, allowing to traverse the document using
namespace-less xpaths. See example below.</p>
<p>Returns <code class="docutils literal"><span class="pre">True</span></code> if there is any real content selected or <code class="docutils literal"><span class="pre">False</span></code>
otherwise.  In other words, the boolean value of a <a class="reference internal" href="#scrapy.selector.Selector" title="scrapy.selector.Selector"><code class="xref py py-class docutils literal"><span class="pre">Selector</span></code></a> is
given by the contents it selects.</p>
<p>The <a class="reference internal" href="#scrapy.selector.SelectorList" title="scrapy.selector.SelectorList"><code class="xref py py-class docutils literal"><span class="pre">SelectorList</span></code></a> class is a subclass of the builtin <code class="docutils literal"><span class="pre">list</span></code>
class, which provides a few additional methods.</p>
<p>Call the <code class="docutils literal"><span class="pre">.xpath()</span></code> method for each element in this list and return
their results flattened as another <a class="reference internal" href="#scrapy.selector.SelectorList" title="scrapy.selector.SelectorList"><code class="xref py py-class docutils literal"><span class="pre">SelectorList</span></code></a>.</p>
<p><code class="docutils literal"><span class="pre">query</span></code> is the same argument as the one in <a class="reference internal" href="#scrapy.selector.Selector.xpath" title="scrapy.selector.Selector.xpath"><code class="xref py py-meth docutils literal"><span class="pre">Selector.xpath()</span></code></a></p>
<p>Call the <code class="docutils literal"><span class="pre">.css()</span></code> method for each element in this list and return
their results flattened as another <a class="reference internal" href="#scrapy.selector.SelectorList" title="scrapy.selector.SelectorList"><code class="xref py py-class docutils literal"><span class="pre">SelectorList</span></code></a>.</p>
<p><code class="docutils literal"><span class="pre">query</span></code> is the same argument as the one in <a class="reference internal" href="#scrapy.selector.Selector.css" title="scrapy.selector.Selector.css"><code class="xref py py-meth docutils literal"><span class="pre">Selector.css()</span></code></a></p>
<p>Call the <code class="docutils literal"><span class="pre">.extract()</span></code> method for each element in this list and return
their results flattened, as a list of unicode strings.</p>
<p>Call the <code class="docutils literal"><span class="pre">.re()</span></code> method for each element in this list and return
their results flattened, as a list of unicode strings.</p>
<p>Here’s a couple of <a class="reference internal" href="#scrapy.selector.Selector" title="scrapy.selector.Selector"><code class="xref py py-class docutils literal"><span class="pre">Selector</span></code></a> examples to illustrate several concepts.
In all cases, we assume there is already a <a class="reference internal" href="#scrapy.selector.Selector" title="scrapy.selector.Selector"><code class="xref py py-class docutils literal"><span class="pre">Selector</span></code></a> instantiated with
a <a class="reference internal" href="request-response.html#scrapy.http.HtmlResponse" title="scrapy.http.HtmlResponse"><code class="xref py py-class docutils literal"><span class="pre">HtmlResponse</span></code></a> object like this:</p>
<p class="first">Select all <code class="docutils literal"><span class="pre">&lt;h1&gt;</span></code> elements from an HTML response body, returning a list of
<a class="reference internal" href="#scrapy.selector.Selector" title="scrapy.selector.Selector"><code class="xref py py-class docutils literal"><span class="pre">Selector</span></code></a> objects (ie. a <a class="reference internal" href="#scrapy.selector.SelectorList" title="scrapy.selector.SelectorList"><code class="xref py py-class docutils literal"><span class="pre">SelectorList</span></code></a> object):</p>
<p class="first">Extract the text of all <code class="docutils literal"><span class="pre">&lt;h1&gt;</span></code> elements from an HTML response body,
returning a list of unicode strings:</p>
<p class="first">Iterate over all <code class="docutils literal"><span class="pre">&lt;p&gt;</span></code> tags and print their class attribute:</p>
<p>Here’s a couple of examples to illustrate several concepts. In both cases we
assume there is already a <a class="reference internal" href="#scrapy.selector.Selector" title="scrapy.selector.Selector"><code class="xref py py-class docutils literal"><span class="pre">Selector</span></code></a> instantiated with an
<a class="reference internal" href="request-response.html#scrapy.http.XmlResponse" title="scrapy.http.XmlResponse"><code class="xref py py-class docutils literal"><span class="pre">XmlResponse</span></code></a> object like this:</p>
<p class="first">Select all <code class="docutils literal"><span class="pre">&lt;product&gt;</span></code> elements from an XML response body, returning a list
of <a class="reference internal" href="#scrapy.selector.Selector" title="scrapy.selector.Selector"><code class="xref py py-class docutils literal"><span class="pre">Selector</span></code></a> objects (ie. a <a class="reference internal" href="#scrapy.selector.SelectorList" title="scrapy.selector.SelectorList"><code class="xref py py-class docutils literal"><span class="pre">SelectorList</span></code></a> object):</p>
<p class="first">Extract all prices from a <a class="reference external" href="https://support.google.com/merchants/answer/160589?hl=en&amp;ref_topic=2473799">Google Base XML feed</a> which requires registering
a namespace:</p>
<p>When dealing with scraping projects, it is often quite convenient to get rid of
namespaces altogether and just work with element names, to write more
simple/convenient XPaths. You can use the
<a class="reference internal" href="#scrapy.selector.Selector.remove_namespaces" title="scrapy.selector.Selector.remove_namespaces"><code class="xref py py-meth docutils literal"><span class="pre">Selector.remove_namespaces()</span></code></a> method for that.</p>
<p>Let’s show an example that illustrates this with GitHub blog atom feed.</p>
<p>First, we open the shell with the url we want to scrape:</p>
<p>Once in the shell we can try selecting all <code class="docutils literal"><span class="pre">&lt;link&gt;</span></code> objects and see that it
doesn’t work (because the Atom XML namespace is obfuscating those nodes):</p>
<p>But once we call the <a class="reference internal" href="#scrapy.selector.Selector.remove_namespaces" title="scrapy.selector.Selector.remove_namespaces"><code class="xref py py-meth docutils literal"><span class="pre">Selector.remove_namespaces()</span></code></a> method, all
nodes can be accessed directly by their names:</p>
<p>If you wonder why the namespace removal procedure isn’t always called by default
instead of having to call it manually, this is because of two reasons, which, in order
of relevance, are:</p>
<p>
        © Copyright 2008-2016, Scrapy developers.
      
        <span class="commit">
          Revision <code>5aebdac4</code>.
        </span>
</p>
<p class="caption"><span class="caption-text">First steps</span></p>
<p class="caption"><span class="caption-text">Basic concepts</span></p>
<p class="caption"><span class="caption-text">Built-in services</span></p>
<p class="caption"><span class="caption-text">Solving specific problems</span></p>
<p class="caption"><span class="caption-text">Extending Scrapy</span></p>
<p class="caption"><span class="caption-text">All the rest</span></p>
<p>The main goal in scraping is to extract structured data from unstructured
sources, typically, web pages. Scrapy spiders can return the extracted data
as Python dicts. While convenient and familiar, Python dicts lack structure:
it is easy to make a typo in a field name or return inconsistent data,
especially in a larger project with many spiders.</p>
<p>To define common output data format Scrapy provides the <a class="reference internal" href="#scrapy.item.Item" title="scrapy.item.Item"><code class="xref py py-class docutils literal"><span class="pre">Item</span></code></a> class.
<a class="reference internal" href="#scrapy.item.Item" title="scrapy.item.Item"><code class="xref py py-class docutils literal"><span class="pre">Item</span></code></a> objects are simple containers used to collect the scraped data.
They provide a <a class="reference external" href="https://docs.python.org/2/library/stdtypes.html#dict">dictionary-like</a> API with a convenient syntax for declaring
their available fields.</p>
<p>Various Scrapy components use extra information provided by Items:
exporters look at declared fields to figure out columns to export,
serialization can be customized using Item fields metadata, <code class="xref py py-mod docutils literal"><span class="pre">trackref</span></code>
tracks Item instances to help finding memory leaks
(see <a class="reference internal" href="leaks.html#topics-leaks-trackrefs"><span class="std std-ref">Debugging memory leaks with trackref</span></a>), etc.</p>
<p>Items are declared using a simple class definition syntax and <a class="reference internal" href="#scrapy.item.Field" title="scrapy.item.Field"><code class="xref py py-class docutils literal"><span class="pre">Field</span></code></a>
objects. Here is an example:</p>
<p class="first admonition-title">Note</p>
<p class="last">Those familiar with <a class="reference external" href="https://www.djangoproject.com/">Django</a> will notice that Scrapy Items are
declared similar to <a class="reference external" href="https://docs.djangoproject.com/en/dev/topics/db/models/">Django Models</a>, except that Scrapy Items are much
simpler as there is no concept of different field types.</p>
<p><a class="reference internal" href="#scrapy.item.Field" title="scrapy.item.Field"><code class="xref py py-class docutils literal"><span class="pre">Field</span></code></a> objects are used to specify metadata for each field. For
example, the serializer function for the <code class="docutils literal"><span class="pre">last_updated</span></code> field illustrated in
the example above.</p>
<p>You can specify any kind of metadata for each field. There is no restriction on
the values accepted by <a class="reference internal" href="#scrapy.item.Field" title="scrapy.item.Field"><code class="xref py py-class docutils literal"><span class="pre">Field</span></code></a> objects. For this same
reason, there is no reference list of all available metadata keys. Each key
defined in <a class="reference internal" href="#scrapy.item.Field" title="scrapy.item.Field"><code class="xref py py-class docutils literal"><span class="pre">Field</span></code></a> objects could be used by a different component, and
only those components know about it. You can also define and use any other
<a class="reference internal" href="#scrapy.item.Field" title="scrapy.item.Field"><code class="xref py py-class docutils literal"><span class="pre">Field</span></code></a> key in your project too, for your own needs. The main goal of
<a class="reference internal" href="#scrapy.item.Field" title="scrapy.item.Field"><code class="xref py py-class docutils literal"><span class="pre">Field</span></code></a> objects is to provide a way to define all field metadata in one
place. Typically, those components whose behaviour depends on each field use
certain field keys to configure that behaviour. You must refer to their
documentation to see which metadata keys are used by each component.</p>
<p>It’s important to note that the <a class="reference internal" href="#scrapy.item.Field" title="scrapy.item.Field"><code class="xref py py-class docutils literal"><span class="pre">Field</span></code></a> objects used to declare the item
do not stay assigned as class attributes. Instead, they can be accessed through
the <a class="reference internal" href="#scrapy.item.Item.fields" title="scrapy.item.Item.fields"><code class="xref py py-attr docutils literal"><span class="pre">Item.fields</span></code></a> attribute.</p>
<p>Here are some examples of common tasks performed with items, using the
<code class="docutils literal"><span class="pre">Product</span></code> item <a class="reference internal" href="#topics-items-declaring"><span class="std std-ref">declared above</span></a>. You will
notice the API is very similar to the <a class="reference external" href="https://docs.python.org/2/library/stdtypes.html#dict">dict API</a>.</p>
<p>To access all populated values, just use the typical <a class="reference external" href="https://docs.python.org/2/library/stdtypes.html#dict">dict API</a>:</p>
<p>Copying items:</p>
<p>Creating dicts from items:</p>
<p>Creating items from dicts:</p>
<p>You can extend Items (to add more fields or to change some metadata for some
fields) by declaring a subclass of your original Item.</p>
<p>For example:</p>
<p>You can also extend field metadata by using the previous field metadata and
appending more values, or changing existing values, like this:</p>
<p>That adds (or replaces) the <code class="docutils literal"><span class="pre">serializer</span></code> metadata key for the <code class="docutils literal"><span class="pre">name</span></code> field,
keeping all the previously existing metadata values.</p>
<p>Return a new Item optionally initialized from the given argument.</p>
<p>Items replicate the standard <a class="reference external" href="https://docs.python.org/2/library/stdtypes.html#dict">dict API</a>, including its constructor. The
only additional attribute provided by Items is:</p>
<p>A dictionary containing <em>all declared fields</em> for this Item, not only
those populated. The keys are the field names and the values are the
<a class="reference internal" href="#scrapy.item.Field" title="scrapy.item.Field"><code class="xref py py-class docutils literal"><span class="pre">Field</span></code></a> objects used in the <a class="reference internal" href="#topics-items-declaring"><span class="std std-ref">Item declaration</span></a>.</p>
<p>The <a class="reference internal" href="#scrapy.item.Field" title="scrapy.item.Field"><code class="xref py py-class docutils literal"><span class="pre">Field</span></code></a> class is just an alias to the built-in <a class="reference external" href="https://docs.python.org/2/library/stdtypes.html#dict">dict</a> class and
doesn’t provide any extra functionality or attributes. In other words,
<a class="reference internal" href="#scrapy.item.Field" title="scrapy.item.Field"><code class="xref py py-class docutils literal"><span class="pre">Field</span></code></a> objects are plain-old Python dicts. A separate class is used
to support the <a class="reference internal" href="#topics-items-declaring"><span class="std std-ref">item declaration syntax</span></a>
based on class attributes.</p>
<p>
        © Copyright 2008-2016, Scrapy developers.
      
        <span class="commit">
          Revision <code>5aebdac4</code>.
        </span>
</p>
<p class="caption"><span class="caption-text">First steps</span></p>
<p class="caption"><span class="caption-text">Basic concepts</span></p>
<p class="caption"><span class="caption-text">Built-in services</span></p>
<p class="caption"><span class="caption-text">Solving specific problems</span></p>
<p class="caption"><span class="caption-text">Extending Scrapy</span></p>
<p class="caption"><span class="caption-text">All the rest</span></p>
<p>Item Loaders provide a convenient mechanism for populating scraped <a class="reference internal" href="items.html#topics-items"><span class="std std-ref">Items</span></a>. Even though Items can be populated using their own
dictionary-like API, Item Loaders provide a much more convenient API for
populating them from a scraping process, by automating some common tasks like
parsing the raw extracted data before assigning it.</p>
<p>In other words, <a class="reference internal" href="items.html#topics-items"><span class="std std-ref">Items</span></a> provide the <em>container</em> of
scraped data, while Item Loaders provide the mechanism for <em>populating</em> that
container.</p>
<p>Item Loaders are designed to provide a flexible, efficient and easy mechanism
for extending and overriding different field parsing rules, either by spider,
or by source format (HTML, XML, etc) without becoming a nightmare to maintain.</p>
<p>To use an Item Loader, you must first instantiate it. You can either
instantiate it with a dict-like object (e.g. Item or dict) or without one, in
which case an Item is automatically instantiated in the Item Loader constructor
using the Item class specified in the <a class="reference internal" href="#scrapy.loader.ItemLoader.default_item_class" title="scrapy.loader.ItemLoader.default_item_class"><code class="xref py py-attr docutils literal"><span class="pre">ItemLoader.default_item_class</span></code></a>
attribute.</p>
<p>Then, you start collecting values into the Item Loader, typically using
<a class="reference internal" href="selectors.html#topics-selectors"><span class="std std-ref">Selectors</span></a>. You can add more than one value to
the same item field; the Item Loader will know how to “join” those values later
using a proper processing function.</p>
<p>Here is a typical Item Loader usage in a <a class="reference internal" href="spiders.html#topics-spiders"><span class="std std-ref">Spider</span></a>, using
the <a class="reference internal" href="items.html#topics-items-declaring"><span class="std std-ref">Product item</span></a> declared in the <a class="reference internal" href="items.html#topics-items"><span class="std std-ref">Items
chapter</span></a>:</p>
<p>By quickly looking at that code, we can see the <code class="docutils literal"><span class="pre">name</span></code> field is being
extracted from two different XPath locations in the page:</p>
<p>In other words, data is being collected by extracting it from two XPath
locations, using the <a class="reference internal" href="#scrapy.loader.ItemLoader.add_xpath" title="scrapy.loader.ItemLoader.add_xpath"><code class="xref py py-meth docutils literal"><span class="pre">add_xpath()</span></code></a> method. This is the
data that will be assigned to the <code class="docutils literal"><span class="pre">name</span></code> field later.</p>
<p>Afterwards, similar calls are used for <code class="docutils literal"><span class="pre">price</span></code> and <code class="docutils literal"><span class="pre">stock</span></code> fields
(the latter using a CSS selector with the <a class="reference internal" href="#scrapy.loader.ItemLoader.add_css" title="scrapy.loader.ItemLoader.add_css"><code class="xref py py-meth docutils literal"><span class="pre">add_css()</span></code></a> method),
and finally the <code class="docutils literal"><span class="pre">last_update</span></code> field is populated directly with a literal value
(<code class="docutils literal"><span class="pre">today</span></code>) using a different method: <a class="reference internal" href="#scrapy.loader.ItemLoader.add_value" title="scrapy.loader.ItemLoader.add_value"><code class="xref py py-meth docutils literal"><span class="pre">add_value()</span></code></a>.</p>
<p>Finally, when all data is collected, the <a class="reference internal" href="#scrapy.loader.ItemLoader.load_item" title="scrapy.loader.ItemLoader.load_item"><code class="xref py py-meth docutils literal"><span class="pre">ItemLoader.load_item()</span></code></a> method is
called which actually returns the item populated with the data
previously extracted and collected with the <a class="reference internal" href="#scrapy.loader.ItemLoader.add_xpath" title="scrapy.loader.ItemLoader.add_xpath"><code class="xref py py-meth docutils literal"><span class="pre">add_xpath()</span></code></a>,
<a class="reference internal" href="#scrapy.loader.ItemLoader.add_css" title="scrapy.loader.ItemLoader.add_css"><code class="xref py py-meth docutils literal"><span class="pre">add_css()</span></code></a>, and <a class="reference internal" href="#scrapy.loader.ItemLoader.add_value" title="scrapy.loader.ItemLoader.add_value"><code class="xref py py-meth docutils literal"><span class="pre">add_value()</span></code></a> calls.</p>
<p>An Item Loader contains one input processor and one output processor for each
(item) field. The input processor processes the extracted data as soon as it’s
received (through the <a class="reference internal" href="#scrapy.loader.ItemLoader.add_xpath" title="scrapy.loader.ItemLoader.add_xpath"><code class="xref py py-meth docutils literal"><span class="pre">add_xpath()</span></code></a>, <a class="reference internal" href="#scrapy.loader.ItemLoader.add_css" title="scrapy.loader.ItemLoader.add_css"><code class="xref py py-meth docutils literal"><span class="pre">add_css()</span></code></a> or
<a class="reference internal" href="#scrapy.loader.ItemLoader.add_value" title="scrapy.loader.ItemLoader.add_value"><code class="xref py py-meth docutils literal"><span class="pre">add_value()</span></code></a> methods) and the result of the input processor is
collected and kept inside the ItemLoader. After collecting all data, the
<a class="reference internal" href="#scrapy.loader.ItemLoader.load_item" title="scrapy.loader.ItemLoader.load_item"><code class="xref py py-meth docutils literal"><span class="pre">ItemLoader.load_item()</span></code></a> method is called to populate and get the populated
<a class="reference internal" href="items.html#scrapy.item.Item" title="scrapy.item.Item"><code class="xref py py-class docutils literal"><span class="pre">Item</span></code></a> object.  That’s when the output processor is
called with the data previously collected (and processed using the input
processor). The result of the output processor is the final value that gets
assigned to the item.</p>
<p>Let’s see an example to illustrate how the input and output processors are
called for a particular field (the same applies for any other field):</p>
<p>So what happens is:</p>
<p>It’s worth noticing that processors are just callable objects, which are called
with the data to be parsed, and return a parsed value. So you can use any
function as input or output processor. The only requirement is that they must
accept one (and only one) positional argument, which will be an iterator.</p>
<p class="first admonition-title">Note</p>
<p class="last">Both input and output processors must receive an iterator as their
first argument. The output of those functions can be anything. The result of
input processors will be appended to an internal list (in the Loader)
containing the collected values (for that field). The result of the output
processors is the value that will be finally assigned to the item.</p>
<p>The other thing you need to keep in mind is that the values returned by input
processors are collected internally (in lists) and then passed to output
processors to populate the fields.</p>
<p>Last, but not least, Scrapy comes with some <a class="reference internal" href="#topics-loaders-available-processors"><span class="std std-ref">commonly used processors</span></a> built-in for convenience.</p>
<p>Item Loaders are declared like Items, by using a class definition syntax. Here
is an example:</p>
<p>As you can see, input processors are declared using the <code class="docutils literal"><span class="pre">_in</span></code> suffix while
output processors are declared using the <code class="docutils literal"><span class="pre">_out</span></code> suffix. And you can also
declare a default input/output processors using the
<a class="reference internal" href="#scrapy.loader.ItemLoader.default_input_processor" title="scrapy.loader.ItemLoader.default_input_processor"><code class="xref py py-attr docutils literal"><span class="pre">ItemLoader.default_input_processor</span></code></a> and
<a class="reference internal" href="#scrapy.loader.ItemLoader.default_output_processor" title="scrapy.loader.ItemLoader.default_output_processor"><code class="xref py py-attr docutils literal"><span class="pre">ItemLoader.default_output_processor</span></code></a> attributes.</p>
<p>As seen in the previous section, input and output processors can be declared in
the Item Loader definition, and it’s very common to declare input processors
this way. However, there is one more place where you can specify the input and
output processors to use: in the <a class="reference internal" href="items.html#topics-items-fields"><span class="std std-ref">Item Field</span></a>
metadata. Here is an example:</p>
<p>The precedence order, for both input and output processors, is as follows:</p>
<p>See also: <a class="reference internal" href="#topics-loaders-extending"><span class="std std-ref">Reusing and extending Item Loaders</span></a>.</p>
<p>The Item Loader Context is a dict of arbitrary key/values which is shared among
all input and output processors in the Item Loader. It can be passed when
declaring, instantiating or using Item Loader. They are used to modify the
behaviour of the input/output processors.</p>
<p>For example, suppose you have a function <code class="docutils literal"><span class="pre">parse_length</span></code> which receives a text
value and extracts a length from it:</p>
<p>By accepting a <code class="docutils literal"><span class="pre">loader_context</span></code> argument the function is explicitly telling
the Item Loader that it’s able to receive an Item Loader context, so the Item
Loader passes the currently active context when calling it, and the processor
function (<code class="docutils literal"><span class="pre">parse_length</span></code> in this case) can thus use them.</p>
<p>There are several ways to modify Item Loader context values:</p>
<p class="first">By modifying the currently active Item Loader context
(<a class="reference internal" href="#scrapy.loader.ItemLoader.context" title="scrapy.loader.ItemLoader.context"><code class="xref py py-attr docutils literal"><span class="pre">context</span></code></a> attribute):</p>
<p class="first">On Item Loader instantiation (the keyword arguments of Item Loader
constructor are stored in the Item Loader context):</p>
<p class="first">On Item Loader declaration, for those input/output processors that support
instantiating them with an Item Loader context. <code class="xref py py-class docutils literal"><span class="pre">MapCompose</span></code> is one of
them:</p>
<p>Return a new Item Loader for populating the given Item. If no item is
given, one is instantiated automatically using the class in
<a class="reference internal" href="#scrapy.loader.ItemLoader.default_item_class" title="scrapy.loader.ItemLoader.default_item_class"><code class="xref py py-attr docutils literal"><span class="pre">default_item_class</span></code></a>.</p>
<p>When instantiated with a <cite>selector</cite> or a <cite>response</cite> parameters
the <a class="reference internal" href="#scrapy.loader.ItemLoader" title="scrapy.loader.ItemLoader"><code class="xref py py-class docutils literal"><span class="pre">ItemLoader</span></code></a> class provides convenient mechanisms for extracting
data from web pages using <a class="reference internal" href="selectors.html#topics-selectors"><span class="std std-ref">selectors</span></a>.</p>
<p>The item, selector, response and the remaining keyword arguments are
assigned to the Loader context (accessible through the <a class="reference internal" href="#scrapy.loader.ItemLoader.context" title="scrapy.loader.ItemLoader.context"><code class="xref py py-attr docutils literal"><span class="pre">context</span></code></a> attribute).</p>
<p><a class="reference internal" href="#scrapy.loader.ItemLoader" title="scrapy.loader.ItemLoader"><code class="xref py py-class docutils literal"><span class="pre">ItemLoader</span></code></a> instances have the following methods:</p>
<p>Process the given <code class="docutils literal"><span class="pre">value</span></code> by the given <code class="docutils literal"><span class="pre">processors</span></code> and keyword
arguments.</p>
<p>Available keyword arguments:</p>
<p>Examples:</p>
<p>Process and then add the given <code class="docutils literal"><span class="pre">value</span></code> for the given field.</p>
<p>The value is first passed through <a class="reference internal" href="#scrapy.loader.ItemLoader.get_value" title="scrapy.loader.ItemLoader.get_value"><code class="xref py py-meth docutils literal"><span class="pre">get_value()</span></code></a> by giving the
<code class="docutils literal"><span class="pre">processors</span></code> and <code class="docutils literal"><span class="pre">kwargs</span></code>, and then passed through the
<a class="reference internal" href="#topics-loaders-processors"><span class="std std-ref">field input processor</span></a> and its result
appended to the data collected for that field. If the field already
contains collected data, the new data is added.</p>
<p>The given <code class="docutils literal"><span class="pre">field_name</span></code> can be <code class="docutils literal"><span class="pre">None</span></code>, in which case values for
multiple fields may be added. And the processed value should be a dict
with field_name mapped to values.</p>
<p>Examples:</p>
<p>Similar to <a class="reference internal" href="#scrapy.loader.ItemLoader.add_value" title="scrapy.loader.ItemLoader.add_value"><code class="xref py py-meth docutils literal"><span class="pre">add_value()</span></code></a> but replaces the collected data with the
new value instead of adding it.</p>
<p>Similar to <a class="reference internal" href="#scrapy.loader.ItemLoader.get_value" title="scrapy.loader.ItemLoader.get_value"><code class="xref py py-meth docutils literal"><span class="pre">ItemLoader.get_value()</span></code></a> but receives an XPath instead of a
value, which is used to extract a list of unicode strings from the
selector associated with this <a class="reference internal" href="#scrapy.loader.ItemLoader" title="scrapy.loader.ItemLoader"><code class="xref py py-class docutils literal"><span class="pre">ItemLoader</span></code></a>.</p>
<p>Examples:</p>
<p>Similar to <a class="reference internal" href="#scrapy.loader.ItemLoader.add_value" title="scrapy.loader.ItemLoader.add_value"><code class="xref py py-meth docutils literal"><span class="pre">ItemLoader.add_value()</span></code></a> but receives an XPath instead of a
value, which is used to extract a list of unicode strings from the
selector associated with this <a class="reference internal" href="#scrapy.loader.ItemLoader" title="scrapy.loader.ItemLoader"><code class="xref py py-class docutils literal"><span class="pre">ItemLoader</span></code></a>.</p>
<p>See <a class="reference internal" href="#scrapy.loader.ItemLoader.get_xpath" title="scrapy.loader.ItemLoader.get_xpath"><code class="xref py py-meth docutils literal"><span class="pre">get_xpath()</span></code></a> for <code class="docutils literal"><span class="pre">kwargs</span></code>.</p>
<p>Examples:</p>
<p>Similar to <a class="reference internal" href="#scrapy.loader.ItemLoader.add_xpath" title="scrapy.loader.ItemLoader.add_xpath"><code class="xref py py-meth docutils literal"><span class="pre">add_xpath()</span></code></a> but replaces collected data instead of
adding it.</p>
<p>Similar to <a class="reference internal" href="#scrapy.loader.ItemLoader.get_value" title="scrapy.loader.ItemLoader.get_value"><code class="xref py py-meth docutils literal"><span class="pre">ItemLoader.get_value()</span></code></a> but receives a CSS selector
instead of a value, which is used to extract a list of unicode strings
from the selector associated with this <a class="reference internal" href="#scrapy.loader.ItemLoader" title="scrapy.loader.ItemLoader"><code class="xref py py-class docutils literal"><span class="pre">ItemLoader</span></code></a>.</p>
<p>Examples:</p>
<p>Similar to <a class="reference internal" href="#scrapy.loader.ItemLoader.add_value" title="scrapy.loader.ItemLoader.add_value"><code class="xref py py-meth docutils literal"><span class="pre">ItemLoader.add_value()</span></code></a> but receives a CSS selector
instead of a value, which is used to extract a list of unicode strings
from the selector associated with this <a class="reference internal" href="#scrapy.loader.ItemLoader" title="scrapy.loader.ItemLoader"><code class="xref py py-class docutils literal"><span class="pre">ItemLoader</span></code></a>.</p>
<p>See <a class="reference internal" href="#scrapy.loader.ItemLoader.get_css" title="scrapy.loader.ItemLoader.get_css"><code class="xref py py-meth docutils literal"><span class="pre">get_css()</span></code></a> for <code class="docutils literal"><span class="pre">kwargs</span></code>.</p>
<p>Examples:</p>
<p>Similar to <a class="reference internal" href="#scrapy.loader.ItemLoader.add_css" title="scrapy.loader.ItemLoader.add_css"><code class="xref py py-meth docutils literal"><span class="pre">add_css()</span></code></a> but replaces collected data instead of
adding it.</p>
<p>Populate the item with the data collected so far, and return it. The
data collected is first passed through the <a class="reference internal" href="#topics-loaders-processors"><span class="std std-ref">output processors</span></a> to get the final value to assign to each
item field.</p>
<p>Create a nested loader with an xpath selector.
The supplied selector is applied relative to selector associated
with this <a class="reference internal" href="#scrapy.loader.ItemLoader" title="scrapy.loader.ItemLoader"><code class="xref py py-class docutils literal"><span class="pre">ItemLoader</span></code></a>. The nested loader shares the <code class="xref py py-class docutils literal"><span class="pre">Item</span></code>
with the parent <a class="reference internal" href="#scrapy.loader.ItemLoader" title="scrapy.loader.ItemLoader"><code class="xref py py-class docutils literal"><span class="pre">ItemLoader</span></code></a> so calls to <a class="reference internal" href="#scrapy.loader.ItemLoader.add_xpath" title="scrapy.loader.ItemLoader.add_xpath"><code class="xref py py-meth docutils literal"><span class="pre">add_xpath()</span></code></a>,
<a class="reference internal" href="#scrapy.loader.ItemLoader.add_value" title="scrapy.loader.ItemLoader.add_value"><code class="xref py py-meth docutils literal"><span class="pre">add_value()</span></code></a>, <a class="reference internal" href="#scrapy.loader.ItemLoader.replace_value" title="scrapy.loader.ItemLoader.replace_value"><code class="xref py py-meth docutils literal"><span class="pre">replace_value()</span></code></a>, etc. will behave as expected.</p>
<p>Create a nested loader with a css selector.
The supplied selector is applied relative to selector associated
with this <a class="reference internal" href="#scrapy.loader.ItemLoader" title="scrapy.loader.ItemLoader"><code class="xref py py-class docutils literal"><span class="pre">ItemLoader</span></code></a>. The nested loader shares the <code class="xref py py-class docutils literal"><span class="pre">Item</span></code>
with the parent <a class="reference internal" href="#scrapy.loader.ItemLoader" title="scrapy.loader.ItemLoader"><code class="xref py py-class docutils literal"><span class="pre">ItemLoader</span></code></a> so calls to <a class="reference internal" href="#scrapy.loader.ItemLoader.add_xpath" title="scrapy.loader.ItemLoader.add_xpath"><code class="xref py py-meth docutils literal"><span class="pre">add_xpath()</span></code></a>,
<a class="reference internal" href="#scrapy.loader.ItemLoader.add_value" title="scrapy.loader.ItemLoader.add_value"><code class="xref py py-meth docutils literal"><span class="pre">add_value()</span></code></a>, <a class="reference internal" href="#scrapy.loader.ItemLoader.replace_value" title="scrapy.loader.ItemLoader.replace_value"><code class="xref py py-meth docutils literal"><span class="pre">replace_value()</span></code></a>, etc. will behave as expected.</p>
<p>Return the collected values for the given field.</p>
<p>Return the collected values parsed using the output processor, for the
given field. This method doesn’t populate or modify the item at all.</p>
<p>Return the input processor for the given field.</p>
<p>Return the output processor for the given field.</p>
<p><a class="reference internal" href="#scrapy.loader.ItemLoader" title="scrapy.loader.ItemLoader"><code class="xref py py-class docutils literal"><span class="pre">ItemLoader</span></code></a> instances have the following attributes:</p>
<p>The <a class="reference internal" href="items.html#scrapy.item.Item" title="scrapy.item.Item"><code class="xref py py-class docutils literal"><span class="pre">Item</span></code></a> object being parsed by this Item Loader.</p>
<p>The currently active <a class="reference internal" href="#topics-loaders-context"><span class="std std-ref">Context</span></a> of this
Item Loader.</p>
<p>An Item class (or factory), used to instantiate items when not given in
the constructor.</p>
<p>The default input processor to use for those fields which don’t specify
one.</p>
<p>The default output processor to use for those fields which don’t specify
one.</p>
<p>The class used to construct the <a class="reference internal" href="#scrapy.loader.ItemLoader.selector" title="scrapy.loader.ItemLoader.selector"><code class="xref py py-attr docutils literal"><span class="pre">selector</span></code></a> of this
<a class="reference internal" href="#scrapy.loader.ItemLoader" title="scrapy.loader.ItemLoader"><code class="xref py py-class docutils literal"><span class="pre">ItemLoader</span></code></a>, if only a response is given in the constructor.
If a selector is given in the constructor this attribute is ignored.
This attribute is sometimes overridden in subclasses.</p>
<p>The <a class="reference internal" href="selectors.html#scrapy.selector.Selector" title="scrapy.selector.Selector"><code class="xref py py-class docutils literal"><span class="pre">Selector</span></code></a> object to extract data from.
It’s either the selector given in the constructor or one created from
the response given in the constructor using the
<a class="reference internal" href="#scrapy.loader.ItemLoader.default_selector_class" title="scrapy.loader.ItemLoader.default_selector_class"><code class="xref py py-attr docutils literal"><span class="pre">default_selector_class</span></code></a>. This attribute is meant to be
read-only.</p>
<p>When parsing related values from a subsection of a document, it can be
useful to create nested loaders.  Imagine you’re extracting details from
a footer of a page that looks something like:</p>
<p>Example:</p>
<p>Without nested loaders, you need to specify the full xpath (or css) for each value
that you wish to extract.</p>
<p>Example:</p>
<p>Instead, you can create a nested loader with the footer selector and add values
relative to the footer.  The functionality is the same but you avoid repeating
the footer selector.</p>
<p>Example:</p>
<p>You can nest loaders arbitrarily and they work with either xpath or css selectors.
As a general guideline, use nested loaders when they make your code simpler but do
not go overboard with nesting or your parser can become difficult to read.</p>
<p>As your project grows bigger and acquires more and more spiders, maintenance
becomes a fundamental problem, especially when you have to deal with many
different parsing rules for each spider, having a lot of exceptions, but also
wanting to reuse the common processors.</p>
<p>Item Loaders are designed to ease the maintenance burden of parsing rules,
without losing flexibility and, at the same time, providing a convenient
mechanism for extending and overriding them. For this reason Item Loaders
support traditional Python class inheritance for dealing with differences of
specific spiders (or groups of spiders).</p>
<p>Suppose, for example, that some particular site encloses their product names in
three dashes (e.g. <code class="docutils literal"><span class="pre">---Plasma</span> <span class="pre">TV---</span></code>) and you don’t want to end up scraping
those dashes in the final product names.</p>
<p>Here’s how you can remove those dashes by reusing and extending the default
Product Item Loader (<code class="docutils literal"><span class="pre">ProductLoader</span></code>):</p>
<p>Another case where extending Item Loaders can be very helpful is when you have
multiple source formats, for example XML and HTML. In the XML version you may
want to remove <code class="docutils literal"><span class="pre">CDATA</span></code> occurrences. Here’s an example of how to do it:</p>
<p>And that’s how you typically extend input processors.</p>
<p>As for output processors, it is more common to declare them in the field metadata,
as they usually depend only on the field and not on each specific site parsing
rule (as input processors do). See also:
<a class="reference internal" href="#topics-loaders-processors-declaring"><span class="std std-ref">Declaring Input and Output Processors</span></a>.</p>
<p>There are many other possible ways to extend, inherit and override your Item
Loaders, and different Item Loaders hierarchies may fit better for different
projects. Scrapy only provides the mechanism; it doesn’t impose any specific
organization of your Loaders collection - that’s up to you and your project’s
needs.</p>
<p>Even though you can use any callable function as input and output processors,
Scrapy provides some commonly used processors, which are described below. Some
of them, like the <a class="reference internal" href="#scrapy.loader.processors.MapCompose" title="scrapy.loader.processors.MapCompose"><code class="xref py py-class docutils literal"><span class="pre">MapCompose</span></code></a> (which is typically used as input
processor) compose the output of several functions executed in order, to
produce the final parsed value.</p>
<p>Here is a list of all built-in processors:</p>
<p>The simplest processor, which doesn’t do anything. It returns the original
values unchanged. It doesn’t receive any constructor arguments, nor does it
accept Loader contexts.</p>
<p>Example:</p>
<p>Returns the first non-null/non-empty value from the values received,
so it’s typically used as an output processor to single-valued fields.
It doesn’t receive any constructor arguments, nor does it accept Loader contexts.</p>
<p>Example:</p>
<p>Returns the values joined with the separator given in the constructor, which
defaults to <code class="docutils literal"><span class="pre">u'</span> <span class="pre">'</span></code>. It doesn’t accept Loader contexts.</p>
<p>When using the default separator, this processor is equivalent to the
function: <code class="docutils literal"><span class="pre">u'</span> <span class="pre">'.join</span></code></p>
<p>Examples:</p>
<p>A processor which is constructed from the composition of the given
functions. This means that each input value of this processor is passed to
the first function, and the result of that function is passed to the second
function, and so on, until the last function returns the output value of
this processor.</p>
<p>By default, stop process on <code class="docutils literal"><span class="pre">None</span></code> value. This behaviour can be changed by
passing keyword argument <code class="docutils literal"><span class="pre">stop_on_none=False</span></code>.</p>
<p>Example:</p>
<p>Each function can optionally receive a <code class="docutils literal"><span class="pre">loader_context</span></code> parameter. For
those which do, this processor will pass the currently active <a class="reference internal" href="#topics-loaders-context"><span class="std std-ref">Loader
context</span></a> through that parameter.</p>
<p>The keyword arguments passed in the constructor are used as the default
Loader context values passed to each function call. However, the final
Loader context values passed to functions are overridden with the currently
active Loader context accessible through the <code class="xref py py-meth docutils literal"><span class="pre">ItemLoader.context()</span></code>
attribute.</p>
<p>A processor which is constructed from the composition of the given
functions, similar to the <a class="reference internal" href="#scrapy.loader.processors.Compose" title="scrapy.loader.processors.Compose"><code class="xref py py-class docutils literal"><span class="pre">Compose</span></code></a> processor. The difference with
this processor is the way internal results are passed among functions,
which is as follows:</p>
<p>The input value of this processor is <em>iterated</em> and the first function is
applied to each element. The results of these function calls (one for each element)
are concatenated to construct a new iterable, which is then used to apply the
second function, and so on, until the last function is applied to each
value of the list of values collected so far. The output values of the last
function are concatenated together to produce the output of this processor.</p>
<p>Each particular function can return a value or a list of values, which is
flattened with the list of values returned by the same function applied to
the other input values. The functions can also return <code class="docutils literal"><span class="pre">None</span></code> in which
case the output of that function is ignored for further processing over the
chain.</p>
<p>This processor provides a convenient way to compose functions that only
work with single values (instead of iterables). For this reason the
<a class="reference internal" href="#scrapy.loader.processors.MapCompose" title="scrapy.loader.processors.MapCompose"><code class="xref py py-class docutils literal"><span class="pre">MapCompose</span></code></a> processor is typically used as input processor, since
data is often extracted using the
<a class="reference internal" href="selectors.html#scrapy.selector.Selector.extract" title="scrapy.selector.Selector.extract"><code class="xref py py-meth docutils literal"><span class="pre">extract()</span></code></a> method of <a class="reference internal" href="selectors.html#topics-selectors"><span class="std std-ref">selectors</span></a>, which returns a list of unicode strings.</p>
<p>The example below should clarify how it works:</p>
<p>As with the Compose processor, functions can receive Loader contexts, and
constructor keyword arguments are used as default context values. See
<a class="reference internal" href="#scrapy.loader.processors.Compose" title="scrapy.loader.processors.Compose"><code class="xref py py-class docutils literal"><span class="pre">Compose</span></code></a> processor for more info.</p>
<p>Queries the value using the json path provided to the constructor and returns the output.
Requires jmespath (<a class="reference external" href="https://github.com/jmespath/jmespath.py">https://github.com/jmespath/jmespath.py</a>) to run.
This processor takes only one input at a time.</p>
<p>Example:</p>
<p>Working with Json:</p>
<p>
        © Copyright 2008-2016, Scrapy developers.
      
        <span class="commit">
          Revision <code>5aebdac4</code>.
        </span>
</p>
<p class="caption"><span class="caption-text">First steps</span></p>
<p class="caption"><span class="caption-text">Basic concepts</span></p>
<p class="caption"><span class="caption-text">Built-in services</span></p>
<p class="caption"><span class="caption-text">Solving specific problems</span></p>
<p class="caption"><span class="caption-text">Extending Scrapy</span></p>
<p class="caption"><span class="caption-text">All the rest</span></p>
<p>The Scrapy shell is an interactive shell where you can try and debug your
scraping code very quickly, without having to run the spider. It’s meant to be
used for testing data extraction code, but you can actually use it for testing
any kind of code as it is also a regular Python shell.</p>
<p>The shell is used for testing XPath or CSS expressions and see how they work
and what data they extract from the web pages you’re trying to scrape. It
allows you to interactively test your expressions while you’re writing your
spider, without having to run the spider to test every change.</p>
<p>Once you get familiarized with the Scrapy shell, you’ll see that it’s an
invaluable tool for developing and debugging your spiders.</p>
<p>If you have <a class="reference external" href="http://ipython.org/">IPython</a> installed, the Scrapy shell will use it (instead of the
standard Python console). The <a class="reference external" href="http://ipython.org/">IPython</a> console is much more powerful and
provides smart auto-completion and colorized output, among other things.</p>
<p>We highly recommend you install <a class="reference external" href="http://ipython.org/">IPython</a>, specially if you’re working on
Unix systems (where <a class="reference external" href="http://ipython.org/">IPython</a> excels). See the <a class="reference external" href="http://ipython.org/install.html">IPython installation guide</a>
for more info.</p>
<p>Scrapy also has support for <a class="reference external" href="http://www.bpython-interpreter.org/">bpython</a>, and will try to use it where <a class="reference external" href="http://ipython.org/">IPython</a>
is unavailable.</p>
<p>Through scrapy’s settings you can configure it to use any one of
<code class="docutils literal"><span class="pre">ipython</span></code>, <code class="docutils literal"><span class="pre">bpython</span></code> or the standard <code class="docutils literal"><span class="pre">python</span></code> shell, regardless of which
are installed. This is done by setting the <code class="docutils literal"><span class="pre">SCRAPY_PYTHON_SHELL</span></code> environment
variable; or by defining it in your <a class="reference internal" href="commands.html#topics-config-settings"><span class="std std-ref">scrapy.cfg</span></a>:</p>
<p>To launch the Scrapy shell you can use the <a class="reference internal" href="commands.html#std:command-shell"><code class="xref std std-command docutils literal"><span class="pre">shell</span></code></a> command like
this:</p>
<p>Where the <code class="docutils literal"><span class="pre">&lt;url&gt;</span></code> is the URL you want to scrape.</p>
<p><a class="reference internal" href="commands.html#std:command-shell"><code class="xref std std-command docutils literal"><span class="pre">shell</span></code></a> also works for local files. This can be handy if you want
to play around with a local copy of a web page. <a class="reference internal" href="commands.html#std:command-shell"><code class="xref std std-command docutils literal"><span class="pre">shell</span></code></a> understands
the following syntaxes for local files:</p>
<p class="first admonition-title">Note</p>
<p>When using relative file paths, be explicit and prepend them
with <code class="docutils literal"><span class="pre">./</span></code> (or <code class="docutils literal"><span class="pre">../</span></code> when relevant).
<code class="docutils literal"><span class="pre">scrapy</span> <span class="pre">shell</span> <span class="pre">index.html</span></code> will not work as one might expect (and
this is by design, not a bug).</p>
<p>Because <a class="reference internal" href="commands.html#std:command-shell"><code class="xref std std-command docutils literal"><span class="pre">shell</span></code></a> favors HTTP URLs over File URIs,
and <code class="docutils literal"><span class="pre">index.html</span></code> being syntactically similar to <code class="docutils literal"><span class="pre">example.com</span></code>,
<a class="reference internal" href="commands.html#std:command-shell"><code class="xref std std-command docutils literal"><span class="pre">shell</span></code></a> will treat <code class="docutils literal"><span class="pre">index.html</span></code> as a domain name and trigger
a DNS lookup error:</p>
<p class="last"><a class="reference internal" href="commands.html#std:command-shell"><code class="xref std std-command docutils literal"><span class="pre">shell</span></code></a> will not test beforehand if a file called <code class="docutils literal"><span class="pre">index.html</span></code>
exists in the current directory. Again, be explicit.</p>
<p>The Scrapy shell is just a regular Python console (or <a class="reference external" href="http://ipython.org/">IPython</a> console if you
have it available) which provides some additional shortcut functions for
convenience.</p>
<p>The Scrapy shell automatically creates some convenient objects from the
downloaded page, like the <a class="reference internal" href="request-response.html#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal"><span class="pre">Response</span></code></a> object and the
<a class="reference internal" href="selectors.html#scrapy.selector.Selector" title="scrapy.selector.Selector"><code class="xref py py-class docutils literal"><span class="pre">Selector</span></code></a> objects (for both HTML and XML
content).</p>
<p>Those objects are:</p>
<p>Here’s an example of a typical shell session where we start by scraping the
<a class="reference external" href="http://scrapy.org">http://scrapy.org</a> page, and then proceed to scrape the <a class="reference external" href="https://reddit.com">https://reddit.com</a>
page. Finally, we modify the (Reddit) request method to POST and re-fetch it
getting an error. We end the session by typing Ctrl-D (in Unix systems) or
Ctrl-Z in Windows.</p>
<p>Keep in mind that the data extracted here may not be the same when you try it,
as those pages are not static and could have changed by the time you test this.
The only purpose of this example is to get you familiarized with how the Scrapy
shell works.</p>
<p>First, we launch the shell:</p>
<p>Then, the shell fetches the URL (using the Scrapy downloader) and prints the
list of available objects and useful shortcuts (you’ll notice that these lines
all start with the <code class="docutils literal"><span class="pre">[s]</span></code> prefix):</p>
<p>After that, we can start playing with the objects:</p>
<p>Sometimes you want to inspect the responses that are being processed in a
certain point of your spider, if only to check that response you expect is
getting there.</p>
<p>This can be achieved by using the <code class="docutils literal"><span class="pre">scrapy.shell.inspect_response</span></code> function.</p>
<p>Here’s an example of how you would call it from your spider:</p>
<p>When you run the spider, you will get something similar to this:</p>
<p>Then, you can check if the extraction code is working:</p>
<p>Nope, it doesn’t. So you can open the response in your web browser and see if
it’s the response you were expecting:</p>
<p>Finally you hit Ctrl-D (or Ctrl-Z in Windows) to exit the shell and resume the
crawling:</p>
<p>Note that you can’t use the <code class="docutils literal"><span class="pre">fetch</span></code> shortcut here since the Scrapy engine is
blocked by the shell. However, after you leave the shell, the spider will
continue crawling where it stopped, as shown above.</p>
<p>
        © Copyright 2008-2016, Scrapy developers.
      
        <span class="commit">
          Revision <code>5aebdac4</code>.
        </span>
</p>
<p class="caption"><span class="caption-text">First steps</span></p>
<p class="caption"><span class="caption-text">Basic concepts</span></p>
<p class="caption"><span class="caption-text">Built-in services</span></p>
<p class="caption"><span class="caption-text">Solving specific problems</span></p>
<p class="caption"><span class="caption-text">Extending Scrapy</span></p>
<p class="caption"><span class="caption-text">All the rest</span></p>
<p>After an item has been scraped by a spider, it is sent to the Item Pipeline
which processes it through several components that are executed sequentially.</p>
<p>Each item pipeline component (sometimes referred as just “Item Pipeline”) is a
Python class that implements a simple method. They receive an item and perform
an action over it, also deciding if the item should continue through the
pipeline or be dropped and no longer processed.</p>
<p>Typical uses of item pipelines are:</p>
<p>Each item pipeline component is a Python class that must implement the following method:</p>
<p>This method is called for every item pipeline component. <a class="reference internal" href="#process_item" title="process_item"><code class="xref py py-meth docutils literal"><span class="pre">process_item()</span></code></a>
must either: return a dict with data, return an <a class="reference internal" href="items.html#scrapy.item.Item" title="scrapy.item.Item"><code class="xref py py-class docutils literal"><span class="pre">Item</span></code></a>
(or any descendant class) object, return a <a class="reference external" href="https://twistedmatrix.com/documents/current/core/howto/defer.html">Twisted Deferred</a> or raise
<a class="reference internal" href="exceptions.html#scrapy.exceptions.DropItem" title="scrapy.exceptions.DropItem"><code class="xref py py-exc docutils literal"><span class="pre">DropItem</span></code></a> exception. Dropped items are no longer
processed by further pipeline components.</p>
<p>Additionally, they may also implement the following methods:</p>
<p>This method is called when the spider is opened.</p>
<p>This method is called when the spider is closed.</p>
<p>If present, this classmethod is called to create a pipeline instance
from a <a class="reference internal" href="api.html#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler"><code class="xref py py-class docutils literal"><span class="pre">Crawler</span></code></a>. It must return a new instance
of the pipeline. Crawler object provides access to all Scrapy core
components like settings and signals; it is a way for pipeline to
access them and hook its functionality into Scrapy.</p>
<p>Let’s take a look at the following hypothetical pipeline that adjusts the
<code class="docutils literal"><span class="pre">price</span></code> attribute for those items that do not include VAT
(<code class="docutils literal"><span class="pre">price_excludes_vat</span></code> attribute), and drops those items which don’t
contain a price:</p>
<p>The following pipeline stores all scraped items (from all spiders) into a
single <code class="docutils literal"><span class="pre">items.jl</span></code> file, containing one item per line serialized in JSON
format:</p>
<p class="first admonition-title">Note</p>
<p class="last">The purpose of JsonWriterPipeline is just to introduce how to write
item pipelines. If you really want to store all scraped items into a JSON
file you should use the <a class="reference internal" href="feed-exports.html#topics-feed-exports"><span class="std std-ref">Feed exports</span></a>.</p>
<p>In this example we’ll write items to <a class="reference external" href="https://www.mongodb.org/">MongoDB</a> using <a class="reference external" href="https://api.mongodb.org/python/current/">pymongo</a>.
MongoDB address and database name are specified in Scrapy settings;
MongoDB collection is named after item class.</p>
<p>The main point of this example is to show how to use <a class="reference internal" href="#from_crawler" title="from_crawler"><code class="xref py py-meth docutils literal"><span class="pre">from_crawler()</span></code></a>
method and how to clean up the resources properly.:</p>
<p>This example demonstrates how to return <a class="reference external" href="https://twistedmatrix.com/documents/current/core/howto/defer.html">Deferred</a> from <a class="reference internal" href="#process_item" title="process_item"><code class="xref py py-meth docutils literal"><span class="pre">process_item()</span></code></a> method.
It uses <a class="reference external" href="https://splash.readthedocs.io/en/stable/">Splash</a> to render screenshot of item url. Pipeline
makes request to locally running instance of <a class="reference external" href="https://splash.readthedocs.io/en/stable/">Splash</a>. After request is downloaded
and Deferred callback fires, it saves item to a file and adds filename to an item.</p>
<p>A filter that looks for duplicate items, and drops those items that were
already processed. Let’s say that our items have a unique id, but our spider
returns multiples items with the same id:</p>
<p>To activate an Item Pipeline component you must add its class to the
<a class="reference internal" href="settings.html#std:setting-ITEM_PIPELINES"><code class="xref std std-setting docutils literal"><span class="pre">ITEM_PIPELINES</span></code></a> setting, like in the following example:</p>
<p>The integer values you assign to classes in this setting determine the
order in which they run: items go through from lower valued to higher
valued classes. It’s customary to define these numbers in the 0-1000 range.</p>
<p>
        © Copyright 2008-2016, Scrapy developers.
      
        <span class="commit">
          Revision <code>5aebdac4</code>.
        </span>
</p>
<p class="caption"><span class="caption-text">First steps</span></p>
<p class="caption"><span class="caption-text">Basic concepts</span></p>
<p class="caption"><span class="caption-text">Built-in services</span></p>
<p class="caption"><span class="caption-text">Solving specific problems</span></p>
<p class="caption"><span class="caption-text">Extending Scrapy</span></p>
<p class="caption"><span class="caption-text">All the rest</span></p>
<p><span class="versionmodified">New in version 0.10.</span></p>
<p>One of the most frequently required features when implementing scrapers is
being able to store the scraped data properly and, quite often, that means
generating an “export file” with the scraped data (commonly called “export
feed”) to be consumed by other systems.</p>
<p>Scrapy provides this functionality out of the box with the Feed Exports, which
allows you to generate a feed with the scraped items, using multiple
serialization formats and storage backends.</p>
<p>For serializing the scraped data, the feed exports use the <a class="reference internal" href="exporters.html#topics-exporters"><span class="std std-ref">Item exporters</span></a>. These formats are supported out of the box:</p>
<p>But you can also extend the supported format through the
<a class="reference internal" href="#std:setting-FEED_EXPORTERS"><code class="xref std std-setting docutils literal"><span class="pre">FEED_EXPORTERS</span></code></a> setting.</p>
<p>When using the feed exports you define where to store the feed using a <a class="reference external" href="https://en.wikipedia.org/wiki/Uniform_Resource_Identifier">URI</a>
(through the <a class="reference internal" href="#std:setting-FEED_URI"><code class="xref std std-setting docutils literal"><span class="pre">FEED_URI</span></code></a> setting). The feed exports supports multiple
storage backend types which are defined by the URI scheme.</p>
<p>The storages backends supported out of the box are:</p>
<p>Some storage backends may be unavailable if the required external libraries are
not available. For example, the S3 backend is only available if the <a class="reference external" href="https://github.com/boto/botocore">botocore</a>
or <a class="reference external" href="https://github.com/boto/boto">boto</a> library is installed (Scrapy supports <a class="reference external" href="https://github.com/boto/boto">boto</a> only on Python 2).</p>
<p>The storage URI can also contain parameters that get replaced when the feed is
being created. These parameters are:</p>
<p>Any other named parameter gets replaced by the spider attribute of the same
name. For example, <code class="docutils literal"><span class="pre">%(site_id)s</span></code> would get replaced by the <code class="docutils literal"><span class="pre">spider.site_id</span></code>
attribute the moment the feed is being created.</p>
<p>Here are some examples to illustrate:</p>
<p>The feeds are stored in the local filesystem.</p>
<p>Note that for the local filesystem storage (only) you can omit the scheme if
you specify an absolute path like <code class="docutils literal"><span class="pre">/tmp/export.csv</span></code>. This only works on Unix
systems though.</p>
<p>The feeds are stored in a FTP server.</p>
<p>The feeds are stored on <a class="reference external" href="https://aws.amazon.com/s3/">Amazon S3</a>.</p>
<p>The AWS credentials can be passed as user/password in the URI, or they can be
passed through the following settings:</p>
<p>The feeds are written to the standard output of the Scrapy process.</p>
<p>These are the settings used for configuring the feed exports:</p>
<p>Default: <code class="docutils literal"><span class="pre">None</span></code></p>
<p>The URI of the export feed. See <a class="reference internal" href="#topics-feed-storage-backends"><span class="std std-ref">Storage backends</span></a> for
supported URI schemes.</p>
<p>This setting is required for enabling the feed exports.</p>
<p>The serialization format to be used for the feed. See
<a class="reference internal" href="#topics-feed-format"><span class="std std-ref">Serialization formats</span></a> for possible values.</p>
<p>Default: <code class="docutils literal"><span class="pre">None</span></code></p>
<p>The encoding to be used for the feed.</p>
<p>If unset or set to <code class="docutils literal"><span class="pre">None</span></code> (default) it uses UTF-8 for everything except JSON output,
which uses safe numeric encoding (<code class="docutils literal"><span class="pre">\uXXXX</span></code> sequences) for historic reasons.</p>
<p>Use <code class="docutils literal"><span class="pre">utf-8</span></code> if you want UTF-8 for JSON too.</p>
<p>Default: <code class="docutils literal"><span class="pre">None</span></code></p>
<p>A list of fields to export, optional.
Example: <code class="docutils literal"><span class="pre">FEED_EXPORT_FIELDS</span> <span class="pre">=</span> <span class="pre">["foo",</span> <span class="pre">"bar",</span> <span class="pre">"baz"]</span></code>.</p>
<p>Use FEED_EXPORT_FIELDS option to define fields to export and their order.</p>
<p>When FEED_EXPORT_FIELDS is empty or None (default), Scrapy uses fields
defined in dicts or <a class="reference internal" href="items.html#scrapy.item.Item" title="scrapy.item.Item"><code class="xref py py-class docutils literal"><span class="pre">Item</span></code></a> subclasses a spider is yielding.</p>
<p>If an exporter requires a fixed set of fields (this is the case for
<a class="reference internal" href="#topics-feed-format-csv"><span class="std std-ref">CSV</span></a> export format) and FEED_EXPORT_FIELDS
is empty or None, then Scrapy tries to infer field names from the
exported data - currently it uses field names from the first item.</p>
<p>Default: <code class="docutils literal"><span class="pre">0</span></code></p>
<p>Amount of spaces used to indent the output on each level. If <code class="docutils literal"><span class="pre">FEED_EXPORT_INDENT</span></code>
is a non-negative integer, then array elements and object members will be pretty-printed
with that indent level. An indent level of <code class="docutils literal"><span class="pre">0</span></code> (the default), or negative,
will put each item on a new line. <code class="docutils literal"><span class="pre">None</span></code> selects the most compact representation.</p>
<p>Currently implemented only by <a class="reference internal" href="exporters.html#scrapy.exporters.JsonItemExporter" title="scrapy.exporters.JsonItemExporter"><code class="xref py py-class docutils literal"><span class="pre">JsonItemExporter</span></code></a>
and <a class="reference internal" href="exporters.html#scrapy.exporters.XmlItemExporter" title="scrapy.exporters.XmlItemExporter"><code class="xref py py-class docutils literal"><span class="pre">XmlItemExporter</span></code></a>, i.e. when you are exporting
to <code class="docutils literal"><span class="pre">.json</span></code> or <code class="docutils literal"><span class="pre">.xml</span></code>.</p>
<p>Default: <code class="docutils literal"><span class="pre">False</span></code></p>
<p>Whether to export empty feeds (ie. feeds with no items).</p>
<p>Default: <code class="docutils literal"><span class="pre">{}</span></code></p>
<p>A dict containing additional feed storage backends supported by your project.
The keys are URI schemes and the values are paths to storage classes.</p>
<p>Default:</p>
<p>A dict containing the built-in feed storage backends supported by Scrapy. You
can disable any of these backends by assigning <code class="docutils literal"><span class="pre">None</span></code> to their URI scheme in
<a class="reference internal" href="#std:setting-FEED_STORAGES"><code class="xref std std-setting docutils literal"><span class="pre">FEED_STORAGES</span></code></a>. E.g., to disable the built-in FTP storage backend
(without replacement), place this in your <code class="docutils literal"><span class="pre">settings.py</span></code>:</p>
<p>Default: <code class="docutils literal"><span class="pre">{}</span></code></p>
<p>A dict containing additional exporters supported by your project. The keys are
serialization formats and the values are paths to <a class="reference internal" href="exporters.html#topics-exporters"><span class="std std-ref">Item exporter</span></a> classes.</p>
<p>Default:</p>
<p>A dict containing the built-in feed exporters supported by Scrapy. You can
disable any of these exporters by assigning <code class="docutils literal"><span class="pre">None</span></code> to their serialization
format in <a class="reference internal" href="#std:setting-FEED_EXPORTERS"><code class="xref std std-setting docutils literal"><span class="pre">FEED_EXPORTERS</span></code></a>. E.g., to disable the built-in CSV exporter
(without replacement), place this in your <code class="docutils literal"><span class="pre">settings.py</span></code>:</p>
<p>
        © Copyright 2008-2016, Scrapy developers.
      
        <span class="commit">
          Revision <code>5aebdac4</code>.
        </span>
</p>
<p class="caption"><span class="caption-text">First steps</span></p>
<p class="caption"><span class="caption-text">Basic concepts</span></p>
<p class="caption"><span class="caption-text">Built-in services</span></p>
<p class="caption"><span class="caption-text">Solving specific problems</span></p>
<p class="caption"><span class="caption-text">Extending Scrapy</span></p>
<p class="caption"><span class="caption-text">All the rest</span></p>
<p>Scrapy uses <a class="reference internal" href="#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal"><span class="pre">Request</span></code></a> and <a class="reference internal" href="#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal"><span class="pre">Response</span></code></a> objects for crawling web
sites.</p>
<p>Typically, <a class="reference internal" href="#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal"><span class="pre">Request</span></code></a> objects are generated in the spiders and pass
across the system until they reach the Downloader, which executes the request
and returns a <a class="reference internal" href="#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal"><span class="pre">Response</span></code></a> object which travels back to the spider that
issued the request.</p>
<p>Both <a class="reference internal" href="#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal"><span class="pre">Request</span></code></a> and <a class="reference internal" href="#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal"><span class="pre">Response</span></code></a> classes have subclasses which add
functionality not required in the base classes. These are described
below in <a class="reference internal" href="#topics-request-response-ref-request-subclasses"><span class="std std-ref">Request subclasses</span></a> and
<a class="reference internal" href="#topics-request-response-ref-response-subclasses"><span class="std std-ref">Response subclasses</span></a>.</p>
<p>A <a class="reference internal" href="#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal"><span class="pre">Request</span></code></a> object represents an HTTP request, which is usually
generated in the Spider and executed by the Downloader, and thus generating
a <a class="reference internal" href="#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal"><span class="pre">Response</span></code></a>.</p>
<p>the request cookies. These can be sent in two forms.</p>
<p>The latter form allows for customizing the <code class="docutils literal"><span class="pre">domain</span></code> and <code class="docutils literal"><span class="pre">path</span></code>
attributes of the cookie. This is only useful if the cookies are saved
for later requests.</p>
<p>When some site returns cookies (in a response) those are stored in the
cookies for that domain and will be sent again in future requests. That’s
the typical behaviour of any regular web browser. However, if, for some
reason, you want to avoid merging with existing cookies you can instruct
Scrapy to do so by setting the <code class="docutils literal"><span class="pre">dont_merge_cookies</span></code> key to True in the
<a class="reference internal" href="#scrapy.http.Request.meta" title="scrapy.http.Request.meta"><code class="xref py py-attr docutils literal"><span class="pre">Request.meta</span></code></a>.</p>
<p>Example of request without merging cookies:</p>
<p>For more info see <a class="reference internal" href="downloader-middleware.html#cookies-mw"><span class="std std-ref">CookiesMiddleware</span></a>.</p>
<p>A string containing the URL of this request. Keep in mind that this
attribute contains the escaped URL, so it can differ from the URL passed in
the constructor.</p>
<p>This attribute is read-only. To change the URL of a Request use
<a class="reference internal" href="#scrapy.http.Request.replace" title="scrapy.http.Request.replace"><code class="xref py py-meth docutils literal"><span class="pre">replace()</span></code></a>.</p>
<p>A string representing the HTTP method in the request. This is guaranteed to
be uppercase. Example: <code class="docutils literal"><span class="pre">"GET"</span></code>, <code class="docutils literal"><span class="pre">"POST"</span></code>, <code class="docutils literal"><span class="pre">"PUT"</span></code>, etc</p>
<p>A dictionary-like object which contains the request headers.</p>
<p>A str that contains the request body.</p>
<p>This attribute is read-only. To change the body of a Request use
<a class="reference internal" href="#scrapy.http.Request.replace" title="scrapy.http.Request.replace"><code class="xref py py-meth docutils literal"><span class="pre">replace()</span></code></a>.</p>
<p>A dict that contains arbitrary metadata for this request. This dict is
empty for new Requests, and is usually  populated by different Scrapy
components (extensions, middlewares, etc). So the data contained in this
dict depends on the extensions you have enabled.</p>
<p>See <a class="reference internal" href="#topics-request-meta"><span class="std std-ref">Request.meta special keys</span></a> for a list of special meta keys
recognized by Scrapy.</p>
<p>This dict is <a class="reference external" href="https://docs.python.org/2/library/copy.html">shallow copied</a> when the request is cloned using the
<code class="docutils literal"><span class="pre">copy()</span></code> or <code class="docutils literal"><span class="pre">replace()</span></code> methods, and can also be accessed, in your
spider, from the <code class="docutils literal"><span class="pre">response.meta</span></code> attribute.</p>
<p>Return a new Request which is a copy of this Request. See also:
<a class="reference internal" href="#topics-request-response-ref-request-callback-arguments"><span class="std std-ref">Passing additional data to callback functions</span></a>.</p>
<p>Return a Request object with the same members, except for those members
given new values by whichever keyword arguments are specified. The
attribute <a class="reference internal" href="#scrapy.http.Request.meta" title="scrapy.http.Request.meta"><code class="xref py py-attr docutils literal"><span class="pre">Request.meta</span></code></a> is copied by default (unless a new value
is given in the <code class="docutils literal"><span class="pre">meta</span></code> argument). See also
<a class="reference internal" href="#topics-request-response-ref-request-callback-arguments"><span class="std std-ref">Passing additional data to callback functions</span></a>.</p>
<p>The callback of a request is a function that will be called when the response
of that request is downloaded. The callback function will be called with the
downloaded <a class="reference internal" href="#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal"><span class="pre">Response</span></code></a> object as its first argument.</p>
<p>Example:</p>
<p>In some cases you may be interested in passing arguments to those callback
functions so you can receive the arguments later, in the second callback. You
can use the <a class="reference internal" href="#scrapy.http.Request.meta" title="scrapy.http.Request.meta"><code class="xref py py-attr docutils literal"><span class="pre">Request.meta</span></code></a> attribute for that.</p>
<p>Here’s an example of how to pass an item using this mechanism, to populate
different fields from different pages:</p>
<p>The errback of a request is a function that will be called when an exception
is raise while processing it.</p>
<p>It receives a <a class="reference external" href="https://twistedmatrix.com/documents/current/api/twisted.python.failure.Failure.html">Twisted Failure</a> instance as first parameter and can be
used to track connection establishment timeouts, DNS errors etc.</p>
<p>Here’s an example spider logging all errors and catching some specific
errors if needed:</p>
<p>The <a class="reference internal" href="#scrapy.http.Request.meta" title="scrapy.http.Request.meta"><code class="xref py py-attr docutils literal"><span class="pre">Request.meta</span></code></a> attribute can contain any arbitrary data, but there
are some special keys recognized by Scrapy and its built-in extensions.</p>
<p>Those are:</p>
<p>The IP of the outgoing IP address to use for the performing the request.</p>
<p>The amount of time (in secs) that the downloader will wait before timing out.
See also: <a class="reference internal" href="settings.html#std:setting-DOWNLOAD_TIMEOUT"><code class="xref std std-setting docutils literal"><span class="pre">DOWNLOAD_TIMEOUT</span></code></a>.</p>
<p>The amount of time spent to fetch the response, since the request has been
started, i.e. HTTP message sent over the network. This meta key only becomes
available when the response has been downloaded. While most other meta keys are
used to control Scrapy behavior, this one is supposed to be read-only.</p>
<p>Whether or not to fail on broken responses. See:
<a class="reference internal" href="settings.html#std:setting-DOWNLOAD_FAIL_ON_DATALOSS"><code class="xref std std-setting docutils literal"><span class="pre">DOWNLOAD_FAIL_ON_DATALOSS</span></code></a>.</p>
<p>The meta key is used set retry times per request. When initialized, the
<a class="reference internal" href="#std:reqmeta-max_retry_times"><code class="xref std std-reqmeta docutils literal"><span class="pre">max_retry_times</span></code></a> meta key takes higher precedence over the
<a class="reference internal" href="downloader-middleware.html#std:setting-RETRY_TIMES"><code class="xref std std-setting docutils literal"><span class="pre">RETRY_TIMES</span></code></a> setting.</p>
<p>Here is the list of built-in <a class="reference internal" href="#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal"><span class="pre">Request</span></code></a> subclasses. You can also subclass
it to implement your own custom functionality.</p>
<p>The FormRequest class extends the base <a class="reference internal" href="#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal"><span class="pre">Request</span></code></a> with functionality for
dealing with HTML forms. It uses <a class="reference external" href="http://lxml.de/lxmlhtml.html#forms">lxml.html forms</a>  to pre-populate form
fields with form data from <a class="reference internal" href="#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal"><span class="pre">Response</span></code></a> objects.</p>
<p>The <a class="reference internal" href="#scrapy.http.FormRequest" title="scrapy.http.FormRequest"><code class="xref py py-class docutils literal"><span class="pre">FormRequest</span></code></a> class adds a new argument to the constructor. The
remaining arguments are the same as for the <a class="reference internal" href="#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal"><span class="pre">Request</span></code></a> class and are
not documented here.</p>
<p>The <a class="reference internal" href="#scrapy.http.FormRequest" title="scrapy.http.FormRequest"><code class="xref py py-class docutils literal"><span class="pre">FormRequest</span></code></a> objects support the following class method in
addition to the standard <a class="reference internal" href="#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal"><span class="pre">Request</span></code></a> methods:</p>
<p>Returns a new <a class="reference internal" href="#scrapy.http.FormRequest" title="scrapy.http.FormRequest"><code class="xref py py-class docutils literal"><span class="pre">FormRequest</span></code></a> object with its form field values
pre-populated with those found in the HTML <code class="docutils literal"><span class="pre">&lt;form&gt;</span></code> element contained
in the given response. For an example see
<a class="reference internal" href="#topics-request-response-ref-request-userlogin"><span class="std std-ref">Using FormRequest.from_response() to simulate a user login</span></a>.</p>
<p>The policy is to automatically simulate a click, by default, on any form
control that looks clickable, like a <code class="docutils literal"><span class="pre">&lt;input</span> <span class="pre">type="submit"&gt;</span></code>.  Even
though this is quite convenient, and often the desired behaviour,
sometimes it can cause problems which could be hard to debug. For
example, when working with forms that are filled and/or submitted using
javascript, the default <a class="reference internal" href="#scrapy.http.FormRequest.from_response" title="scrapy.http.FormRequest.from_response"><code class="xref py py-meth docutils literal"><span class="pre">from_response()</span></code></a> behaviour may not be the
most appropriate. To disable this behaviour you can set the
<code class="docutils literal"><span class="pre">dont_click</span></code> argument to <code class="docutils literal"><span class="pre">True</span></code>. Also, if you want to change the
control clicked (instead of disabling it) you can also use the
<code class="docutils literal"><span class="pre">clickdata</span></code> argument.</p>
<p class="first admonition-title">Caution</p>
<p class="last">Using this method with select elements which have leading
or trailing whitespace in the option values will not work due to a
<a class="reference external" href="https://bugs.launchpad.net/lxml/+bug/1665241">bug in lxml</a>, which should be fixed in lxml 3.8 and above.</p>
<p>The other parameters of this class method are passed directly to the
<a class="reference internal" href="#scrapy.http.FormRequest" title="scrapy.http.FormRequest"><code class="xref py py-class docutils literal"><span class="pre">FormRequest</span></code></a> constructor.</p>
<p><span class="versionmodified">New in version 0.10.3: </span>The <code class="docutils literal"><span class="pre">formname</span></code> parameter.</p>
<p><span class="versionmodified">New in version 0.17: </span>The <code class="docutils literal"><span class="pre">formxpath</span></code> parameter.</p>
<p><span class="versionmodified">New in version 1.1.0: </span>The <code class="docutils literal"><span class="pre">formcss</span></code> parameter.</p>
<p><span class="versionmodified">New in version 1.1.0: </span>The <code class="docutils literal"><span class="pre">formid</span></code> parameter.</p>
<p>If you want to simulate a HTML Form POST in your spider and send a couple of
key-value fields, you can return a <a class="reference internal" href="#scrapy.http.FormRequest" title="scrapy.http.FormRequest"><code class="xref py py-class docutils literal"><span class="pre">FormRequest</span></code></a> object (from your
spider) like this:</p>
<p>It is usual for web sites to provide pre-populated form fields through <code class="docutils literal"><span class="pre">&lt;input</span>
<span class="pre">type="hidden"&gt;</span></code> elements, such as session related data or authentication
tokens (for login pages). When scraping, you’ll want these fields to be
automatically pre-populated and only override a couple of them, such as the
user name and password. You can use the <a class="reference internal" href="#scrapy.http.FormRequest.from_response" title="scrapy.http.FormRequest.from_response"><code class="xref py py-meth docutils literal"><span class="pre">FormRequest.from_response()</span></code></a>
method for this job. Here’s an example spider which uses it:</p>
<p>A <a class="reference internal" href="#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal"><span class="pre">Response</span></code></a> object represents an HTTP response, which is usually
downloaded (by the Downloader) and fed to the Spiders for processing.</p>
<p>A string containing the URL of the response.</p>
<p>This attribute is read-only. To change the URL of a Response use
<a class="reference internal" href="#scrapy.http.Response.replace" title="scrapy.http.Response.replace"><code class="xref py py-meth docutils literal"><span class="pre">replace()</span></code></a>.</p>
<p>An integer representing the HTTP status of the response. Example: <code class="docutils literal"><span class="pre">200</span></code>,
<code class="docutils literal"><span class="pre">404</span></code>.</p>
<p>A dictionary-like object which contains the response headers. Values can
be accessed using <code class="xref py py-meth docutils literal"><span class="pre">get()</span></code> to return the first header value with the
specified name or <code class="xref py py-meth docutils literal"><span class="pre">getlist()</span></code> to return all header values with the
specified name. For example, this call will give you all cookies in the
headers:</p>
<p>The body of this Response. Keep in mind that Response.body
is always a bytes object. If you want the unicode version use
<a class="reference internal" href="#scrapy.http.TextResponse.text" title="scrapy.http.TextResponse.text"><code class="xref py py-attr docutils literal"><span class="pre">TextResponse.text</span></code></a> (only available in <a class="reference internal" href="#scrapy.http.TextResponse" title="scrapy.http.TextResponse"><code class="xref py py-class docutils literal"><span class="pre">TextResponse</span></code></a>
and subclasses).</p>
<p>This attribute is read-only. To change the body of a Response use
<a class="reference internal" href="#scrapy.http.Response.replace" title="scrapy.http.Response.replace"><code class="xref py py-meth docutils literal"><span class="pre">replace()</span></code></a>.</p>
<p>The <a class="reference internal" href="#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal"><span class="pre">Request</span></code></a> object that generated this response. This attribute is
assigned in the Scrapy engine, after the response and the request have passed
through all <a class="reference internal" href="downloader-middleware.html#topics-downloader-middleware"><span class="std std-ref">Downloader Middlewares</span></a>.
In particular, this means that:</p>
<p>A shortcut to the <a class="reference internal" href="#scrapy.http.Request.meta" title="scrapy.http.Request.meta"><code class="xref py py-attr docutils literal"><span class="pre">Request.meta</span></code></a> attribute of the
<a class="reference internal" href="#scrapy.http.Response.request" title="scrapy.http.Response.request"><code class="xref py py-attr docutils literal"><span class="pre">Response.request</span></code></a> object (ie. <code class="docutils literal"><span class="pre">self.request.meta</span></code>).</p>
<p>Unlike the <a class="reference internal" href="#scrapy.http.Response.request" title="scrapy.http.Response.request"><code class="xref py py-attr docutils literal"><span class="pre">Response.request</span></code></a> attribute, the <a class="reference internal" href="#scrapy.http.Response.meta" title="scrapy.http.Response.meta"><code class="xref py py-attr docutils literal"><span class="pre">Response.meta</span></code></a>
attribute is propagated along redirects and retries, so you will get
the original <a class="reference internal" href="#scrapy.http.Request.meta" title="scrapy.http.Request.meta"><code class="xref py py-attr docutils literal"><span class="pre">Request.meta</span></code></a> sent from your spider.</p>
<p class="first admonition-title">See also</p>
<p class="last"><a class="reference internal" href="#scrapy.http.Request.meta" title="scrapy.http.Request.meta"><code class="xref py py-attr docutils literal"><span class="pre">Request.meta</span></code></a> attribute</p>
<p>A list that contains flags for this response. Flags are labels used for
tagging Responses. For example: <cite>‘cached’</cite>, <cite>‘redirected</cite>‘, etc. And
they’re shown on the string representation of the Response (<cite>__str__</cite>
method) which is used by the engine for logging.</p>
<p>Returns a new Response which is a copy of this Response.</p>
<p>Returns a Response object with the same members, except for those members
given new values by whichever keyword arguments are specified. The
attribute <a class="reference internal" href="#scrapy.http.Response.meta" title="scrapy.http.Response.meta"><code class="xref py py-attr docutils literal"><span class="pre">Response.meta</span></code></a> is copied by default.</p>
<p>Constructs an absolute url by combining the Response’s <a class="reference internal" href="#scrapy.http.Response.url" title="scrapy.http.Response.url"><code class="xref py py-attr docutils literal"><span class="pre">url</span></code></a> with
a possible relative url.</p>
<p>This is a wrapper over <a class="reference external" href="https://docs.python.org/2/library/urlparse.html#urlparse.urljoin">urlparse.urljoin</a>, it’s merely an alias for
making this call:</p>
<p>Return a <a class="reference internal" href="#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal"><span class="pre">Request</span></code></a> instance to follow a link <code class="docutils literal"><span class="pre">url</span></code>.
It accepts the same arguments as <code class="docutils literal"><span class="pre">Request.__init__</span></code> method,
but <code class="docutils literal"><span class="pre">url</span></code> can be a relative URL or a <code class="docutils literal"><span class="pre">scrapy.link.Link</span></code> object,
not only an absolute URL.</p>
<p><a class="reference internal" href="#scrapy.http.TextResponse" title="scrapy.http.TextResponse"><code class="xref py py-class docutils literal"><span class="pre">TextResponse</span></code></a> provides a <a class="reference internal" href="#scrapy.http.TextResponse.follow" title="scrapy.http.TextResponse.follow"><code class="xref py py-meth docutils literal"><span class="pre">follow()</span></code></a> 
method which supports selectors in addition to absolute/relative URLs
and Link objects.</p>
<p>Here is the list of available built-in Response subclasses. You can also
subclass the Response class to implement your own functionality.</p>
<p><a class="reference internal" href="#scrapy.http.TextResponse" title="scrapy.http.TextResponse"><code class="xref py py-class docutils literal"><span class="pre">TextResponse</span></code></a> objects adds encoding capabilities to the base
<a class="reference internal" href="#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal"><span class="pre">Response</span></code></a> class, which is meant to be used only for binary data,
such as images, sounds or any media file.</p>
<p><a class="reference internal" href="#scrapy.http.TextResponse" title="scrapy.http.TextResponse"><code class="xref py py-class docutils literal"><span class="pre">TextResponse</span></code></a> objects support a new constructor argument, in
addition to the base <a class="reference internal" href="#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal"><span class="pre">Response</span></code></a> objects. The remaining functionality
is the same as for the <a class="reference internal" href="#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal"><span class="pre">Response</span></code></a> class and is not documented here.</p>
<p><a class="reference internal" href="#scrapy.http.TextResponse" title="scrapy.http.TextResponse"><code class="xref py py-class docutils literal"><span class="pre">TextResponse</span></code></a> objects support the following attributes in addition
to the standard <a class="reference internal" href="#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal"><span class="pre">Response</span></code></a> ones:</p>
<p>Response body, as unicode.</p>
<p>The same as <code class="docutils literal"><span class="pre">response.body.decode(response.encoding)</span></code>, but the
result is cached after the first call, so you can access
<code class="docutils literal"><span class="pre">response.text</span></code> multiple times without extra overhead.</p>
<p class="first admonition-title">Note</p>
<p class="last"><code class="docutils literal"><span class="pre">unicode(response.body)</span></code> is not a correct way to convert response
body to unicode: you would be using the system default encoding
(typically <cite>ascii</cite>) instead of the response encoding.</p>
<p>A string with the encoding of this response. The encoding is resolved by
trying the following mechanisms, in order:</p>
<p>A <a class="reference internal" href="selectors.html#scrapy.selector.Selector" title="scrapy.selector.Selector"><code class="xref py py-class docutils literal"><span class="pre">Selector</span></code></a> instance using the response as
target. The selector is lazily instantiated on first access.</p>
<p><a class="reference internal" href="#scrapy.http.TextResponse" title="scrapy.http.TextResponse"><code class="xref py py-class docutils literal"><span class="pre">TextResponse</span></code></a> objects support the following methods in addition to
the standard <a class="reference internal" href="#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal"><span class="pre">Response</span></code></a> ones:</p>
<p>A shortcut to <code class="docutils literal"><span class="pre">TextResponse.selector.xpath(query)</span></code>:</p>
<p>A shortcut to <code class="docutils literal"><span class="pre">TextResponse.selector.css(query)</span></code>:</p>
<p>Return a <a class="reference internal" href="#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal"><span class="pre">Request</span></code></a> instance to follow a link <code class="docutils literal"><span class="pre">url</span></code>.
It accepts the same arguments as <code class="docutils literal"><span class="pre">Request.__init__</span></code> method,
but <code class="docutils literal"><span class="pre">url</span></code> can be not only an absolute URL, but also</p>
<p>See <a class="reference internal" href="../intro/tutorial.html#response-follow-example"><span class="std std-ref">A shortcut for creating Requests</span></a> for usage examples.</p>
<p>The same as <a class="reference internal" href="#scrapy.http.TextResponse.text" title="scrapy.http.TextResponse.text"><code class="xref py py-attr docutils literal"><span class="pre">text</span></code></a>, but available as a method. This method is
kept for backwards compatibility; please prefer <code class="docutils literal"><span class="pre">response.text</span></code>.</p>
<p>The <a class="reference internal" href="#scrapy.http.HtmlResponse" title="scrapy.http.HtmlResponse"><code class="xref py py-class docutils literal"><span class="pre">HtmlResponse</span></code></a> class is a subclass of <a class="reference internal" href="#scrapy.http.TextResponse" title="scrapy.http.TextResponse"><code class="xref py py-class docutils literal"><span class="pre">TextResponse</span></code></a>
which adds encoding auto-discovering support by looking into the HTML <a class="reference external" href="http://www.w3schools.com/TAGS/att_meta_http_equiv.asp">meta
http-equiv</a> attribute.  See <a class="reference internal" href="#scrapy.http.TextResponse.encoding" title="scrapy.http.TextResponse.encoding"><code class="xref py py-attr docutils literal"><span class="pre">TextResponse.encoding</span></code></a>.</p>
<p>The <a class="reference internal" href="#scrapy.http.XmlResponse" title="scrapy.http.XmlResponse"><code class="xref py py-class docutils literal"><span class="pre">XmlResponse</span></code></a> class is a subclass of <a class="reference internal" href="#scrapy.http.TextResponse" title="scrapy.http.TextResponse"><code class="xref py py-class docutils literal"><span class="pre">TextResponse</span></code></a> which
adds encoding auto-discovering support by looking into the XML declaration
line.  See <a class="reference internal" href="#scrapy.http.TextResponse.encoding" title="scrapy.http.TextResponse.encoding"><code class="xref py py-attr docutils literal"><span class="pre">TextResponse.encoding</span></code></a>.</p>
<p>
        © Copyright 2008-2016, Scrapy developers.
      
        <span class="commit">
          Revision <code>5aebdac4</code>.
        </span>
</p>
<p class="caption"><span class="caption-text">First steps</span></p>
<p class="caption"><span class="caption-text">Basic concepts</span></p>
<p class="caption"><span class="caption-text">Built-in services</span></p>
<p class="caption"><span class="caption-text">Solving specific problems</span></p>
<p class="caption"><span class="caption-text">Extending Scrapy</span></p>
<p class="caption"><span class="caption-text">All the rest</span></p>
<p>Link extractors are objects whose only purpose is to extract links from web
pages (<a class="reference internal" href="request-response.html#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal"><span class="pre">scrapy.http.Response</span></code></a> objects) which will be eventually
followed.</p>
<p>There is <code class="docutils literal"><span class="pre">scrapy.linkextractors.LinkExtractor</span></code> available
in Scrapy, but you can create your own custom Link Extractors to suit your
needs by implementing a simple interface.</p>
<p>The only public method that every link extractor has is <code class="docutils literal"><span class="pre">extract_links</span></code>,
which receives a <a class="reference internal" href="request-response.html#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal"><span class="pre">Response</span></code></a> object and returns a list
of <code class="xref py py-class docutils literal"><span class="pre">scrapy.link.Link</span></code> objects. Link extractors are meant to be
instantiated once and their <code class="docutils literal"><span class="pre">extract_links</span></code> method called several times
with different responses to extract links to follow.</p>
<p>Link extractors are used in the <a class="reference internal" href="spiders.html#scrapy.spiders.CrawlSpider" title="scrapy.spiders.CrawlSpider"><code class="xref py py-class docutils literal"><span class="pre">CrawlSpider</span></code></a>
class (available in Scrapy), through a set of rules, but you can also use it in
your spiders, even if you don’t subclass from
<a class="reference internal" href="spiders.html#scrapy.spiders.CrawlSpider" title="scrapy.spiders.CrawlSpider"><code class="xref py py-class docutils literal"><span class="pre">CrawlSpider</span></code></a>, as its purpose is very simple: to
extract links.</p>
<p>Link extractors classes bundled with Scrapy are provided in the
<a class="reference internal" href="#module-scrapy.linkextractors" title="scrapy.linkextractors: Link extractors classes"><code class="xref py py-mod docutils literal"><span class="pre">scrapy.linkextractors</span></code></a> module.</p>
<p>The default link extractor is <code class="docutils literal"><span class="pre">LinkExtractor</span></code>, which is the same as
<a class="reference internal" href="#scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor" title="scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor"><code class="xref py py-class docutils literal"><span class="pre">LxmlLinkExtractor</span></code></a>:</p>
<p>There used to be other link extractor classes in previous Scrapy versions,
but they are deprecated now.</p>
<p>LxmlLinkExtractor is the recommended link extractor with handy filtering
options. It is implemented using lxml’s robust HTMLParser.</p>
<p>a function which receives each value extracted from
the tag and attributes scanned and can modify the value and return a
new one, or return <code class="docutils literal"><span class="pre">None</span></code> to ignore the link altogether. If not
given, <code class="docutils literal"><span class="pre">process_value</span></code> defaults to <code class="docutils literal"><span class="pre">lambda</span> <span class="pre">x:</span> <span class="pre">x</span></code>.</p>
<p>For example, to extract links from this code:</p>
<p>You can use the following function in <code class="docutils literal"><span class="pre">process_value</span></code>:</p>
<p>
        © Copyright 2008-2016, Scrapy developers.
      
        <span class="commit">
          Revision <code>5aebdac4</code>.
        </span>
</p>
<p class="caption"><span class="caption-text">First steps</span></p>
<p class="caption"><span class="caption-text">Basic concepts</span></p>
<p class="caption"><span class="caption-text">Built-in services</span></p>
<p class="caption"><span class="caption-text">Solving specific problems</span></p>
<p class="caption"><span class="caption-text">Extending Scrapy</span></p>
<p class="caption"><span class="caption-text">All the rest</span></p>
<p>The Scrapy settings allows you to customize the behaviour of all Scrapy
components, including the core, extensions, pipelines and spiders themselves.</p>
<p>The infrastructure of the settings provides a global namespace of key-value mappings
that the code can use to pull configuration values from. The settings can be
populated through different mechanisms, which are described below.</p>
<p>The settings are also the mechanism for selecting the currently active Scrapy
project (in case you have many).</p>
<p>For a list of available built-in settings see: <a class="reference internal" href="#topics-settings-ref"><span class="std std-ref">Built-in settings reference</span></a>.</p>
<p>When you use Scrapy, you have to tell it which settings you’re using. You can
do this by using an environment variable, <code class="docutils literal"><span class="pre">SCRAPY_SETTINGS_MODULE</span></code>.</p>
<p>The value of <code class="docutils literal"><span class="pre">SCRAPY_SETTINGS_MODULE</span></code> should be in Python path syntax, e.g.
<code class="docutils literal"><span class="pre">myproject.settings</span></code>. Note that the settings module should be on the
Python <a class="reference external" href="https://docs.python.org/2/tutorial/modules.html#the-module-search-path">import search path</a>.</p>
<p>Settings can be populated using different mechanisms, each of which having a
different precedence. Here is the list of them in decreasing order of
precedence:</p>
<p>The population of these settings sources is taken care of internally, but a
manual handling is possible using API calls. See the
<a class="reference internal" href="api.html#topics-api-settings"><span class="std std-ref">Settings API</span></a> topic for reference.</p>
<p>These mechanisms are described in more detail below.</p>
<p>Arguments provided by the command line are the ones that take most precedence,
overriding any other options. You can explicitly override one (or more)
settings using the <code class="docutils literal"><span class="pre">-s</span></code> (or <code class="docutils literal"><span class="pre">--set</span></code>) command line option.</p>
<p>Example:</p>
<p>Spiders (See the <a class="reference internal" href="spiders.html#topics-spiders"><span class="std std-ref">Spiders</span></a> chapter for reference) can define their
own settings that will take precedence and override the project ones. They can
do so by setting their <a class="reference internal" href="spiders.html#scrapy.spiders.Spider.custom_settings" title="scrapy.spiders.Spider.custom_settings"><code class="xref py py-attr docutils literal"><span class="pre">custom_settings</span></code></a> attribute:</p>
<p>The project settings module is the standard configuration file for your Scrapy
project, it’s where most of your custom settings will be populated. For a
standard Scrapy project, this means you’ll be adding or changing the settings
in the <code class="docutils literal"><span class="pre">settings.py</span></code> file created for your project.</p>
<p>Each <a class="reference internal" href="commands.html"><span class="doc">Scrapy tool</span></a> command can have its own default
settings, which override the global default settings. Those custom command
settings are specified in the <code class="docutils literal"><span class="pre">default_settings</span></code> attribute of the command
class.</p>
<p>The global defaults are located in the <code class="docutils literal"><span class="pre">scrapy.settings.default_settings</span></code>
module and documented in the <a class="reference internal" href="#topics-settings-ref"><span class="std std-ref">Built-in settings reference</span></a> section.</p>
<p>In a spider, the settings are available through <code class="docutils literal"><span class="pre">self.settings</span></code>:</p>
<p class="first admonition-title">Note</p>
<p class="last">The <code class="docutils literal"><span class="pre">settings</span></code> attribute is set in the base Spider class after the spider
is initialized.  If you want to use the settings before the initialization
(e.g., in your spider’s <code class="docutils literal"><span class="pre">__init__()</span></code> method), you’ll need to override the
<a class="reference internal" href="spiders.html#scrapy.spiders.Spider.from_crawler" title="scrapy.spiders.Spider.from_crawler"><code class="xref py py-meth docutils literal"><span class="pre">from_crawler()</span></code></a> method.</p>
<p>Settings can be accessed through the <a class="reference internal" href="api.html#scrapy.crawler.Crawler.settings" title="scrapy.crawler.Crawler.settings"><code class="xref py py-attr docutils literal"><span class="pre">scrapy.crawler.Crawler.settings</span></code></a>
attribute of the Crawler that is passed to <code class="docutils literal"><span class="pre">from_crawler</span></code> method in
extensions, middlewares and item pipelines:</p>
<p>The settings object can be used like a dict (e.g.,
<code class="docutils literal"><span class="pre">settings['LOG_ENABLED']</span></code>), but it’s usually preferred to extract the setting
in the format you need it to avoid type errors, using one of the methods
provided by the <a class="reference internal" href="api.html#scrapy.settings.Settings" title="scrapy.settings.Settings"><code class="xref py py-class docutils literal"><span class="pre">Settings</span></code></a> API.</p>
<p>Setting names are usually prefixed with the component that they configure. For
example, proper setting names for a fictional robots.txt extension would be
<code class="docutils literal"><span class="pre">ROBOTSTXT_ENABLED</span></code>, <code class="docutils literal"><span class="pre">ROBOTSTXT_OBEY</span></code>, <code class="docutils literal"><span class="pre">ROBOTSTXT_CACHEDIR</span></code>, etc.</p>
<p>Here’s a list of all available Scrapy settings, in alphabetical order, along
with their default values and the scope where they apply.</p>
<p>The scope, where available, shows where the setting is being used, if it’s tied
to any particular component. In that case the module of that component will be
shown, typically an extension, middleware or pipeline. It also means that the
component must be enabled in order for the setting to have any effect.</p>
<p>Default: <code class="docutils literal"><span class="pre">None</span></code></p>
<p>The AWS access key used by code that requires access to <a class="reference external" href="https://aws.amazon.com/">Amazon Web services</a>,
such as the <a class="reference internal" href="feed-exports.html#topics-feed-storage-s3"><span class="std std-ref">S3 feed storage backend</span></a>.</p>
<p>Default: <code class="docutils literal"><span class="pre">None</span></code></p>
<p>The AWS secret key used by code that requires access to <a class="reference external" href="https://aws.amazon.com/">Amazon Web services</a>,
such as the <a class="reference internal" href="feed-exports.html#topics-feed-storage-s3"><span class="std std-ref">S3 feed storage backend</span></a>.</p>
<p>Default: <code class="docutils literal"><span class="pre">'scrapybot'</span></code></p>
<p>The name of the bot implemented by this Scrapy project (also known as the
project name). This will be used to construct the User-Agent by default, and
also for logging.</p>
<p>It’s automatically populated with your project name when you create your
project with the <a class="reference internal" href="commands.html#std:command-startproject"><code class="xref std std-command docutils literal"><span class="pre">startproject</span></code></a> command.</p>
<p>Default: <code class="docutils literal"><span class="pre">100</span></code></p>
<p>Maximum number of concurrent items (per response) to process in parallel in the
Item Processor (also known as the <a class="reference internal" href="item-pipeline.html#topics-item-pipeline"><span class="std std-ref">Item Pipeline</span></a>).</p>
<p>Default: <code class="docutils literal"><span class="pre">16</span></code></p>
<p>The maximum number of concurrent (ie. simultaneous) requests that will be
performed by the Scrapy downloader.</p>
<p>Default: <code class="docutils literal"><span class="pre">8</span></code></p>
<p>The maximum number of concurrent (ie. simultaneous) requests that will be
performed to any single domain.</p>
<p>See also: <a class="reference internal" href="autothrottle.html#topics-autothrottle"><span class="std std-ref">AutoThrottle extension</span></a> and its
<a class="reference internal" href="autothrottle.html#std:setting-AUTOTHROTTLE_TARGET_CONCURRENCY"><code class="xref std std-setting docutils literal"><span class="pre">AUTOTHROTTLE_TARGET_CONCURRENCY</span></code></a> option.</p>
<p>Default: <code class="docutils literal"><span class="pre">0</span></code></p>
<p>The maximum number of concurrent (ie. simultaneous) requests that will be
performed to any single IP. If non-zero, the
<a class="reference internal" href="#std:setting-CONCURRENT_REQUESTS_PER_DOMAIN"><code class="xref std std-setting docutils literal"><span class="pre">CONCURRENT_REQUESTS_PER_DOMAIN</span></code></a> setting is ignored, and this one is
used instead. In other words, concurrency limits will be applied per IP, not
per domain.</p>
<p>This setting also affects <a class="reference internal" href="#std:setting-DOWNLOAD_DELAY"><code class="xref std std-setting docutils literal"><span class="pre">DOWNLOAD_DELAY</span></code></a> and
<a class="reference internal" href="autothrottle.html#topics-autothrottle"><span class="std std-ref">AutoThrottle extension</span></a>: if <a class="reference internal" href="#std:setting-CONCURRENT_REQUESTS_PER_IP"><code class="xref std std-setting docutils literal"><span class="pre">CONCURRENT_REQUESTS_PER_IP</span></code></a>
is non-zero, download delay is enforced per IP, not per domain.</p>
<p>Default: <code class="docutils literal"><span class="pre">'scrapy.item.Item'</span></code></p>
<p>The default class that will be used for instantiating items in the <a class="reference internal" href="shell.html#topics-shell"><span class="std std-ref">the
Scrapy shell</span></a>.</p>
<p>Default:</p>
<p>The default headers used for Scrapy HTTP Requests. They’re populated in the
<a class="reference internal" href="downloader-middleware.html#scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware" title="scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware"><code class="xref py py-class docutils literal"><span class="pre">DefaultHeadersMiddleware</span></code></a>.</p>
<p>Default: <code class="docutils literal"><span class="pre">0</span></code></p>
<p>Scope: <code class="docutils literal"><span class="pre">scrapy.spidermiddlewares.depth.DepthMiddleware</span></code></p>
<p>The maximum depth that will be allowed to crawl for any site. If zero, no limit
will be imposed.</p>
<p>Default: <code class="docutils literal"><span class="pre">0</span></code></p>
<p>Scope: <code class="docutils literal"><span class="pre">scrapy.spidermiddlewares.depth.DepthMiddleware</span></code></p>
<p>An integer that is used to adjust the request priority based on its depth:</p>
<p>See also: <a class="reference internal" href="../faq.html#faq-bfo-dfo"><span class="std std-ref">Does Scrapy crawl in breadth-first or depth-first order?</span></a> about tuning Scrapy for BFO or DFO.</p>
<p class="first admonition-title">Note</p>
<p class="last">This setting adjusts priority <strong>in the opposite way</strong> compared to
other priority settings <a class="reference internal" href="#std:setting-REDIRECT_PRIORITY_ADJUST"><code class="xref std std-setting docutils literal"><span class="pre">REDIRECT_PRIORITY_ADJUST</span></code></a>
and <a class="reference internal" href="#std:setting-RETRY_PRIORITY_ADJUST"><code class="xref std std-setting docutils literal"><span class="pre">RETRY_PRIORITY_ADJUST</span></code></a>.</p>
<p>Default: <code class="docutils literal"><span class="pre">True</span></code></p>
<p>Scope: <code class="docutils literal"><span class="pre">scrapy.spidermiddlewares.depth.DepthMiddleware</span></code></p>
<p>Whether to collect maximum depth stats.</p>
<p>Default: <code class="docutils literal"><span class="pre">False</span></code></p>
<p>Scope: <code class="docutils literal"><span class="pre">scrapy.spidermiddlewares.depth.DepthMiddleware</span></code></p>
<p>Whether to collect verbose depth stats. If this is enabled, the number of
requests for each depth is collected in the stats.</p>
<p>Default: <code class="docutils literal"><span class="pre">True</span></code></p>
<p>Whether to enable DNS in-memory cache.</p>
<p>Default: <code class="docutils literal"><span class="pre">10000</span></code></p>
<p>DNS in-memory cache size.</p>
<p>Default: <code class="docutils literal"><span class="pre">60</span></code></p>
<p>Timeout for processing of DNS queries in seconds. Float is supported.</p>
<p>Default: <code class="docutils literal"><span class="pre">'scrapy.core.downloader.Downloader'</span></code></p>
<p>The downloader to use for crawling.</p>
<p>Default: <code class="docutils literal"><span class="pre">'scrapy.core.downloader.webclient.ScrapyHTTPClientFactory'</span></code></p>
<p>Defines a Twisted <code class="docutils literal"><span class="pre">protocol.ClientFactory</span></code>  class to use for HTTP/1.0
connections (for <code class="docutils literal"><span class="pre">HTTP10DownloadHandler</span></code>).</p>
<p class="first admonition-title">Note</p>
<p class="last">HTTP/1.0 is rarely used nowadays so you can safely ignore this setting,
unless you use Twisted&lt;11.1, or if you really want to use HTTP/1.0
and override <a class="reference internal" href="#std:setting-DOWNLOAD_HANDLERS_BASE"><code class="xref std std-setting docutils literal"><span class="pre">DOWNLOAD_HANDLERS_BASE</span></code></a> for <code class="docutils literal"><span class="pre">http(s)</span></code> scheme
accordingly, i.e. to
<code class="docutils literal"><span class="pre">'scrapy.core.downloader.handlers.http.HTTP10DownloadHandler'</span></code>.</p>
<p>Default: <code class="docutils literal"><span class="pre">'scrapy.core.downloader.contextfactory.ScrapyClientContextFactory'</span></code></p>
<p>Represents the classpath to the ContextFactory to use.</p>
<p>Here, “ContextFactory” is a Twisted term for SSL/TLS contexts, defining
the TLS/SSL protocol version to use, whether to do certificate verification,
or even enable client-side authentication (and various other things).</p>
<p class="first admonition-title">Note</p>
<p>Scrapy default context factory <strong>does NOT perform remote server
certificate verification</strong>. This is usually fine for web scraping.</p>
<p class="last">If you do need remote server certificate verification enabled,
Scrapy also has another context factory class that you can set,
<code class="docutils literal"><span class="pre">'scrapy.core.downloader.contextfactory.BrowserLikeContextFactory'</span></code>,
which uses the platform’s certificates to validate remote endpoints.
<strong>This is only available if you use Twisted&gt;=14.0.</strong></p>
<p>If you do use a custom ContextFactory, make sure it accepts a <code class="docutils literal"><span class="pre">method</span></code>
parameter at init (this is the <code class="docutils literal"><span class="pre">OpenSSL.SSL</span></code> method mapping
<a class="reference internal" href="#std:setting-DOWNLOADER_CLIENT_TLS_METHOD"><code class="xref std std-setting docutils literal"><span class="pre">DOWNLOADER_CLIENT_TLS_METHOD</span></code></a>).</p>
<p>Default: <code class="docutils literal"><span class="pre">'TLS'</span></code></p>
<p>Use this setting to customize the TLS/SSL method used by the default
HTTP/1.1 downloader.</p>
<p>This setting must be one of these string values:</p>
<p class="first admonition-title">Note</p>
<p class="last">We recommend that you use PyOpenSSL&gt;=0.13 and Twisted&gt;=0.13
or above (Twisted&gt;=14.0 if you can).</p>
<p>Default:: <code class="docutils literal"><span class="pre">{}</span></code></p>
<p>A dict containing the downloader middlewares enabled in your project, and their
orders. For more info see <a class="reference internal" href="downloader-middleware.html#topics-downloader-middleware-setting"><span class="std std-ref">Activating a downloader middleware</span></a>.</p>
<p>Default:</p>
<p>A dict containing the downloader middlewares enabled by default in Scrapy. Low
orders are closer to the engine, high orders are closer to the downloader. You
should never modify this setting in your project, modify
<a class="reference internal" href="#std:setting-DOWNLOADER_MIDDLEWARES"><code class="xref std std-setting docutils literal"><span class="pre">DOWNLOADER_MIDDLEWARES</span></code></a> instead.  For more info see
<a class="reference internal" href="downloader-middleware.html#topics-downloader-middleware-setting"><span class="std std-ref">Activating a downloader middleware</span></a>.</p>
<p>Default: <code class="docutils literal"><span class="pre">True</span></code></p>
<p>Whether to enable downloader stats collection.</p>
<p>Default: <code class="docutils literal"><span class="pre">0</span></code></p>
<p>The amount of time (in secs) that the downloader should wait before downloading
consecutive pages from the same website. This can be used to throttle the
crawling speed to avoid hitting servers too hard. Decimal numbers are
supported.  Example:</p>
<p>This setting is also affected by the <a class="reference internal" href="#std:setting-RANDOMIZE_DOWNLOAD_DELAY"><code class="xref std std-setting docutils literal"><span class="pre">RANDOMIZE_DOWNLOAD_DELAY</span></code></a>
setting (which is enabled by default). By default, Scrapy doesn’t wait a fixed
amount of time between requests, but uses a random interval between 0.5 * <a class="reference internal" href="#std:setting-DOWNLOAD_DELAY"><code class="xref std std-setting docutils literal"><span class="pre">DOWNLOAD_DELAY</span></code></a> and 1.5 * <a class="reference internal" href="#std:setting-DOWNLOAD_DELAY"><code class="xref std std-setting docutils literal"><span class="pre">DOWNLOAD_DELAY</span></code></a>.</p>
<p>When <a class="reference internal" href="#std:setting-CONCURRENT_REQUESTS_PER_IP"><code class="xref std std-setting docutils literal"><span class="pre">CONCURRENT_REQUESTS_PER_IP</span></code></a> is non-zero, delays are enforced
per ip address instead of per domain.</p>
<p>You can also change this setting per spider by setting <code class="docutils literal"><span class="pre">download_delay</span></code>
spider attribute.</p>
<p>Default: <code class="docutils literal"><span class="pre">{}</span></code></p>
<p>A dict containing the request downloader handlers enabled in your project.
See <a class="reference internal" href="#std:setting-DOWNLOAD_HANDLERS_BASE"><code class="xref std std-setting docutils literal"><span class="pre">DOWNLOAD_HANDLERS_BASE</span></code></a> for example format.</p>
<p>Default:</p>
<p>A dict containing the request download handlers enabled by default in Scrapy.
You should never modify this setting in your project, modify
<a class="reference internal" href="#std:setting-DOWNLOAD_HANDLERS"><code class="xref std std-setting docutils literal"><span class="pre">DOWNLOAD_HANDLERS</span></code></a> instead.</p>
<p>You can disable any of these download handlers by assigning <code class="docutils literal"><span class="pre">None</span></code> to their
URI scheme in <a class="reference internal" href="#std:setting-DOWNLOAD_HANDLERS"><code class="xref std std-setting docutils literal"><span class="pre">DOWNLOAD_HANDLERS</span></code></a>. E.g., to disable the built-in FTP
handler (without replacement), place this in your <code class="docutils literal"><span class="pre">settings.py</span></code>:</p>
<p>Default: <code class="docutils literal"><span class="pre">180</span></code></p>
<p>The amount of time (in secs) that the downloader will wait before timing out.</p>
<p class="first admonition-title">Note</p>
<p class="last">This timeout can be set per spider using <code class="xref py py-attr docutils literal"><span class="pre">download_timeout</span></code>
spider attribute and per-request using <a class="reference internal" href="request-response.html#std:reqmeta-download_timeout"><code class="xref std std-reqmeta docutils literal"><span class="pre">download_timeout</span></code></a>
Request.meta key.</p>
<p>Default: <cite>1073741824</cite> (1024MB)</p>
<p>The maximum response size (in bytes) that downloader will download.</p>
<p>If you want to disable it set to 0.</p>
<p class="first admonition-title">Note</p>
<p>This size can be set per spider using <code class="xref py py-attr docutils literal"><span class="pre">download_maxsize</span></code>
spider attribute and per-request using <a class="reference internal" href="#std:reqmeta-download_maxsize"><code class="xref std std-reqmeta docutils literal"><span class="pre">download_maxsize</span></code></a>
Request.meta key.</p>
<p class="last">This feature needs Twisted &gt;= 11.1.</p>
<p>Default: <cite>33554432</cite> (32MB)</p>
<p>The response size (in bytes) that downloader will start to warn.</p>
<p>If you want to disable it set to 0.</p>
<p class="first admonition-title">Note</p>
<p>This size can be set per spider using <code class="xref py py-attr docutils literal"><span class="pre">download_warnsize</span></code>
spider attribute and per-request using <code class="xref std std-reqmeta docutils literal"><span class="pre">download_warnsize</span></code>
Request.meta key.</p>
<p class="last">This feature needs Twisted &gt;= 11.1.</p>
<p>Default: <code class="docutils literal"><span class="pre">True</span></code></p>
<p>Whether or not to fail on broken responses, that is, declared
<code class="docutils literal"><span class="pre">Content-Length</span></code> does not match content sent by the server or chunked
response was not properly finish. If <code class="docutils literal"><span class="pre">True</span></code>, these responses raise a
<code class="docutils literal"><span class="pre">ResponseFailed([_DataLoss])</span></code> error. If <code class="docutils literal"><span class="pre">False</span></code>, these responses
are passed through and the flag <code class="docutils literal"><span class="pre">dataloss</span></code> is added to the response, i.e.:
<code class="docutils literal"><span class="pre">'dataloss'</span> <span class="pre">in</span> <span class="pre">response.flags</span></code> is <code class="docutils literal"><span class="pre">True</span></code>.</p>
<p>Optionally, this can be set per-request basis by using the
<a class="reference internal" href="request-response.html#std:reqmeta-download_fail_on_dataloss"><code class="xref std std-reqmeta docutils literal"><span class="pre">download_fail_on_dataloss</span></code></a> Request.meta key to <code class="docutils literal"><span class="pre">False</span></code>.</p>
<p class="first admonition-title">Note</p>
<p class="last">A broken response, or data loss error, may happen under several
circumstances, from server misconfiguration to network errors to data
corruption. It is up to the user to decide if it makes sense to process
broken responses considering they may contain partial or incomplete content.
If setting:<cite>RETRY_ENABLED</cite> is <code class="docutils literal"><span class="pre">True</span></code> and this setting is set to <code class="docutils literal"><span class="pre">True</span></code>,
the <code class="docutils literal"><span class="pre">ResponseFailed([_DataLoss])</span></code> failure will be retried as usual.</p>
<p>Default: <code class="docutils literal"><span class="pre">'scrapy.dupefilters.RFPDupeFilter'</span></code></p>
<p>The class used to detect and filter duplicate requests.</p>
<p>The default (<code class="docutils literal"><span class="pre">RFPDupeFilter</span></code>) filters based on request fingerprint using
the <code class="docutils literal"><span class="pre">scrapy.utils.request.request_fingerprint</span></code> function. In order to change
the way duplicates are checked you could subclass <code class="docutils literal"><span class="pre">RFPDupeFilter</span></code> and
override its <code class="docutils literal"><span class="pre">request_fingerprint</span></code> method. This method should accept
scrapy <a class="reference internal" href="request-response.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal"><span class="pre">Request</span></code></a> object and return its fingerprint
(a string).</p>
<p>You can disable filtering of duplicate requests by setting
<a class="reference internal" href="#std:setting-DUPEFILTER_CLASS"><code class="xref std std-setting docutils literal"><span class="pre">DUPEFILTER_CLASS</span></code></a> to <code class="docutils literal"><span class="pre">'scrapy.dupefilters.BaseDupeFilter'</span></code>.
Be very careful about this however, because you can get into crawling loops.
It’s usually a better idea to set the <code class="docutils literal"><span class="pre">dont_filter</span></code> parameter to
<code class="docutils literal"><span class="pre">True</span></code> on the specific <a class="reference internal" href="request-response.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal"><span class="pre">Request</span></code></a> that should not be
filtered.</p>
<p>Default: <code class="docutils literal"><span class="pre">False</span></code></p>
<p>By default, <code class="docutils literal"><span class="pre">RFPDupeFilter</span></code> only logs the first duplicate request.
Setting <a class="reference internal" href="#std:setting-DUPEFILTER_DEBUG"><code class="xref std std-setting docutils literal"><span class="pre">DUPEFILTER_DEBUG</span></code></a> to <code class="docutils literal"><span class="pre">True</span></code> will make it log all duplicate requests.</p>
<p>Default: <code class="docutils literal"><span class="pre">vi</span></code> (on Unix systems) or the IDLE editor (on Windows)</p>
<p>The editor to use for editing spiders with the <a class="reference internal" href="commands.html#std:command-edit"><code class="xref std std-command docutils literal"><span class="pre">edit</span></code></a> command.
Additionally, if the <code class="docutils literal"><span class="pre">EDITOR</span></code> environment variable is set, the <a class="reference internal" href="commands.html#std:command-edit"><code class="xref std std-command docutils literal"><span class="pre">edit</span></code></a>
command will prefer it over the default setting.</p>
<p>Default:: <code class="docutils literal"><span class="pre">{}</span></code></p>
<p>A dict containing the extensions enabled in your project, and their orders.</p>
<p>Default:</p>
<p>A dict containing the extensions available by default in Scrapy, and their
orders. This setting contains all stable built-in extensions. Keep in mind that
some of them need to be enabled through a setting.</p>
<p>For more information See the <a class="reference internal" href="extensions.html#topics-extensions"><span class="std std-ref">extensions user guide</span></a>
and the <a class="reference internal" href="extensions.html#topics-extensions-ref"><span class="std std-ref">list of available extensions</span></a>.</p>
<p>The Feed Temp dir allows you to set a custom folder to save crawler
temporary files before uploading with <a class="reference internal" href="feed-exports.html#topics-feed-storage-ftp"><span class="std std-ref">FTP feed storage</span></a> and
<a class="reference internal" href="feed-exports.html#topics-feed-storage-s3"><span class="std std-ref">Amazon S3</span></a>.</p>
<p>Default: <code class="docutils literal"><span class="pre">True</span></code></p>
<p>Whether or not to use passive mode when initiating FTP transfers.</p>
<p>Default: <code class="docutils literal"><span class="pre">"guest"</span></code></p>
<p>The password to use for FTP connections when there is no <code class="docutils literal"><span class="pre">"ftp_password"</span></code>
in <code class="docutils literal"><span class="pre">Request</span></code> meta.</p>
<p class="first admonition-title">Note</p>
<p class="last">Paraphrasing <a class="reference external" href="https://tools.ietf.org/html/rfc1635">RFC 1635</a>, although it is common to use either the password
“guest” or one’s e-mail address for anonymous FTP,
some FTP servers explicitly ask for the user’s e-mail address
and will not allow login with the “guest” password.</p>
<p>Default: <code class="docutils literal"><span class="pre">"anonymous"</span></code></p>
<p>The username to use for FTP connections when there is no <code class="docutils literal"><span class="pre">"ftp_user"</span></code>
in <code class="docutils literal"><span class="pre">Request</span></code> meta.</p>
<p>Default: <code class="docutils literal"><span class="pre">{}</span></code></p>
<p>A dict containing the item pipelines to use, and their orders. Order values are
arbitrary, but it is customary to define them in the 0-1000 range. Lower orders
process before higher orders.</p>
<p>Example:</p>
<p>Default: <code class="docutils literal"><span class="pre">{}</span></code></p>
<p>A dict containing the pipelines enabled by default in Scrapy. You should never
modify this setting in your project, modify <a class="reference internal" href="#std:setting-ITEM_PIPELINES"><code class="xref std std-setting docutils literal"><span class="pre">ITEM_PIPELINES</span></code></a> instead.</p>
<p>Default: <code class="docutils literal"><span class="pre">True</span></code></p>
<p>Whether to enable logging.</p>
<p>Default: <code class="docutils literal"><span class="pre">'utf-8'</span></code></p>
<p>The encoding to use for logging.</p>
<p>Default: <code class="docutils literal"><span class="pre">None</span></code></p>
<p>File name to use for logging output. If <code class="docutils literal"><span class="pre">None</span></code>, standard error will be used.</p>
<p>Default: <code class="docutils literal"><span class="pre">'%(asctime)s</span> <span class="pre">[%(name)s]</span> <span class="pre">%(levelname)s:</span> <span class="pre">%(message)s'</span></code></p>
<p>String for formatting log messsages. Refer to the <a class="reference external" href="https://docs.python.org/2/library/logging.html#logrecord-attributes">Python logging documentation</a> for the whole list of available
placeholders.</p>
<p>Default: <code class="docutils literal"><span class="pre">'%Y-%m-%d</span> <span class="pre">%H:%M:%S'</span></code></p>
<p>String for formatting date/time, expansion of the <code class="docutils literal"><span class="pre">%(asctime)s</span></code> placeholder
in <a class="reference internal" href="#std:setting-LOG_FORMAT"><code class="xref std std-setting docutils literal"><span class="pre">LOG_FORMAT</span></code></a>. Refer to the <a class="reference external" href="https://docs.python.org/2/library/datetime.html#strftime-and-strptime-behavior">Python datetime documentation</a> for the whole list of available
directives.</p>
<p>Default: <code class="docutils literal"><span class="pre">'DEBUG'</span></code></p>
<p>Minimum level to log. Available levels are: CRITICAL, ERROR, WARNING,
INFO, DEBUG. For more info see <a class="reference internal" href="logging.html#topics-logging"><span class="std std-ref">Logging</span></a>.</p>
<p>Default: <code class="docutils literal"><span class="pre">False</span></code></p>
<p>If <code class="docutils literal"><span class="pre">True</span></code>, all standard output (and error) of your process will be redirected
to the log. For example if you <code class="docutils literal"><span class="pre">print</span> <span class="pre">'hello'</span></code> it will appear in the Scrapy
log.</p>
<p>Default: <code class="docutils literal"><span class="pre">False</span></code></p>
<p>If <code class="docutils literal"><span class="pre">True</span></code>, the logs will just contain the root path. If it is set to <code class="docutils literal"><span class="pre">False</span></code>
then it displays the component responsible for the log output</p>
<p>Default: <code class="docutils literal"><span class="pre">False</span></code></p>
<p>Whether to enable memory debugging.</p>
<p>Default: <code class="docutils literal"><span class="pre">[]</span></code></p>
<p>When memory debugging is enabled a memory report will be sent to the specified
addresses if this setting is not empty, otherwise the report will be written to
the log.</p>
<p>Example:</p>
<p>Default: <code class="docutils literal"><span class="pre">True</span></code></p>
<p>Scope: <code class="docutils literal"><span class="pre">scrapy.extensions.memusage</span></code></p>
<p>Whether to enable the memory usage extension. This extension keeps track of
a peak memory used by the process (it writes it to stats). It can also
optionally shutdown the Scrapy process when it exceeds a memory limit
(see <a class="reference internal" href="#std:setting-MEMUSAGE_LIMIT_MB"><code class="xref std std-setting docutils literal"><span class="pre">MEMUSAGE_LIMIT_MB</span></code></a>), and notify by email when that happened
(see <a class="reference internal" href="#std:setting-MEMUSAGE_NOTIFY_MAIL"><code class="xref std std-setting docutils literal"><span class="pre">MEMUSAGE_NOTIFY_MAIL</span></code></a>).</p>
<p>See <a class="reference internal" href="extensions.html#topics-extensions-ref-memusage"><span class="std std-ref">Memory usage extension</span></a>.</p>
<p>Default: <code class="docutils literal"><span class="pre">0</span></code></p>
<p>Scope: <code class="docutils literal"><span class="pre">scrapy.extensions.memusage</span></code></p>
<p>The maximum amount of memory to allow (in megabytes) before shutting down
Scrapy  (if MEMUSAGE_ENABLED is True). If zero, no check will be performed.</p>
<p>See <a class="reference internal" href="extensions.html#topics-extensions-ref-memusage"><span class="std std-ref">Memory usage extension</span></a>.</p>
<p><span class="versionmodified">New in version 1.1.</span></p>
<p>Default: <code class="docutils literal"><span class="pre">60.0</span></code></p>
<p>Scope: <code class="docutils literal"><span class="pre">scrapy.extensions.memusage</span></code></p>
<p>The <a class="reference internal" href="extensions.html#topics-extensions-ref-memusage"><span class="std std-ref">Memory usage extension</span></a>
checks the current memory usage, versus the limits set by
<a class="reference internal" href="#std:setting-MEMUSAGE_LIMIT_MB"><code class="xref std std-setting docutils literal"><span class="pre">MEMUSAGE_LIMIT_MB</span></code></a> and <a class="reference internal" href="#std:setting-MEMUSAGE_WARNING_MB"><code class="xref std std-setting docutils literal"><span class="pre">MEMUSAGE_WARNING_MB</span></code></a>,
at fixed time intervals.</p>
<p>This sets the length of these intervals, in seconds.</p>
<p>See <a class="reference internal" href="extensions.html#topics-extensions-ref-memusage"><span class="std std-ref">Memory usage extension</span></a>.</p>
<p>Default: <code class="docutils literal"><span class="pre">False</span></code></p>
<p>Scope: <code class="docutils literal"><span class="pre">scrapy.extensions.memusage</span></code></p>
<p>A list of emails to notify if the memory limit has been reached.</p>
<p>Example:</p>
<p>See <a class="reference internal" href="extensions.html#topics-extensions-ref-memusage"><span class="std std-ref">Memory usage extension</span></a>.</p>
<p>Default: <code class="docutils literal"><span class="pre">0</span></code></p>
<p>Scope: <code class="docutils literal"><span class="pre">scrapy.extensions.memusage</span></code></p>
<p>The maximum amount of memory to allow (in megabytes) before sending a warning
email notifying about it. If zero, no warning will be produced.</p>
<p>Default: <code class="docutils literal"><span class="pre">''</span></code></p>
<p>Module where to create new spiders using the <a class="reference internal" href="commands.html#std:command-genspider"><code class="xref std std-command docutils literal"><span class="pre">genspider</span></code></a> command.</p>
<p>Example:</p>
<p>Default: <code class="docutils literal"><span class="pre">True</span></code></p>
<p>If enabled, Scrapy will wait a random amount of time (between 0.5 * <a class="reference internal" href="#std:setting-DOWNLOAD_DELAY"><code class="xref std std-setting docutils literal"><span class="pre">DOWNLOAD_DELAY</span></code></a> and 1.5 * <a class="reference internal" href="#std:setting-DOWNLOAD_DELAY"><code class="xref std std-setting docutils literal"><span class="pre">DOWNLOAD_DELAY</span></code></a>) while fetching requests from the same
website.</p>
<p>This randomization decreases the chance of the crawler being detected (and
subsequently blocked) by sites which analyze requests looking for statistically
significant similarities in the time between their requests.</p>
<p>The randomization policy is the same used by <a class="reference external" href="http://www.gnu.org/software/wget/manual/wget.html">wget</a> <code class="docutils literal"><span class="pre">--random-wait</span></code> option.</p>
<p>If <a class="reference internal" href="#std:setting-DOWNLOAD_DELAY"><code class="xref std std-setting docutils literal"><span class="pre">DOWNLOAD_DELAY</span></code></a> is zero (default) this option has no effect.</p>
<p>Default: <code class="docutils literal"><span class="pre">10</span></code></p>
<p>The maximum limit for Twisted Reactor thread pool size. This is common
multi-purpose thread pool used by various Scrapy components. Threaded
DNS Resolver, BlockingFeedStorage, S3FilesStore just to name a few. Increase
this value if you’re experiencing problems with insufficient blocking IO.</p>
<p>Default: <code class="docutils literal"><span class="pre">20</span></code></p>
<p>Defines the maximum times a request can be redirected. After this maximum the
request’s response is returned as is. We used Firefox default value for the
same task.</p>
<p>Default: <code class="docutils literal"><span class="pre">+2</span></code></p>
<p>Scope: <code class="docutils literal"><span class="pre">scrapy.downloadermiddlewares.redirect.RedirectMiddleware</span></code></p>
<p>Adjust redirect request priority relative to original request:</p>
<p>Default: <code class="docutils literal"><span class="pre">-1</span></code></p>
<p>Scope: <code class="docutils literal"><span class="pre">scrapy.downloadermiddlewares.retry.RetryMiddleware</span></code></p>
<p>Adjust retry request priority relative to original request:</p>
<p>Default: <code class="docutils literal"><span class="pre">False</span></code></p>
<p>Scope: <code class="docutils literal"><span class="pre">scrapy.downloadermiddlewares.robotstxt</span></code></p>
<p>If enabled, Scrapy will respect robots.txt policies. For more information see
<a class="reference internal" href="downloader-middleware.html#topics-dlmw-robots"><span class="std std-ref">RobotsTxtMiddleware</span></a>.</p>
<p class="first admonition-title">Note</p>
<p class="last">While the default value is <code class="docutils literal"><span class="pre">False</span></code> for historical reasons,
this option is enabled by default in settings.py file generated
by <code class="docutils literal"><span class="pre">scrapy</span> <span class="pre">startproject</span></code> command.</p>
<p>Default: <code class="docutils literal"><span class="pre">'scrapy.core.scheduler.Scheduler'</span></code></p>
<p>The scheduler to use for crawling.</p>
<p>Default: <code class="docutils literal"><span class="pre">False</span></code></p>
<p>Setting to <code class="docutils literal"><span class="pre">True</span></code> will log debug information about the requests scheduler.
This currently logs (only once) if the requests cannot be serialized to disk.
Stats counter (<code class="docutils literal"><span class="pre">scheduler/unserializable</span></code>) tracks the number of times this happens.</p>
<p>Example entry in logs:</p>
<p>Default: <code class="docutils literal"><span class="pre">'scrapy.squeues.PickleLifoDiskQueue'</span></code></p>
<p>Type of disk queue that will be used by scheduler. Other available types are
<code class="docutils literal"><span class="pre">scrapy.squeues.PickleFifoDiskQueue</span></code>, <code class="docutils literal"><span class="pre">scrapy.squeues.MarshalFifoDiskQueue</span></code>,
<code class="docutils literal"><span class="pre">scrapy.squeues.MarshalLifoDiskQueue</span></code>.</p>
<p>Default: <code class="docutils literal"><span class="pre">'scrapy.squeues.LifoMemoryQueue'</span></code></p>
<p>Type of in-memory queue used by scheduler. Other available type is:
<code class="docutils literal"><span class="pre">scrapy.squeues.FifoMemoryQueue</span></code>.</p>
<p>Default: <code class="docutils literal"><span class="pre">'queuelib.PriorityQueue'</span></code></p>
<p>Type of priority queue used by scheduler.</p>
<p>Default:: <code class="docutils literal"><span class="pre">{}</span></code></p>
<p>A dict containing the spider contracts enabled in your project, used for
testing spiders. For more info see <a class="reference internal" href="contracts.html#topics-contracts"><span class="std std-ref">Spiders Contracts</span></a>.</p>
<p>Default:</p>
<p>A dict containing the scrapy contracts enabled by default in Scrapy. You should
never modify this setting in your project, modify <a class="reference internal" href="#std:setting-SPIDER_CONTRACTS"><code class="xref std std-setting docutils literal"><span class="pre">SPIDER_CONTRACTS</span></code></a>
instead. For more info see <a class="reference internal" href="contracts.html#topics-contracts"><span class="std std-ref">Spiders Contracts</span></a>.</p>
<p>You can disable any of these contracts by assigning <code class="docutils literal"><span class="pre">None</span></code> to their class
path in <a class="reference internal" href="#std:setting-SPIDER_CONTRACTS"><code class="xref std std-setting docutils literal"><span class="pre">SPIDER_CONTRACTS</span></code></a>. E.g., to disable the built-in
<code class="docutils literal"><span class="pre">ScrapesContract</span></code>, place this in your <code class="docutils literal"><span class="pre">settings.py</span></code>:</p>
<p>Default: <code class="docutils literal"><span class="pre">'scrapy.spiderloader.SpiderLoader'</span></code></p>
<p>The class that will be used for loading spiders, which must implement the
<a class="reference internal" href="api.html#topics-api-spiderloader"><span class="std std-ref">SpiderLoader API</span></a>.</p>
<p><span class="versionmodified">New in version 1.3.3.</span></p>
<p>Default: <code class="docutils literal"><span class="pre">False</span></code></p>
<p>By default, when scrapy tries to import spider classes from <a class="reference internal" href="#std:setting-SPIDER_MODULES"><code class="xref std std-setting docutils literal"><span class="pre">SPIDER_MODULES</span></code></a>,
it will fail loudly if there is any <code class="docutils literal"><span class="pre">ImportError</span></code> exception.
But you can choose to silence this exception and turn it into a simple
warning by setting <code class="docutils literal"><span class="pre">SPIDER_LOADER_WARN_ONLY</span> <span class="pre">=</span> <span class="pre">True</span></code>.</p>
<p class="first admonition-title">Note</p>
<p class="last">Some <a class="reference internal" href="commands.html#topics-commands"><span class="std std-ref">scrapy commands</span></a> run with this setting to <code class="docutils literal"><span class="pre">True</span></code>
already (i.e. they will only issue a warning and will not fail)
since they do not actually need to load spider classes to work:
<a class="reference internal" href="commands.html#std:command-runspider"><code class="xref std std-command docutils literal"><span class="pre">scrapy</span> <span class="pre">runspider</span></code></a>,
<a class="reference internal" href="commands.html#std:command-settings"><code class="xref std std-command docutils literal"><span class="pre">scrapy</span> <span class="pre">settings</span></code></a>,
<a class="reference internal" href="commands.html#std:command-startproject"><code class="xref std std-command docutils literal"><span class="pre">scrapy</span> <span class="pre">startproject</span></code></a>,
<a class="reference internal" href="commands.html#std:command-version"><code class="xref std std-command docutils literal"><span class="pre">scrapy</span> <span class="pre">version</span></code></a>.</p>
<p>Default:: <code class="docutils literal"><span class="pre">{}</span></code></p>
<p>A dict containing the spider middlewares enabled in your project, and their
orders. For more info see <a class="reference internal" href="spider-middleware.html#topics-spider-middleware-setting"><span class="std std-ref">Activating a spider middleware</span></a>.</p>
<p>Default:</p>
<p>A dict containing the spider middlewares enabled by default in Scrapy, and
their orders. Low orders are closer to the engine, high orders are closer to
the spider. For more info see <a class="reference internal" href="spider-middleware.html#topics-spider-middleware-setting"><span class="std std-ref">Activating a spider middleware</span></a>.</p>
<p>Default: <code class="docutils literal"><span class="pre">[]</span></code></p>
<p>A list of modules where Scrapy will look for spiders.</p>
<p>Example:</p>
<p>Default: <code class="docutils literal"><span class="pre">'scrapy.statscollectors.MemoryStatsCollector'</span></code></p>
<p>The class to use for collecting stats, who must implement the
<a class="reference internal" href="api.html#topics-api-stats"><span class="std std-ref">Stats Collector API</span></a>.</p>
<p>Default: <code class="docutils literal"><span class="pre">True</span></code></p>
<p>Dump the <a class="reference internal" href="stats.html#topics-stats"><span class="std std-ref">Scrapy stats</span></a> (to the Scrapy log) once the spider
finishes.</p>
<p>For more info see: <a class="reference internal" href="stats.html#topics-stats"><span class="std std-ref">Stats Collection</span></a>.</p>
<p>Default: <code class="docutils literal"><span class="pre">[]</span></code> (empty list)</p>
<p>Send Scrapy stats after spiders finish scraping. See
<code class="xref py py-class docutils literal"><span class="pre">StatsMailer</span></code> for more info.</p>
<p>Default: <code class="docutils literal"><span class="pre">True</span></code></p>
<p>A boolean which specifies if the <a class="reference internal" href="telnetconsole.html#topics-telnetconsole"><span class="std std-ref">telnet console</span></a>
will be enabled (provided its extension is also enabled).</p>
<p>Default: <code class="docutils literal"><span class="pre">[6023,</span> <span class="pre">6073]</span></code></p>
<p>The port range to use for the telnet console. If set to <code class="docutils literal"><span class="pre">None</span></code> or <code class="docutils literal"><span class="pre">0</span></code>, a
dynamically assigned port is used. For more info see
<a class="reference internal" href="telnetconsole.html#topics-telnetconsole"><span class="std std-ref">Telnet Console</span></a>.</p>
<p>Default: <code class="docutils literal"><span class="pre">templates</span></code> dir inside scrapy module</p>
<p>The directory where to look for templates when creating new projects with
<a class="reference internal" href="commands.html#std:command-startproject"><code class="xref std std-command docutils literal"><span class="pre">startproject</span></code></a> command and new spiders with <a class="reference internal" href="commands.html#std:command-genspider"><code class="xref std std-command docutils literal"><span class="pre">genspider</span></code></a>
command.</p>
<p>The project name must not conflict with the name of custom files or directories
in the <code class="docutils literal"><span class="pre">project</span></code> subdirectory.</p>
<p>Default: <code class="docutils literal"><span class="pre">2083</span></code></p>
<p>Scope: <code class="docutils literal"><span class="pre">spidermiddlewares.urllength</span></code></p>
<p>The maximum URL length to allow for crawled URLs. For more information about
the default value for this setting see: <a class="reference external" href="http://www.boutell.com/newfaq/misc/urllength.html">http://www.boutell.com/newfaq/misc/urllength.html</a></p>
<p>Default: <code class="docutils literal"><span class="pre">"Scrapy/VERSION</span> <span class="pre">(+http://scrapy.org)"</span></code></p>
<p>The default User-Agent to use when crawling, unless overridden.</p>
<p>The following settings are documented elsewhere, please check each specific
case to see how to enable and use them.</p>
<p>
        © Copyright 2008-2016, Scrapy developers.
      
        <span class="commit">
          Revision <code>5aebdac4</code>.
        </span>
</p>
<p class="caption"><span class="caption-text">First steps</span></p>
<p class="caption"><span class="caption-text">Basic concepts</span></p>
<p class="caption"><span class="caption-text">Built-in services</span></p>
<p class="caption"><span class="caption-text">Solving specific problems</span></p>
<p class="caption"><span class="caption-text">Extending Scrapy</span></p>
<p class="caption"><span class="caption-text">All the rest</span></p>
<p>Here’s a list of all exceptions included in Scrapy and their usage.</p>
<p>The exception that must be raised by item pipeline stages to stop processing an
Item. For more information see <a class="reference internal" href="item-pipeline.html#topics-item-pipeline"><span class="std std-ref">Item Pipeline</span></a>.</p>
<p>This exception can be raised from a spider callback to request the spider to be
closed/stopped. Supported arguments:</p>
<p>For example:</p>
<p>This exception can be raised by the Scheduler or any downloader middleware to
indicate that the request should be ignored.</p>
<p>This exception can be raised by some components to indicate that they will
remain disabled. Those components include:</p>
<p>The exception must be raised in the component’s <code class="docutils literal"><span class="pre">__init__</span></code> method.</p>
<p>This exception is raised to indicate an unsupported feature.</p>
<p>
        © Copyright 2008-2016, Scrapy developers.
      
        <span class="commit">
          Revision <code>5aebdac4</code>.
        </span>
</p>
<p class="caption"><span class="caption-text">First steps</span></p>
<p class="caption"><span class="caption-text">Basic concepts</span></p>
<p class="caption"><span class="caption-text">Built-in services</span></p>
<p class="caption"><span class="caption-text">Solving specific problems</span></p>
<p class="caption"><span class="caption-text">Extending Scrapy</span></p>
<p class="caption"><span class="caption-text">All the rest</span></p>
<p class="first admonition-title">Note</p>
<p class="last"><code class="xref py py-mod docutils literal"><span class="pre">scrapy.log</span></code> has been deprecated alongside its functions in favor of
explicit calls to the Python standard logging. Keep reading to learn more
about the new logging system.</p>
<p>Scrapy uses <a class="reference external" href="https://docs.python.org/3/library/logging.html">Python’s builtin logging system</a> for event logging. We’ll
provide some simple examples to get you started, but for more advanced
use-cases it’s strongly suggested to read thoroughly its documentation.</p>
<p>Logging works out of the box, and can be configured to some extent with the
Scrapy settings listed in <a class="reference internal" href="#topics-logging-settings"><span class="std std-ref">Logging settings</span></a>.</p>
<p>Scrapy calls <a class="reference internal" href="#scrapy.utils.log.configure_logging" title="scrapy.utils.log.configure_logging"><code class="xref py py-func docutils literal"><span class="pre">scrapy.utils.log.configure_logging()</span></code></a> to set some reasonable
defaults and handle those settings in <a class="reference internal" href="#topics-logging-settings"><span class="std std-ref">Logging settings</span></a> when
running commands, so it’s recommended to manually call it if you’re running
Scrapy from scripts as described in <a class="reference internal" href="practices.html#run-from-script"><span class="std std-ref">Run Scrapy from a script</span></a>.</p>
<p>Python’s builtin logging defines 5 different levels to indicate the severity of a
given log message. Here are the standard ones, listed in decreasing order:</p>
<p>Here’s a quick example of how to log a message using the <code class="docutils literal"><span class="pre">logging.WARNING</span></code>
level:</p>
<p>There are shortcuts for issuing log messages on any of the standard 5 levels,
and there’s also a general <code class="docutils literal"><span class="pre">logging.log</span></code> method which takes a given level as
argument.  If needed, the last example could be rewritten as:</p>
<p>On top of that, you can create different “loggers” to encapsulate messages. (For
example, a common practice is to create different loggers for every module).
These loggers can be configured independently, and they allow hierarchical
constructions.</p>
<p>The previous examples use the root logger behind the scenes, which is a top level
logger where all messages are propagated to (unless otherwise specified). Using
<code class="docutils literal"><span class="pre">logging</span></code> helpers is merely a shortcut for getting the root logger
explicitly, so this is also an equivalent of the last snippets:</p>
<p>You can use a different logger just by getting its name with the
<code class="docutils literal"><span class="pre">logging.getLogger</span></code> function:</p>
<p>Finally, you can ensure having a custom logger for any module you’re working on
by using the <code class="docutils literal"><span class="pre">__name__</span></code> variable, which is populated with current module’s
path:</p>
<p class="first admonition-title">See also</p>
<p>Scrapy provides a <a class="reference internal" href="spiders.html#scrapy.spiders.Spider.logger" title="scrapy.spiders.Spider.logger"><code class="xref py py-data docutils literal"><span class="pre">logger</span></code></a> within each Spider
instance, which can be accessed and used like this:</p>
<p>That logger is created using the Spider’s name, but you can use any custom
Python logger you want. For example:</p>
<p>Loggers on their own don’t manage how messages sent through them are displayed.
For this task, different “handlers” can be attached to any logger instance and
they will redirect those messages to appropriate destinations, such as the
standard output, files, emails, etc.</p>
<p>By default, Scrapy sets and configures a handler for the root logger, based on
the settings below.</p>
<p>These settings can be used to configure the logging:</p>
<p>The first couple of settings define a destination for log messages. If
<a class="reference internal" href="settings.html#std:setting-LOG_FILE"><code class="xref std std-setting docutils literal"><span class="pre">LOG_FILE</span></code></a> is set, messages sent through the root logger will be
redirected to a file named <a class="reference internal" href="settings.html#std:setting-LOG_FILE"><code class="xref std std-setting docutils literal"><span class="pre">LOG_FILE</span></code></a> with encoding
<a class="reference internal" href="settings.html#std:setting-LOG_ENCODING"><code class="xref std std-setting docutils literal"><span class="pre">LOG_ENCODING</span></code></a>. If unset and <a class="reference internal" href="settings.html#std:setting-LOG_ENABLED"><code class="xref std std-setting docutils literal"><span class="pre">LOG_ENABLED</span></code></a> is <code class="docutils literal"><span class="pre">True</span></code>, log
messages will be displayed on the standard error. Lastly, if
<a class="reference internal" href="settings.html#std:setting-LOG_ENABLED"><code class="xref std std-setting docutils literal"><span class="pre">LOG_ENABLED</span></code></a> is <code class="docutils literal"><span class="pre">False</span></code>, there won’t be any visible log output.</p>
<p><a class="reference internal" href="settings.html#std:setting-LOG_LEVEL"><code class="xref std std-setting docutils literal"><span class="pre">LOG_LEVEL</span></code></a> determines the minimum level of severity to display, those
messages with lower severity will be filtered out. It ranges through the
possible levels listed in <a class="reference internal" href="#topics-logging-levels"><span class="std std-ref">Log levels</span></a>.</p>
<p><a class="reference internal" href="settings.html#std:setting-LOG_FORMAT"><code class="xref std std-setting docutils literal"><span class="pre">LOG_FORMAT</span></code></a> and <a class="reference internal" href="settings.html#std:setting-LOG_DATEFORMAT"><code class="xref std std-setting docutils literal"><span class="pre">LOG_DATEFORMAT</span></code></a> specify formatting strings
used as layouts for all messages. Those strings can contain any placeholders
listed in <a class="reference external" href="https://docs.python.org/2/library/logging.html#logrecord-attributes">logging’s logrecord attributes docs</a> and
<a class="reference external" href="https://docs.python.org/2/library/datetime.html#strftime-and-strptime-behavior">datetime’s strftime and strptime directives</a>
respectively.</p>
<p>If <a class="reference internal" href="settings.html#std:setting-LOG_SHORT_NAMES"><code class="xref std std-setting docutils literal"><span class="pre">LOG_SHORT_NAMES</span></code></a> is set, then the logs will not display the scrapy
component that prints the log. It is unset by default, hence logs contain the
scrapy component responsible for that log output.</p>
<p>There are command-line arguments, available for all commands, that you can use
to override some of the Scrapy settings regarding logging.</p>
<p class="first admonition-title">See also</p>
<p>Because Scrapy uses stdlib logging module, you can customize logging using
all features of stdlib logging.</p>
<p>For example, let’s say you’re scraping a website which returns many
HTTP 404 and 500 responses, and you want to hide all messages like this:</p>
<p>The first thing to note is a logger name - it is in brackets:
<code class="docutils literal"><span class="pre">[scrapy.spidermiddlewares.httperror]</span></code>. If you get just <code class="docutils literal"><span class="pre">[scrapy]</span></code> then
<a class="reference internal" href="settings.html#std:setting-LOG_SHORT_NAMES"><code class="xref std std-setting docutils literal"><span class="pre">LOG_SHORT_NAMES</span></code></a> is likely set to True; set it to False and re-run
the crawl.</p>
<p>Next, we can see that the message has INFO level. To hide it
we should set logging level for <code class="docutils literal"><span class="pre">scrapy.spidermiddlewares.httperror</span></code>
higher than INFO; next level after INFO is WARNING. It could be done
e.g. in the spider’s <code class="docutils literal"><span class="pre">__init__</span></code> method:</p>
<p>If you run this spider again then INFO messages from
<code class="docutils literal"><span class="pre">scrapy.spidermiddlewares.httperror</span></code> logger will be gone.</p>
<p>Initialize logging defaults for Scrapy.</p>
<p>This function does:</p>
<p>When <code class="docutils literal"><span class="pre">install_root_handler</span></code> is True (default), this function also
creates a handler for the root logger according to given settings
(see <a class="reference internal" href="#topics-logging-settings"><span class="std std-ref">Logging settings</span></a>). You can override default options
using <code class="docutils literal"><span class="pre">settings</span></code> argument. When <code class="docutils literal"><span class="pre">settings</span></code> is empty or None, defaults
are used.</p>
<p><code class="docutils literal"><span class="pre">configure_logging</span></code> is automatically called when using Scrapy commands,
but needs to be called explicitly when running custom scripts. In that
case, its usage is not required but it’s recommended.</p>
<p>If you plan on configuring the handlers yourself is still recommended you
call this function, passing <cite>install_root_handler=False</cite>. Bear in mind
there won’t be any log output set by default in that case.</p>
<p>To get you started on manually configuring logging’s output, you can use
<a class="reference external" href="https://docs.python.org/2/library/logging.html#logging.basicConfig">logging.basicConfig()</a> to set a basic root handler. This is an example
on how to redirect <code class="docutils literal"><span class="pre">INFO</span></code> or higher messages to a file:</p>
<p>Refer to <a class="reference internal" href="practices.html#run-from-script"><span class="std std-ref">Run Scrapy from a script</span></a> for more details about using Scrapy this
way.</p>
<p>
        © Copyright 2008-2016, Scrapy developers.
      
        <span class="commit">
          Revision <code>5aebdac4</code>.
        </span>
</p>
<p class="caption"><span class="caption-text">First steps</span></p>
<p class="caption"><span class="caption-text">Basic concepts</span></p>
<p class="caption"><span class="caption-text">Built-in services</span></p>
<p class="caption"><span class="caption-text">Solving specific problems</span></p>
<p class="caption"><span class="caption-text">Extending Scrapy</span></p>
<p class="caption"><span class="caption-text">All the rest</span></p>
<p>Scrapy provides a convenient facility for collecting stats in the form of
key/values, where values are often counters. The facility is called the Stats
Collector, and can be accessed through the <a class="reference internal" href="api.html#scrapy.crawler.Crawler.stats" title="scrapy.crawler.Crawler.stats"><code class="xref py py-attr docutils literal"><span class="pre">stats</span></code></a>
attribute of the <a class="reference internal" href="api.html#topics-api-crawler"><span class="std std-ref">Crawler API</span></a>, as illustrated by the examples in
the <a class="reference internal" href="#topics-stats-usecases"><span class="std std-ref">Common Stats Collector uses</span></a> section below.</p>
<p>However, the Stats Collector is always available, so you can always import it
in your module and use its API (to increment or set new stat keys), regardless
of whether the stats collection is enabled or not. If it’s disabled, the API
will still work but it won’t collect anything. This is aimed at simplifying the
stats collector usage: you should spend no more than one line of code for
collecting stats in your spider, Scrapy extension, or whatever code you’re
using the Stats Collector from.</p>
<p>Another feature of the Stats Collector is that it’s very efficient (when
enabled) and extremely efficient (almost unnoticeable) when disabled.</p>
<p>The Stats Collector keeps a stats table per open spider which is automatically
opened when the spider is opened, and closed when the spider is closed.</p>
<p>Access the stats collector through the <a class="reference internal" href="api.html#scrapy.crawler.Crawler.stats" title="scrapy.crawler.Crawler.stats"><code class="xref py py-attr docutils literal"><span class="pre">stats</span></code></a>
attribute. Here is an example of an extension that access stats:</p>
<p>Set stat value:</p>
<p>Increment stat value:</p>
<p>Set stat value only if greater than previous:</p>
<p>Set stat value only if lower than previous:</p>
<p>Get stat value:</p>
<p>Get all stats:</p>
<p>Besides the basic <code class="xref py py-class docutils literal"><span class="pre">StatsCollector</span></code> there are other Stats Collectors
available in Scrapy which extend the basic Stats Collector. You can select
which Stats Collector to use through the <a class="reference internal" href="settings.html#std:setting-STATS_CLASS"><code class="xref std std-setting docutils literal"><span class="pre">STATS_CLASS</span></code></a> setting. The
default Stats Collector used is the <code class="xref py py-class docutils literal"><span class="pre">MemoryStatsCollector</span></code>.</p>
<p>A simple stats collector that keeps the stats of the last scraping run (for
each spider) in memory, after they’re closed. The stats can be accessed
through the <a class="reference internal" href="#scrapy.statscollectors.MemoryStatsCollector.spider_stats" title="scrapy.statscollectors.MemoryStatsCollector.spider_stats"><code class="xref py py-attr docutils literal"><span class="pre">spider_stats</span></code></a> attribute, which is a dict keyed by spider
domain name.</p>
<p>This is the default Stats Collector used in Scrapy.</p>
<p>A dict of dicts (keyed by spider name) containing the stats of the last
scraping run for each spider.</p>
<p>A Stats collector which does nothing but is very efficient (because it does
nothing). This stats collector can be set via the <a class="reference internal" href="settings.html#std:setting-STATS_CLASS"><code class="xref std std-setting docutils literal"><span class="pre">STATS_CLASS</span></code></a>
setting, to disable stats collect in order to improve performance. However,
the performance penalty of stats collection is usually marginal compared to
other Scrapy workload like parsing pages.</p>
<p>
        © Copyright 2008-2016, Scrapy developers.
      
        <span class="commit">
          Revision <code>5aebdac4</code>.
        </span>
</p>
<p class="caption"><span class="caption-text">First steps</span></p>
<p class="caption"><span class="caption-text">Basic concepts</span></p>
<p class="caption"><span class="caption-text">Built-in services</span></p>
<p class="caption"><span class="caption-text">Solving specific problems</span></p>
<p class="caption"><span class="caption-text">Extending Scrapy</span></p>
<p class="caption"><span class="caption-text">All the rest</span></p>
<p>Although Python makes sending e-mails relatively easy via the <a class="reference external" href="https://docs.python.org/2/library/smtplib.html">smtplib</a>
library, Scrapy provides its own facility for sending e-mails which is very
easy to use and it’s implemented using <a class="reference external" href="https://twistedmatrix.com/documents/current/core/howto/defer-intro.html">Twisted non-blocking IO</a>, to avoid
interfering with the non-blocking IO of the crawler. It also provides a
simple API for sending attachments and it’s very easy to configure, with a few
<a class="reference internal" href="#topics-email-settings"><span class="std std-ref">settings</span></a>.</p>
<p>There are two ways to instantiate the mail sender. You can instantiate it using
the standard constructor:</p>
<p>Or you can instantiate it passing a Scrapy settings object, which will respect
the <a class="reference internal" href="#topics-email-settings"><span class="std std-ref">settings</span></a>:</p>
<p>And here is how to use it to send an e-mail (without attachments):</p>
<p>MailSender is the preferred class to use for sending emails from Scrapy, as it
uses <a class="reference external" href="https://twistedmatrix.com/documents/current/core/howto/defer-intro.html">Twisted non-blocking IO</a>, like the rest of the framework.</p>
<p>Instantiate using a Scrapy settings object, which will respect
<a class="reference internal" href="#topics-email-settings"><span class="std std-ref">these Scrapy settings</span></a>.</p>
<p>Send email to the given recipients.</p>
<p>These settings define the default constructor values of the <a class="reference internal" href="#scrapy.mail.MailSender" title="scrapy.mail.MailSender"><code class="xref py py-class docutils literal"><span class="pre">MailSender</span></code></a>
class, and can be used to configure e-mail notifications in your project without
writing any code (for those extensions and code that uses <a class="reference internal" href="#scrapy.mail.MailSender" title="scrapy.mail.MailSender"><code class="xref py py-class docutils literal"><span class="pre">MailSender</span></code></a>).</p>
<p>Default: <code class="docutils literal"><span class="pre">'scrapy@localhost'</span></code></p>
<p>Sender email to use (<code class="docutils literal"><span class="pre">From:</span></code> header) for sending emails.</p>
<p>Default: <code class="docutils literal"><span class="pre">'localhost'</span></code></p>
<p>SMTP host to use for sending emails.</p>
<p>Default: <code class="docutils literal"><span class="pre">25</span></code></p>
<p>SMTP port to use for sending emails.</p>
<p>Default: <code class="docutils literal"><span class="pre">None</span></code></p>
<p>User to use for SMTP authentication. If disabled no SMTP authentication will be
performed.</p>
<p>Default: <code class="docutils literal"><span class="pre">None</span></code></p>
<p>Password to use for SMTP authentication, along with <a class="reference internal" href="#std:setting-MAIL_USER"><code class="xref std std-setting docutils literal"><span class="pre">MAIL_USER</span></code></a>.</p>
<p>Default: <code class="docutils literal"><span class="pre">False</span></code></p>
<p>Enforce using STARTTLS. STARTTLS is a way to take an existing insecure connection, and upgrade it to a secure connection using SSL/TLS.</p>
<p>Default: <code class="docutils literal"><span class="pre">False</span></code></p>
<p>Enforce connecting using an SSL encrypted connection</p>
<p>
        © Copyright 2008-2016, Scrapy developers.
      
        <span class="commit">
          Revision <code>5aebdac4</code>.
        </span>
</p>
<p class="caption"><span class="caption-text">First steps</span></p>
<p class="caption"><span class="caption-text">Basic concepts</span></p>
<p class="caption"><span class="caption-text">Built-in services</span></p>
<p class="caption"><span class="caption-text">Solving specific problems</span></p>
<p class="caption"><span class="caption-text">Extending Scrapy</span></p>
<p class="caption"><span class="caption-text">All the rest</span></p>
<p>Scrapy comes with a built-in telnet console for inspecting and controlling a
Scrapy running process. The telnet console is just a regular python shell
running inside the Scrapy process, so you can do literally anything from it.</p>
<p>The telnet console is a <a class="reference internal" href="extensions.html#topics-extensions-ref"><span class="std std-ref">built-in Scrapy extension</span></a> which comes enabled by default, but you can also
disable it if you want. For more information about the extension itself see
<a class="reference internal" href="extensions.html#topics-extensions-ref-telnetconsole"><span class="std std-ref">Telnet console extension</span></a>.</p>
<p>The telnet console listens in the TCP port defined in the
<a class="reference internal" href="#std:setting-TELNETCONSOLE_PORT"><code class="xref std std-setting docutils literal"><span class="pre">TELNETCONSOLE_PORT</span></code></a> setting, which defaults to <code class="docutils literal"><span class="pre">6023</span></code>. To access
the console you need to type:</p>
<p>You need the telnet program which comes installed by default in Windows, and
most Linux distros.</p>
<p>The telnet console is like a regular Python shell running inside the Scrapy
process, so you can do anything from it including importing new modules, etc.</p>
<p>However, the telnet console comes with some default variables defined for
convenience:</p>
<p>Here are some example tasks you can do with the telnet console:</p>
<p>You can use the <code class="docutils literal"><span class="pre">est()</span></code> method of the Scrapy engine to quickly show its state
using the telnet console:</p>
<p>To pause:</p>
<p>To resume:</p>
<p>To stop:</p>
<p>Sent just before the telnet console is opened. You can hook up to this
signal to add, remove or update the variables that will be available in the
telnet local namespace. In order to do that, you need to update the
<code class="docutils literal"><span class="pre">telnet_vars</span></code> dict in your handler.</p>
<p>These are the settings that control the telnet console’s behaviour:</p>
<p>Default: <code class="docutils literal"><span class="pre">[6023,</span> <span class="pre">6073]</span></code></p>
<p>The port range to use for the telnet console. If set to <code class="docutils literal"><span class="pre">None</span></code> or <code class="docutils literal"><span class="pre">0</span></code>, a
dynamically assigned port is used.</p>
<p>Default: <code class="docutils literal"><span class="pre">'127.0.0.1'</span></code></p>
<p>The interface the telnet console should listen on</p>
<p>
        © Copyright 2008-2016, Scrapy developers.
      
        <span class="commit">
          Revision <code>5aebdac4</code>.
        </span>
</p>
<p class="caption"><span class="caption-text">First steps</span></p>
<p class="caption"><span class="caption-text">Basic concepts</span></p>
<p class="caption"><span class="caption-text">Built-in services</span></p>
<p class="caption"><span class="caption-text">Solving specific problems</span></p>
<p class="caption"><span class="caption-text">Extending Scrapy</span></p>
<p class="caption"><span class="caption-text">All the rest</span></p>
<p>webservice has been moved into a separate project.</p>
<p>It is hosted at:</p>
<p>
        © Copyright 2008-2016, Scrapy developers.
      
        <span class="commit">
          Revision <code>5aebdac4</code>.
        </span>
</p>
<p class="caption"><span class="caption-text">First steps</span></p>
<p class="caption"><span class="caption-text">Basic concepts</span></p>
<p class="caption"><span class="caption-text">Built-in services</span></p>
<p class="caption"><span class="caption-text">Solving specific problems</span></p>
<p class="caption"><span class="caption-text">Extending Scrapy</span></p>
<p class="caption"><span class="caption-text">All the rest</span></p>
<p><a class="reference external" href="http://www.crummy.com/software/BeautifulSoup/">BeautifulSoup</a> and <a class="reference external" href="http://lxml.de/">lxml</a> are libraries for parsing HTML and XML. Scrapy is
an application framework for writing web spiders that crawl web sites and
extract data from them.</p>
<p>Scrapy provides a built-in mechanism for extracting data (called
<a class="reference internal" href="topics/selectors.html#topics-selectors"><span class="std std-ref">selectors</span></a>) but you can easily use <a class="reference external" href="http://www.crummy.com/software/BeautifulSoup/">BeautifulSoup</a>
(or <a class="reference external" href="http://lxml.de/">lxml</a>) instead, if you feel more comfortable working with them. After
all, they’re just parsing libraries which can be imported and used from any
Python code.</p>
<p>In other words, comparing <a class="reference external" href="http://www.crummy.com/software/BeautifulSoup/">BeautifulSoup</a> (or <a class="reference external" href="http://lxml.de/">lxml</a>) to Scrapy is like
comparing <a class="reference external" href="http://jinja.pocoo.org/">jinja2</a> to <a class="reference external" href="https://www.djangoproject.com/">Django</a>.</p>
<p>Yes, you can.
As mentioned <a class="reference internal" href="#faq-scrapy-bs-cmp"><span class="std std-ref">above</span></a>, <a class="reference external" href="http://www.crummy.com/software/BeautifulSoup/">BeautifulSoup</a> can be used
for parsing HTML responses in Scrapy callbacks.
You just have to feed the response’s body into a <code class="docutils literal"><span class="pre">BeautifulSoup</span></code> object
and extract whatever data you need from it.</p>
<p>Here’s an example spider using BeautifulSoup API, with <code class="docutils literal"><span class="pre">lxml</span></code> as the HTML parser:</p>
<p class="first admonition-title">Note</p>
<p class="last"><code class="docutils literal"><span class="pre">BeautifulSoup</span></code> supports several HTML/XML parsers.
See <a class="reference external" href="https://www.crummy.com/software/BeautifulSoup/bs4/doc/#specifying-the-parser-to-use">BeautifulSoup’s official documentation</a> on which ones are available.</p>
<p>Scrapy is supported under Python 2.7 and Python 3.3+.
Python 2.6 support was dropped starting at Scrapy 0.20.
Python 3 support was added in Scrapy 1.1.</p>
<p class="first admonition-title">Note</p>
<p class="last">For Python 3 support on Windows, it is recommended to use
Anaconda/Miniconda as <a class="reference internal" href="intro/install.html#intro-install-windows"><span class="std std-ref">outlined in the installation guide</span></a>.</p>
<p>Probably, but we don’t like that word. We think <a class="reference external" href="https://www.djangoproject.com/">Django</a> is a great open source
project and an example to follow, so we’ve used it as an inspiration for
Scrapy.</p>
<p>We believe that, if something is already done well, there’s no need to reinvent
it. This concept, besides being one of the foundations for open source and free
software, not only applies to software but also to documentation, procedures,
policies, etc. So, instead of going through each problem ourselves, we choose
to copy ideas from those projects that have already solved them properly, and
focus on the real problems we need to solve.</p>
<p>We’d be proud if Scrapy serves as an inspiration for other projects. Feel free
to steal from us!</p>
<p>Yes. Support for HTTP proxies is provided (since Scrapy 0.8) through the HTTP
Proxy downloader middleware. See
<a class="reference internal" href="topics/downloader-middleware.html#scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware" title="scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware"><code class="xref py py-class docutils literal"><span class="pre">HttpProxyMiddleware</span></code></a>.</p>
<p>See <a class="reference internal" href="topics/request-response.html#topics-request-response-ref-request-callback-arguments"><span class="std std-ref">Passing additional data to callback functions</span></a>.</p>
<p>You need to install <a class="reference external" href="https://sourceforge.net/projects/pywin32/">pywin32</a> because of <a class="reference external" href="https://twistedmatrix.com/trac/ticket/3707">this Twisted bug</a>.</p>
<p>See <a class="reference internal" href="topics/request-response.html#topics-request-response-ref-request-userlogin"><span class="std std-ref">Using FormRequest.from_response() to simulate a user login</span></a>.</p>
<p>By default, Scrapy uses a <a class="reference external" href="https://en.wikipedia.org/wiki/Stack_(abstract_data_type)">LIFO</a> queue for storing pending requests, which
basically means that it crawls in <a class="reference external" href="https://en.wikipedia.org/wiki/Depth-first_search">DFO order</a>. This order is more convenient
in most cases. If you do want to crawl in true <a class="reference external" href="https://en.wikipedia.org/wiki/Breadth-first_search">BFO order</a>, you can do it by
setting the following settings:</p>
<p>See <a class="reference internal" href="topics/leaks.html#topics-leaks"><span class="std std-ref">Debugging memory leaks</span></a>.</p>
<p>Also, Python has a builtin memory leak issue which is described in
<a class="reference internal" href="topics/leaks.html#topics-leaks-without-leaks"><span class="std std-ref">Leaks without leaks</span></a>.</p>
<p>See previous question.</p>
<p>Yes, see <a class="reference internal" href="topics/downloader-middleware.html#scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware" title="scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware"><code class="xref py py-class docutils literal"><span class="pre">HttpAuthMiddleware</span></code></a>.</p>
<p>Try changing the default <a class="reference external" href="https://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.4">Accept-Language</a> request header by overriding the
<a class="reference internal" href="topics/settings.html#std:setting-DEFAULT_REQUEST_HEADERS"><code class="xref std std-setting docutils literal"><span class="pre">DEFAULT_REQUEST_HEADERS</span></code></a> setting.</p>
<p>See <a class="reference internal" href="intro/examples.html#intro-examples"><span class="std std-ref">Examples</span></a>.</p>
<p>Yes. You can use the <a class="reference internal" href="topics/commands.html#std:command-runspider"><code class="xref std std-command docutils literal"><span class="pre">runspider</span></code></a> command. For example, if you have a
spider written in a <code class="docutils literal"><span class="pre">my_spider.py</span></code> file you can run it with:</p>
<p>See <a class="reference internal" href="topics/commands.html#std:command-runspider"><code class="xref std std-command docutils literal"><span class="pre">runspider</span></code></a> command for more info.</p>
<p>Those messages (logged with <code class="docutils literal"><span class="pre">DEBUG</span></code> level) don’t necessarily mean there is a
problem, so you may not need to fix them.</p>
<p>Those messages are thrown by the Offsite Spider Middleware, which is a spider
middleware (enabled by default) whose purpose is to filter out requests to
domains outside the ones covered by the spider.</p>
<p>For more info see:
<a class="reference internal" href="topics/spider-middleware.html#scrapy.spidermiddlewares.offsite.OffsiteMiddleware" title="scrapy.spidermiddlewares.offsite.OffsiteMiddleware"><code class="xref py py-class docutils literal"><span class="pre">OffsiteMiddleware</span></code></a>.</p>
<p>See <a class="reference internal" href="topics/deploy.html#topics-deploy"><span class="std std-ref">Deploying Spiders</span></a>.</p>
<p>It’ll depend on how large your output is. See <a class="reference internal" href="topics/exporters.html#json-with-large-data"><span class="std std-ref">this warning</span></a> in <a class="reference internal" href="topics/exporters.html#scrapy.exporters.JsonItemExporter" title="scrapy.exporters.JsonItemExporter"><code class="xref py py-class docutils literal"><span class="pre">JsonItemExporter</span></code></a>
documentation.</p>
<p>Some signals support returning deferreds from their handlers, others don’t. See
the <a class="reference internal" href="topics/signals.html#topics-signals-ref"><span class="std std-ref">Built-in signals reference</span></a> to know which ones.</p>
<p>999 is a custom response status code used by Yahoo sites to throttle requests.
Try slowing down the crawling speed by using a download delay of <code class="docutils literal"><span class="pre">2</span></code> (or
higher) in your spider:</p>
<p>Or by setting a global download delay in your project with the
<a class="reference internal" href="topics/settings.html#std:setting-DOWNLOAD_DELAY"><code class="xref std std-setting docutils literal"><span class="pre">DOWNLOAD_DELAY</span></code></a> setting.</p>
<p>Yes, but you can also use the Scrapy shell which allows you to quickly analyze
(and even modify) the response being processed by your spider, which is, quite
often, more useful than plain old <code class="docutils literal"><span class="pre">pdb.set_trace()</span></code>.</p>
<p>For more info see <a class="reference internal" href="topics/shell.html#topics-shell-inspect-response"><span class="std std-ref">Invoking the shell from spiders to inspect responses</span></a>.</p>
<p>To dump into a JSON file:</p>
<p>To dump into a CSV file:</p>
<p>To dump into a XML file:</p>
<p>For more information see <a class="reference internal" href="topics/feed-exports.html#topics-feed-exports"><span class="std std-ref">Feed exports</span></a></p>
<p>The <code class="docutils literal"><span class="pre">__VIEWSTATE</span></code> parameter is used in sites built with ASP.NET/VB.NET. For
more info on how it works see <a class="reference external" href="http://search.cpan.org/~ecarroll/HTML-TreeBuilderX-ASP_NET-0.09/lib/HTML/TreeBuilderX/ASP_NET.pm">this page</a>. Also, here’s an <a class="reference external" href="https://github.com/AmbientLighter/rpn-fas/blob/master/fas/spiders/rnp.py">example spider</a>
which scrapes one of these sites.</p>
<p>Parsing big feeds with XPath selectors can be problematic since they need to
build the DOM of the entire feed in memory, and this can be quite slow and
consume a lot of memory.</p>
<p>In order to avoid parsing all the entire feed at once in memory, you can use
the functions <code class="docutils literal"><span class="pre">xmliter</span></code> and <code class="docutils literal"><span class="pre">csviter</span></code> from <code class="docutils literal"><span class="pre">scrapy.utils.iterators</span></code>
module. In fact, this is what the feed spiders (see <a class="reference internal" href="topics/spiders.html#topics-spiders"><span class="std std-ref">Spiders</span></a>) use
under the cover.</p>
<p>Yes, Scrapy receives and keeps track of cookies sent by servers, and sends them
back on subsequent requests, like any regular web browser does.</p>
<p>For more info see <a class="reference internal" href="topics/request-response.html#topics-request-response"><span class="std std-ref">Requests and Responses</span></a> and <a class="reference internal" href="topics/downloader-middleware.html#cookies-mw"><span class="std std-ref">CookiesMiddleware</span></a>.</p>
<p>Enable the <a class="reference internal" href="topics/downloader-middleware.html#std:setting-COOKIES_DEBUG"><code class="xref std std-setting docutils literal"><span class="pre">COOKIES_DEBUG</span></code></a> setting.</p>
<p>Raise the <a class="reference internal" href="topics/exceptions.html#scrapy.exceptions.CloseSpider" title="scrapy.exceptions.CloseSpider"><code class="xref py py-exc docutils literal"><span class="pre">CloseSpider</span></code></a> exception from a callback. For
more info see: <a class="reference internal" href="topics/exceptions.html#scrapy.exceptions.CloseSpider" title="scrapy.exceptions.CloseSpider"><code class="xref py py-exc docutils literal"><span class="pre">CloseSpider</span></code></a>.</p>
<p>See <a class="reference internal" href="topics/practices.html#bans"><span class="std std-ref">Avoiding getting banned</span></a>.</p>
<p>Both <a class="reference internal" href="topics/spiders.html#spiderargs"><span class="std std-ref">spider arguments</span></a> and <a class="reference internal" href="topics/settings.html#topics-settings"><span class="std std-ref">settings</span></a>
can be used to configure your spider. There is no strict rule that mandates to
use one or the other, but settings are more suited for parameters that, once
set, don’t change much, while spider arguments are meant to change more often,
even on each spider run and sometimes are required for the spider to run at all
(for example, to set the start url of a spider).</p>
<p>To illustrate with an example, assuming you have a spider that needs to log
into a site to scrape data, and you only want to scrape data from a certain
section of the site (which varies each time). In that case, the credentials to
log in would be settings, while the url of the section to scrape would be a
spider argument.</p>
<p>You may need to remove namespaces. See <a class="reference internal" href="topics/selectors.html#removing-namespaces"><span class="std std-ref">Removing namespaces</span></a>.</p>
<p>
        © Copyright 2008-2016, Scrapy developers.
      
        <span class="commit">
          Revision <code>5aebdac4</code>.
        </span>
</p>
<p class="caption"><span class="caption-text">First steps</span></p>
<p class="caption"><span class="caption-text">Basic concepts</span></p>
<p class="caption"><span class="caption-text">Built-in services</span></p>
<p class="caption"><span class="caption-text">Solving specific problems</span></p>
<p class="caption"><span class="caption-text">Extending Scrapy</span></p>
<p class="caption"><span class="caption-text">All the rest</span></p>
<p>This document explains the most common techniques for debugging spiders.
Consider the following scrapy spider below:</p>
<p>Basically this is a simple spider which parses two pages of items (the
start_urls). Items also have a details page with additional information, so we
use the <code class="docutils literal"><span class="pre">meta</span></code> functionality of <a class="reference internal" href="request-response.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal"><span class="pre">Request</span></code></a> to pass a
partially populated item.</p>
<p>The most basic way of checking the output of your spider is to use the
<a class="reference internal" href="commands.html#std:command-parse"><code class="xref std std-command docutils literal"><span class="pre">parse</span></code></a> command. It allows to check the behaviour of different parts
of the spider at the method level. It has the advantage of being flexible and
simple to use, but does not allow debugging code inside a method.</p>
<p>In order to see the item scraped from a specific url:</p>
<p>Using the <code class="docutils literal"><span class="pre">--verbose</span></code> or <code class="docutils literal"><span class="pre">-v</span></code> option we can see the status at each depth level:</p>
<p>Checking items scraped from a single start_url, can also be easily achieved
using:</p>
<p>While the <a class="reference internal" href="commands.html#std:command-parse"><code class="xref std std-command docutils literal"><span class="pre">parse</span></code></a> command is very useful for checking behaviour of a
spider, it is of little help to check what happens inside a callback, besides
showing the response received and the output. How to debug the situation when
<code class="docutils literal"><span class="pre">parse_details</span></code> sometimes receives no item?</p>
<p>Fortunately, the <a class="reference internal" href="commands.html#std:command-shell"><code class="xref std std-command docutils literal"><span class="pre">shell</span></code></a> is your bread and butter in this case (see
<a class="reference internal" href="shell.html#topics-shell-inspect-response"><span class="std std-ref">Invoking the shell from spiders to inspect responses</span></a>):</p>
<p>See also: <a class="reference internal" href="shell.html#topics-shell-inspect-response"><span class="std std-ref">Invoking the shell from spiders to inspect responses</span></a>.</p>
<p>Sometimes you just want to see how a certain response looks in a browser, you
can use the <code class="docutils literal"><span class="pre">open_in_browser</span></code> function for that. Here is an example of how
you would use it:</p>
<p><code class="docutils literal"><span class="pre">open_in_browser</span></code> will open a browser with the response received by Scrapy at
that point, adjusting the <a class="reference external" href="http://www.w3schools.com/tags/tag_base.asp">base tag</a> so that images and styles are displayed
properly.</p>
<p>Logging is another useful option for getting information about your spider run.
Although not as convenient, it comes with the advantage that the logs will be
available in all future runs should they be necessary again:</p>
<p>For more information, check the <a class="reference internal" href="logging.html#topics-logging"><span class="std std-ref">Logging</span></a> section.</p>
<p>
        © Copyright 2008-2016, Scrapy developers.
      
        <span class="commit">
          Revision <code>5aebdac4</code>.
        </span>
</p>
<p class="caption"><span class="caption-text">First steps</span></p>
<p class="caption"><span class="caption-text">Basic concepts</span></p>
<p class="caption"><span class="caption-text">Built-in services</span></p>
<p class="caption"><span class="caption-text">Solving specific problems</span></p>
<p class="caption"><span class="caption-text">Extending Scrapy</span></p>
<p class="caption"><span class="caption-text">All the rest</span></p>
<p><span class="versionmodified">New in version 0.15.</span></p>
<p class="first admonition-title">Note</p>
<p class="last">This is a new feature (introduced in Scrapy 0.15) and may be subject
to minor functionality/API updates. Check the <a class="reference internal" href="../news.html#news"><span class="std std-ref">release notes</span></a> to
be notified of updates.</p>
<p>Testing spiders can get particularly annoying and while nothing prevents you
from writing unit tests the task gets cumbersome quickly. Scrapy offers an
integrated way of testing your spiders by the means of contracts.</p>
<p>This allows you to test each callback of your spider by hardcoding a sample url
and check various constraints for how the callback processes the response. Each
contract is prefixed with an <code class="docutils literal"><span class="pre">@</span></code> and included in the docstring. See the
following example:</p>
<p>This callback is tested using three built-in contracts:</p>
<p>This contract (<code class="docutils literal"><span class="pre">@url</span></code>) sets the sample url used when checking other
contract conditions for this spider. This contract is mandatory. All
callbacks lacking this contract are ignored when running the checks:</p>
<p>This contract (<code class="docutils literal"><span class="pre">@returns</span></code>) sets lower and upper bounds for the items and
requests returned by the spider. The upper bound is optional:</p>
<p>This contract (<code class="docutils literal"><span class="pre">@scrapes</span></code>) checks that all the items returned by the
callback have the specified fields:</p>
<p>Use the <a class="reference internal" href="commands.html#std:command-check"><code class="xref std std-command docutils literal"><span class="pre">check</span></code></a> command to run the contract checks.</p>
<p>If you find you need more power than the built-in scrapy contracts you can
create and load your own contracts in the project by using the
<a class="reference internal" href="settings.html#std:setting-SPIDER_CONTRACTS"><code class="xref std std-setting docutils literal"><span class="pre">SPIDER_CONTRACTS</span></code></a> setting:</p>
<p>Each contract must inherit from <a class="reference internal" href="#scrapy.contracts.Contract" title="scrapy.contracts.Contract"><code class="xref py py-class docutils literal"><span class="pre">scrapy.contracts.Contract</span></code></a> and can
override three methods:</p>
<p>This receives a <code class="docutils literal"><span class="pre">dict</span></code> as an argument containing default arguments
for <a class="reference internal" href="request-response.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal"><span class="pre">Request</span></code></a> object. Must return the same or a
modified version of it.</p>
<p>This allows hooking in various checks on the response received from the
sample request, before it’s being passed to the callback.</p>
<p>This allows processing the output of the callback. Iterators are
converted listified before being passed to this hook.</p>
<p>Here is a demo contract which checks the presence of a custom header in the
response received. Raise <code class="xref py py-class docutils literal"><span class="pre">scrapy.exceptions.ContractFail</span></code> in order to
get the failures pretty printed:</p>
<p>
        © Copyright 2008-2016, Scrapy developers.
      
        <span class="commit">
          Revision <code>5aebdac4</code>.
        </span>
</p>
<p class="caption"><span class="caption-text">First steps</span></p>
<p class="caption"><span class="caption-text">Basic concepts</span></p>
<p class="caption"><span class="caption-text">Built-in services</span></p>
<p class="caption"><span class="caption-text">Solving specific problems</span></p>
<p class="caption"><span class="caption-text">Extending Scrapy</span></p>
<p class="caption"><span class="caption-text">All the rest</span></p>
<p>This section documents common practices when using Scrapy. These are things
that cover many topics and don’t often fall into any other specific section.</p>
<p>You can use the <a class="reference internal" href="api.html#topics-api"><span class="std std-ref">API</span></a> to run Scrapy from a script, instead of
the typical way of running Scrapy via <code class="docutils literal"><span class="pre">scrapy</span> <span class="pre">crawl</span></code>.</p>
<p>Remember that Scrapy is built on top of the Twisted
asynchronous networking library, so you need to run it inside the Twisted reactor.</p>
<p>The first utility you can use to run your spiders is
<a class="reference internal" href="api.html#scrapy.crawler.CrawlerProcess" title="scrapy.crawler.CrawlerProcess"><code class="xref py py-class docutils literal"><span class="pre">scrapy.crawler.CrawlerProcess</span></code></a>. This class will start a Twisted reactor
for you, configuring the logging and setting shutdown handlers. This class is
the one used by all Scrapy commands.</p>
<p>Here’s an example showing how to run a single spider with it.</p>
<p>Make sure to check <a class="reference internal" href="api.html#scrapy.crawler.CrawlerProcess" title="scrapy.crawler.CrawlerProcess"><code class="xref py py-class docutils literal"><span class="pre">CrawlerProcess</span></code></a> documentation to get
acquainted with its usage details.</p>
<p>If you are inside a Scrapy project there are some additional helpers you can
use to import those components within the project. You can automatically import
your spiders passing their name to <a class="reference internal" href="api.html#scrapy.crawler.CrawlerProcess" title="scrapy.crawler.CrawlerProcess"><code class="xref py py-class docutils literal"><span class="pre">CrawlerProcess</span></code></a>, and
use <code class="docutils literal"><span class="pre">get_project_settings</span></code> to get a <a class="reference internal" href="api.html#scrapy.settings.Settings" title="scrapy.settings.Settings"><code class="xref py py-class docutils literal"><span class="pre">Settings</span></code></a>
instance with your project settings.</p>
<p>What follows is a working example of how to do that, using the <a class="reference external" href="https://github.com/scrapinghub/testspiders">testspiders</a>
project as example.</p>
<p>There’s another Scrapy utility that provides more control over the crawling
process: <a class="reference internal" href="api.html#scrapy.crawler.CrawlerRunner" title="scrapy.crawler.CrawlerRunner"><code class="xref py py-class docutils literal"><span class="pre">scrapy.crawler.CrawlerRunner</span></code></a>. This class is a thin wrapper
that encapsulates some simple helpers to run multiple crawlers, but it won’t
start or interfere with existing reactors in any way.</p>
<p>Using this class the reactor should be explicitly run after scheduling your
spiders. It’s recommended you use <a class="reference internal" href="api.html#scrapy.crawler.CrawlerRunner" title="scrapy.crawler.CrawlerRunner"><code class="xref py py-class docutils literal"><span class="pre">CrawlerRunner</span></code></a>
instead of <a class="reference internal" href="api.html#scrapy.crawler.CrawlerProcess" title="scrapy.crawler.CrawlerProcess"><code class="xref py py-class docutils literal"><span class="pre">CrawlerProcess</span></code></a> if your application is
already using Twisted and you want to run Scrapy in the same reactor.</p>
<p>Note that you will also have to shutdown the Twisted reactor yourself after the
spider is finished. This can be achieved by adding callbacks to the deferred
returned by the <a class="reference internal" href="api.html#scrapy.crawler.CrawlerRunner.crawl" title="scrapy.crawler.CrawlerRunner.crawl"><code class="xref py py-meth docutils literal"><span class="pre">CrawlerRunner.crawl</span></code></a> method.</p>
<p>Here’s an example of its usage, along with a callback to manually stop the
reactor after <cite>MySpider</cite> has finished running.</p>
<p class="first admonition-title">See also</p>
<p class="last"><a class="reference external" href="https://twistedmatrix.com/documents/current/core/howto/reactor-basics.html">Twisted Reactor Overview</a>.</p>
<p>By default, Scrapy runs a single spider per process when you run <code class="docutils literal"><span class="pre">scrapy</span>
<span class="pre">crawl</span></code>. However, Scrapy supports running multiple spiders per process using
the <a class="reference internal" href="api.html#topics-api"><span class="std std-ref">internal API</span></a>.</p>
<p>Here is an example that runs multiple spiders simultaneously:</p>
<p>Same example using <a class="reference internal" href="api.html#scrapy.crawler.CrawlerRunner" title="scrapy.crawler.CrawlerRunner"><code class="xref py py-class docutils literal"><span class="pre">CrawlerRunner</span></code></a>:</p>
<p>Same example but running the spiders sequentially by chaining the deferreds:</p>
<p class="first admonition-title">See also</p>
<p class="last"><a class="reference internal" href="#run-from-script"><span class="std std-ref">Run Scrapy from a script</span></a>.</p>
<p>Scrapy doesn’t provide any built-in facility for running crawls in a distribute
(multi-server) manner. However, there are some ways to distribute crawls, which
vary depending on how you plan to distribute them.</p>
<p>If you have many spiders, the obvious way to distribute the load is to setup
many Scrapyd instances and distribute spider runs among those.</p>
<p>If you instead want to run a single (big) spider through many machines, what
you usually do is partition the urls to crawl and send them to each separate
spider. Here is a concrete example:</p>
<p>First, you prepare the list of urls to crawl and put them into separate
files/urls:</p>
<p>Then you fire a spider run on 3 different Scrapyd servers. The spider would
receive a (spider) argument <code class="docutils literal"><span class="pre">part</span></code> with the number of the partition to
crawl:</p>
<p>Some websites implement certain measures to prevent bots from crawling them,
with varying degrees of sophistication. Getting around those measures can be
difficult and tricky, and may sometimes require special infrastructure. Please
consider contacting <a class="reference external" href="http://scrapy.org/support/">commercial support</a> if in doubt.</p>
<p>Here are some tips to keep in mind when dealing with these kinds of sites:</p>
<p>If you are still unable to prevent your bot getting banned, consider contacting
<a class="reference external" href="http://scrapy.org/support/">commercial support</a>.</p>
<p>
        © Copyright 2008-2016, Scrapy developers.
      
        <span class="commit">
          Revision <code>5aebdac4</code>.
        </span>
</p>
<p class="caption"><span class="caption-text">First steps</span></p>
<p class="caption"><span class="caption-text">Basic concepts</span></p>
<p class="caption"><span class="caption-text">Built-in services</span></p>
<p class="caption"><span class="caption-text">Solving specific problems</span></p>
<p class="caption"><span class="caption-text">Extending Scrapy</span></p>
<p class="caption"><span class="caption-text">All the rest</span></p>
<p>Scrapy defaults are optimized for crawling specific sites. These sites are
often handled by a single Scrapy spider, although this is not necessary or
required (for example, there are generic spiders that handle any given site
thrown at them).</p>
<p>In addition to this “focused crawl”, there is another common type of crawling
which covers a large (potentially unlimited) number of domains, and is only
limited by time or other arbitrary constraint, rather than stopping when the
domain was crawled to completion or when there are no more requests to perform.
These are called “broad crawls” and is the typical crawlers employed by search
engines.</p>
<p>These are some common properties often found in broad crawls:</p>
<p>As said above, Scrapy default settings are optimized for focused crawls, not
broad crawls. However, due to its asynchronous architecture, Scrapy is very
well suited for performing fast broad crawls. This page summarizes some things
you need to keep in mind when using Scrapy for doing broad crawls, along with
concrete suggestions of Scrapy settings to tune in order to achieve an
efficient broad crawl.</p>
<p>Concurrency is the number of requests that are processed in parallel. There is
a global limit and a per-domain limit.</p>
<p>The default global concurrency limit in Scrapy is not suitable for crawling
many different domains in parallel, so you will want to increase it. How much
to increase it will depend on how much CPU you crawler will have available. A
good starting point is <code class="docutils literal"><span class="pre">100</span></code>, but the best way to find out is by doing some
trials and identifying at what concurrency your Scrapy process gets CPU
bounded. For optimum performance, you should pick a concurrency where CPU usage
is at 80-90%.</p>
<p>To increase the global concurrency use:</p>
<p>Currently Scrapy does DNS resolution in a blocking way with usage of thread
pool. With higher concurrency levels the crawling could be slow or even fail
hitting DNS resolver timeouts. Possible solution to increase the number of
threads handling DNS queries. The DNS queue will be processed faster speeding
up establishing of connection and crawling overall.</p>
<p>To increase maximum thread pool size use:</p>
<p>If you have multiple crawling processes and single central DNS, it can act
like DoS attack on the DNS server resulting to slow down of entire network or
even blocking your machines. To avoid this setup your own DNS server with
local cache and upstream to some large DNS like OpenDNS or Verizon.</p>
<p>When doing broad crawls you are often only interested in the crawl rates you
get and any errors found. These stats are reported by Scrapy when using the
<code class="docutils literal"><span class="pre">INFO</span></code> log level. In order to save CPU (and log storage requirements) you
should not use <code class="docutils literal"><span class="pre">DEBUG</span></code> log level when preforming large broad crawls in
production. Using <code class="docutils literal"><span class="pre">DEBUG</span></code> level when developing your (broad) crawler may fine
though.</p>
<p>To set the log level use:</p>
<p>Disable cookies unless you <em>really</em> need. Cookies are often not needed when
doing broad crawls (search engine crawlers ignore them), and they improve
performance by saving some CPU cycles and reducing the memory footprint of your
Scrapy crawler.</p>
<p>To disable cookies use:</p>
<p>Retrying failed HTTP requests can slow down the crawls substantially, specially
when sites causes are very slow (or fail) to respond, thus causing a timeout
error which gets retried many times, unnecessarily, preventing crawler capacity
to be reused for other domains.</p>
<p>To disable retries use:</p>
<p>Unless you are crawling from a very slow connection (which shouldn’t be the
case for broad crawls) reduce the download timeout so that stuck requests are
discarded quickly and free up capacity to process the next ones.</p>
<p>To reduce the download timeout use:</p>
<p>Consider disabling redirects, unless you are interested in following them. When
doing broad crawls it’s common to save redirects and resolve them when
revisiting the site at a later crawl. This also help to keep the number of
request constant per crawl batch, otherwise redirect loops may cause the
crawler to dedicate too many resources on any specific domain.</p>
<p>To disable redirects use:</p>
<p>Some pages (up to 1%, based on empirical data from year 2013) declare
themselves as <a class="reference external" href="https://developers.google.com/webmasters/ajax-crawling/docs/getting-started">ajax crawlable</a>. This means they provide plain HTML
version of content that is usually available only via AJAX.
Pages can indicate it in two ways:</p>
<p>Scrapy handles (1) automatically; to handle (2) enable
<a class="reference internal" href="downloader-middleware.html#ajaxcrawl-middleware"><span class="std std-ref">AjaxCrawlMiddleware</span></a>:</p>
<p>When doing broad crawls it’s common to crawl a lot of “index” web pages;
AjaxCrawlMiddleware helps to crawl them correctly.
It is turned OFF by default because it has some performance overhead,
and enabling it for focused crawls doesn’t make much sense.</p>
<p>
        © Copyright 2008-2016, Scrapy developers.
      
        <span class="commit">
          Revision <code>5aebdac4</code>.
        </span>
</p>
<p class="caption"><span class="caption-text">First steps</span></p>
<p class="caption"><span class="caption-text">Basic concepts</span></p>
<p class="caption"><span class="caption-text">Built-in services</span></p>
<p class="caption"><span class="caption-text">Solving specific problems</span></p>
<p class="caption"><span class="caption-text">Extending Scrapy</span></p>
<p class="caption"><span class="caption-text">All the rest</span></p>
<p>Here is a list of tips and advice on using Firefox for scraping, along with a
list of useful Firefox add-ons to ease the scraping process.</p>
<p>Since Firefox add-ons operate on a live browser DOM, what you’ll actually see
when inspecting the page source is not the original HTML, but a modified one
after applying some browser clean up and executing Javascript code.  Firefox,
in particular, is known for adding <code class="docutils literal"><span class="pre">&lt;tbody&gt;</span></code> elements to tables.  Scrapy, on
the other hand, does not modify the original page HTML, so you won’t be able to
extract any data if you use <code class="docutils literal"><span class="pre">&lt;tbody&gt;</span></code> in your XPath expressions.</p>
<p>Therefore, you should keep in mind the following things when working with
Firefox and XPath:</p>
<p><a class="reference external" href="http://getfirebug.com">Firebug</a> is a widely known tool among web developers and it’s also very
useful for scraping. In particular, its <a class="reference external" href="https://www.youtube.com/watch?v=-pT_pDe54aA">Inspect Element</a> feature comes very
handy when you need to construct the XPaths for extracting data because it
allows you to view the HTML code of each page element while moving your mouse
over it.</p>
<p>See <a class="reference internal" href="firebug.html#topics-firebug"><span class="std std-ref">Using Firebug for scraping</span></a> for a detailed guide on how to use Firebug with
Scrapy.</p>
<p><a class="reference external" href="https://addons.mozilla.org/en-US/firefox/addon/xpather/">XPather</a> allows you to test XPath expressions directly on the pages.</p>
<p><a class="reference external" href="https://addons.mozilla.org/en-US/firefox/addon/xpath-checker/">XPath Checker</a> is another Firefox add-on for testing XPaths on your pages.</p>
<p><a class="reference external" href="https://addons.mozilla.org/en-US/firefox/addon/tamper-data/">Tamper Data</a> is a Firefox add-on which allows you to view and modify the HTTP
request headers sent by Firefox. Firebug also allows to view HTTP headers, but
not to modify them.</p>
<p><a class="reference external" href="https://addons.mozilla.org/en-US/firefox/addon/firecookie/">Firecookie</a> makes it easier to view and manage cookies. You can use this
extension to create a new cookie, delete existing cookies, see a list of cookies
for the current site, manage cookies permissions and a lot more.</p>
<p>
        © Copyright 2008-2016, Scrapy developers.
      
        <span class="commit">
          Revision <code>5aebdac4</code>.
        </span>
</p>
<p class="caption"><span class="caption-text">First steps</span></p>
<p class="caption"><span class="caption-text">Basic concepts</span></p>
<p class="caption"><span class="caption-text">Built-in services</span></p>
<p class="caption"><span class="caption-text">Solving specific problems</span></p>
<p class="caption"><span class="caption-text">Extending Scrapy</span></p>
<p class="caption"><span class="caption-text">All the rest</span></p>
<p class="first admonition-title">Note</p>
<p class="last">Google Directory, the example website used in this guide is no longer
available as it <a class="reference external" href="https://searchenginewatch.com/sew/news/2096661/google-directory-shut">has been shut down by Google</a>. The concepts in this guide
are still valid though. If you want to update this guide to use a new
(working) site, your contribution will be more than welcome!. See <a class="reference internal" href="../contributing.html#topics-contributing"><span class="std std-ref">Contributing to Scrapy</span></a>
for information on how to do so.</p>
<p>This document explains how to use <a class="reference external" href="http://getfirebug.com">Firebug</a> (a Firefox add-on) to make the
scraping process easier and more fun. For other useful Firefox add-ons see
<a class="reference internal" href="firefox.html#topics-firefox-addons"><span class="std std-ref">Useful Firefox add-ons for scraping</span></a>. There are some caveats with using Firefox add-ons
to inspect pages, see <a class="reference internal" href="firefox.html#topics-firefox-livedom"><span class="std std-ref">Caveats with inspecting the live browser DOM</span></a>.</p>
<p>In this example, we’ll show how to use <a class="reference external" href="http://getfirebug.com">Firebug</a> to scrape data from the
<a class="reference external" href="http://directory.google.com/">Google Directory</a>, which contains the same data as the <a class="reference external" href="http://www.dmoz.org">Open Directory
Project</a> used in the <a class="reference internal" href="../intro/tutorial.html#intro-tutorial"><span class="std std-ref">tutorial</span></a> but with a different
face.</p>
<p>Firebug comes with a very useful feature called <a class="reference external" href="https://www.youtube.com/watch?v=-pT_pDe54aA">Inspect Element</a> which allows
you to inspect the HTML code of the different page elements just by hovering
your mouse over them. Otherwise you would have to search for the tags manually
through the HTML body which can be a very tedious task.</p>
<p>In the following screenshot you can see the <a class="reference external" href="https://www.youtube.com/watch?v=-pT_pDe54aA">Inspect Element</a> tool in action.</p>
<p>At first sight, we can see that the directory is divided in categories, which
are also divided in subcategories.</p>
<p>However, it seems that there are more subcategories than the ones being shown
in this page, so we’ll keep looking:</p>
<p>As expected, the subcategories contain links to other subcategories, and also
links to actual websites, which is the purpose of the directory.</p>
<p>By looking at the category URLs we can see they share a pattern:</p>
<p>Once we know that, we are able to construct a regular expression to follow
those links. For example, the following one:</p>
<p>So, based on that regular expression we can create the first crawling rule:</p>
<p>The <a class="reference internal" href="spiders.html#scrapy.spiders.Rule" title="scrapy.spiders.Rule"><code class="xref py py-class docutils literal"><span class="pre">Rule</span></code></a> object instructs
<a class="reference internal" href="spiders.html#scrapy.spiders.CrawlSpider" title="scrapy.spiders.CrawlSpider"><code class="xref py py-class docutils literal"><span class="pre">CrawlSpider</span></code></a> based spiders how to follow the
category links. <code class="docutils literal"><span class="pre">parse_category</span></code> will be a method of the spider which will
process and extract data from those pages.</p>
<p>This is how the spider would look so far:</p>
<p>Now we’re going to write the code to extract data from those pages.</p>
<p>With the help of Firebug, we’ll take a look at some page containing links to
websites (say <a class="reference external" href="http://directory.google.com/Top/Arts/Awards/">http://directory.google.com/Top/Arts/Awards/</a>) and find out how we can
extract those links using <a class="reference internal" href="selectors.html#topics-selectors"><span class="std std-ref">Selectors</span></a>. We’ll also
use the <a class="reference internal" href="shell.html#topics-shell"><span class="std std-ref">Scrapy shell</span></a> to test those XPath’s and make sure
they work as we expect.</p>
<p>As you can see, the page markup is not very descriptive: the elements don’t
contain <code class="docutils literal"><span class="pre">id</span></code>, <code class="docutils literal"><span class="pre">class</span></code> or any attribute that clearly identifies them, so
we’ll use the ranking bars as a reference point to select the data to extract
when we construct our XPaths.</p>
<p>After using FireBug, we can see that each link is inside a <code class="docutils literal"><span class="pre">td</span></code> tag, which is
itself inside a <code class="docutils literal"><span class="pre">tr</span></code> tag that also contains the link’s ranking bar (in
another <code class="docutils literal"><span class="pre">td</span></code>).</p>
<p>So we can select the ranking bar, then find its parent (the <code class="docutils literal"><span class="pre">tr</span></code>), and then
finally, the link’s <code class="docutils literal"><span class="pre">td</span></code> (which contains the data we want to scrape).</p>
<p>This results in the following XPath:</p>
<p>It’s important to use the <a class="reference internal" href="shell.html#topics-shell"><span class="std std-ref">Scrapy shell</span></a> to test these
complex XPath expressions and make sure they work as expected.</p>
<p>Basically, that expression will look for the ranking bar’s <code class="docutils literal"><span class="pre">td</span></code> element, and
then select any <code class="docutils literal"><span class="pre">td</span></code> element who has a descendant <code class="docutils literal"><span class="pre">a</span></code> element whose
<code class="docutils literal"><span class="pre">href</span></code> attribute contains the string <code class="docutils literal"><span class="pre">#pagerank</span></code>“</p>
<p>Of course, this is not the only XPath, and maybe not the simpler one to select
that data. Another approach could be, for example, to find any <code class="docutils literal"><span class="pre">font</span></code> tags
that have that grey colour of the links,</p>
<p>Finally, we can write our <code class="docutils literal"><span class="pre">parse_category()</span></code> method:</p>
<p>Be aware that you may find some elements which appear in Firebug but
not in the original HTML, such as the typical case of <code class="docutils literal"><span class="pre">&lt;tbody&gt;</span></code>
elements.</p>
<p>or tags which Therefer   in page HTML
sources may on Firebug inspects the live DOM</p>
<p>
        © Copyright 2008-2016, Scrapy developers.
      
        <span class="commit">
          Revision <code>5aebdac4</code>.
        </span>
</p>
<p class="caption"><span class="caption-text">First steps</span></p>
<p class="caption"><span class="caption-text">Basic concepts</span></p>
<p class="caption"><span class="caption-text">Built-in services</span></p>
<p class="caption"><span class="caption-text">Solving specific problems</span></p>
<p class="caption"><span class="caption-text">Extending Scrapy</span></p>
<p class="caption"><span class="caption-text">All the rest</span></p>
<p>In Scrapy, objects such as Requests, Responses and Items have a finite
lifetime: they are created, used for a while, and finally destroyed.</p>
<p>From all those objects, the Request is probably the one with the longest
lifetime, as it stays waiting in the Scheduler queue until it’s time to process
it. For more info see <a class="reference internal" href="architecture.html#topics-architecture"><span class="std std-ref">Architecture overview</span></a>.</p>
<p>As these Scrapy objects have a (rather long) lifetime, there is always the risk
of accumulating them in memory without releasing them properly and thus causing
what is known as a “memory leak”.</p>
<p>To help debugging memory leaks, Scrapy provides a built-in mechanism for
tracking objects references called <a class="reference internal" href="#topics-leaks-trackrefs"><span class="std std-ref">trackref</span></a>,
and you can also use a third-party library called <a class="reference internal" href="#topics-leaks-guppy"><span class="std std-ref">Guppy</span></a> for more advanced memory debugging (see below for more
info). Both mechanisms must be used from the <a class="reference internal" href="telnetconsole.html#topics-telnetconsole"><span class="std std-ref">Telnet Console</span></a>.</p>
<p>It happens quite often (sometimes by accident, sometimes on purpose) that the
Scrapy developer passes objects referenced in Requests (for example, using the
<a class="reference internal" href="request-response.html#scrapy.http.Request.meta" title="scrapy.http.Request.meta"><code class="xref py py-attr docutils literal"><span class="pre">meta</span></code></a> attribute or the request callback function)
and that effectively bounds the lifetime of those referenced objects to the
lifetime of the Request. This is, by far, the most common cause of memory leaks
in Scrapy projects, and a quite difficult one to debug for newcomers.</p>
<p>In big projects, the spiders are typically written by different people and some
of those spiders could be “leaking” and thus affecting the rest of the other
(well-written) spiders when they get to run concurrently, which, in turn,
affects the whole crawling process.</p>
<p>The leak could also come from a custom middleware, pipeline or extension that
you have written, if you are not releasing the (previously allocated) resources
properly. For example, allocating resources on <a class="reference internal" href="signals.html#std:signal-spider_opened"><code class="xref std std-signal docutils literal"><span class="pre">spider_opened</span></code></a>
but not releasing them on <a class="reference internal" href="signals.html#std:signal-spider_closed"><code class="xref std std-signal docutils literal"><span class="pre">spider_closed</span></code></a> may cause problems if
you’re running <a class="reference internal" href="practices.html#run-multiple-spiders"><span class="std std-ref">multiple spiders per process</span></a>.</p>
<p>By default Scrapy keeps the request queue in memory; it includes
<a class="reference internal" href="request-response.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal"><span class="pre">Request</span></code></a> objects and all objects
referenced in Request attributes (e.g. in <a class="reference internal" href="request-response.html#scrapy.http.Request.meta" title="scrapy.http.Request.meta"><code class="xref py py-attr docutils literal"><span class="pre">meta</span></code></a>).
While not necessarily a leak, this can take a lot of memory. Enabling
<a class="reference internal" href="jobs.html#topics-jobs"><span class="std std-ref">persistent job queue</span></a> could help keeping memory usage
in control.</p>
<p><code class="xref py py-mod docutils literal"><span class="pre">trackref</span></code> is a module provided by Scrapy to debug the most common cases of
memory leaks. It basically tracks the references to all live Requests,
Responses, Item and Selector objects.</p>
<p>You can enter the telnet console and inspect how many objects (of the classes
mentioned above) are currently alive using the <code class="docutils literal"><span class="pre">prefs()</span></code> function which is an
alias to the <a class="reference internal" href="#scrapy.utils.trackref.print_live_refs" title="scrapy.utils.trackref.print_live_refs"><code class="xref py py-func docutils literal"><span class="pre">print_live_refs()</span></code></a> function:</p>
<p>As you can see, that report also shows the “age” of the oldest object in each
class. If you’re running multiple spiders per process chances are you can
figure out which spider is leaking by looking at the oldest request or response.
You can get the oldest object of each class using the
<a class="reference internal" href="#scrapy.utils.trackref.get_oldest" title="scrapy.utils.trackref.get_oldest"><code class="xref py py-func docutils literal"><span class="pre">get_oldest()</span></code></a> function (from the telnet console).</p>
<p>The objects tracked by <code class="docutils literal"><span class="pre">trackrefs</span></code> are all from these classes (and all its
subclasses):</p>
<p>Let’s see a concrete example of a hypothetical case of memory leaks.
Suppose we have some spider with a line similar to this one:</p>
<p>That line is passing a response reference inside a request which effectively
ties the response lifetime to the requests’ one, and that would definitely
cause memory leaks.</p>
<p>Let’s see how we can discover the cause (without knowing it
a-priori, of course) by using the <code class="docutils literal"><span class="pre">trackref</span></code> tool.</p>
<p>After the crawler is running for a few minutes and we notice its memory usage
has grown a lot, we can enter its telnet console and check the live
references:</p>
<p>The fact that there are so many live responses (and that they’re so old) is
definitely suspicious, as responses should have a relatively short lifetime
compared to Requests. The number of responses is similar to the number
of requests, so it looks like they are tied in a some way. We can now go
and check the code of the spider to discover the nasty line that is
generating the leaks (passing response references inside requests).</p>
<p>Sometimes extra information about live objects can be helpful.
Let’s check the oldest response:</p>
<p>If you want to iterate over all objects, instead of getting the oldest one, you
can use the <a class="reference internal" href="#scrapy.utils.trackref.iter_all" title="scrapy.utils.trackref.iter_all"><code class="xref py py-func docutils literal"><span class="pre">scrapy.utils.trackref.iter_all()</span></code></a> function:</p>
<p>If your project has too many spiders executed in parallel,
the output of <code class="xref py py-func docutils literal"><span class="pre">prefs()</span></code> can be difficult to read.
For this reason, that function has a <code class="docutils literal"><span class="pre">ignore</span></code> argument which can be used to
ignore a particular class (and all its subclases). For
example, this won’t show any live references to spiders:</p>
<p>Here are the functions available in the <a class="reference internal" href="#module-scrapy.utils.trackref" title="scrapy.utils.trackref: Track references of live objects"><code class="xref py py-mod docutils literal"><span class="pre">trackref</span></code></a> module.</p>
<p>Inherit from this class (instead of object) if you want to track live
instances with the <code class="docutils literal"><span class="pre">trackref</span></code> module.</p>
<p>Print a report of live references, grouped by class name.</p>
<p>Return the oldest object alive with the given class name, or <code class="docutils literal"><span class="pre">None</span></code> if
none is found. Use <a class="reference internal" href="#scrapy.utils.trackref.print_live_refs" title="scrapy.utils.trackref.print_live_refs"><code class="xref py py-func docutils literal"><span class="pre">print_live_refs()</span></code></a> first to get a list of all
tracked live objects per class name.</p>
<p>Return an iterator over all objects alive with the given class name, or
<code class="docutils literal"><span class="pre">None</span></code> if none is found. Use <a class="reference internal" href="#scrapy.utils.trackref.print_live_refs" title="scrapy.utils.trackref.print_live_refs"><code class="xref py py-func docutils literal"><span class="pre">print_live_refs()</span></code></a> first to get a list
of all tracked live objects per class name.</p>
<p><code class="docutils literal"><span class="pre">trackref</span></code> provides a very convenient mechanism for tracking down memory
leaks, but it only keeps track of the objects that are more likely to cause
memory leaks (Requests, Responses, Items, and Selectors). However, there are
other cases where the memory leaks could come from other (more or less obscure)
objects. If this is your case, and you can’t find your leaks using <code class="docutils literal"><span class="pre">trackref</span></code>,
you still have another resource: the <a class="reference external" href="https://pypi.python.org/pypi/guppy">Guppy library</a>.</p>
<p>If you use <code class="docutils literal"><span class="pre">pip</span></code>, you can install Guppy with the following command:</p>
<p>The telnet console also comes with a built-in shortcut (<code class="docutils literal"><span class="pre">hpy</span></code>) for accessing
Guppy heap objects. Here’s an example to view all Python objects available in
the heap using Guppy:</p>
<p>You can see that most space is used by dicts. Then, if you want to see from
which attribute those dicts are referenced, you could do:</p>
<p>As you can see, the Guppy module is very powerful but also requires some deep
knowledge about Python internals. For more info about Guppy, refer to the
<a class="reference external" href="http://guppy-pe.sourceforge.net/">Guppy documentation</a>.</p>
<p>Sometimes, you may notice that the memory usage of your Scrapy process will
only increase, but never decrease. Unfortunately, this could happen even
though neither Scrapy nor your project are leaking memory. This is due to a
(not so well) known problem of Python, which may not return released memory to
the operating system in some cases. For more information on this issue see:</p>
<p>The improvements proposed by Evan Jones, which are detailed in <a class="reference external" href="http://www.evanjones.ca/memoryallocator/">this paper</a>,
got merged in Python 2.5, but this only reduces the problem, it doesn’t fix it
completely. To quote the paper:</p>
<p>To keep memory consumption reasonable you can split the job into several
smaller jobs or enable <a class="reference internal" href="jobs.html#topics-jobs"><span class="std std-ref">persistent job queue</span></a>
and stop/start spider from time to time.</p>
<p>
        © Copyright 2008-2016, Scrapy developers.
      
        <span class="commit">
          Revision <code>5aebdac4</code>.
        </span>
</p>
<p class="caption"><span class="caption-text">First steps</span></p>
<p class="caption"><span class="caption-text">Basic concepts</span></p>
<p class="caption"><span class="caption-text">Built-in services</span></p>
<p class="caption"><span class="caption-text">Solving specific problems</span></p>
<p class="caption"><span class="caption-text">Extending Scrapy</span></p>
<p class="caption"><span class="caption-text">All the rest</span></p>
<p>Scrapy provides reusable <a class="reference internal" href="item-pipeline.html"><span class="doc">item pipelines</span></a> for
downloading files attached to a particular item (for example, when you scrape
products and also want to download their images locally). These pipelines share
a bit of functionality and structure (we refer to them as media pipelines), but
typically you’ll either use the Files Pipeline or the Images Pipeline.</p>
<p>Both pipelines implement these features:</p>
<p>The Images Pipeline has a few extra functions for processing images:</p>
<p>The pipelines also keep an internal queue of those media URLs which are currently
being scheduled for download, and connect those responses that arrive containing
the same media to that queue. This avoids downloading the same media more than
once when it’s shared by several items.</p>
<p>The typical workflow, when using the <code class="xref py py-class docutils literal"><span class="pre">FilesPipeline</span></code> goes like
this:</p>
<p>Using the <a class="reference internal" href="#scrapy.pipelines.images.ImagesPipeline" title="scrapy.pipelines.images.ImagesPipeline"><code class="xref py py-class docutils literal"><span class="pre">ImagesPipeline</span></code></a> is a lot like using the <code class="xref py py-class docutils literal"><span class="pre">FilesPipeline</span></code>,
except the default field names used are different: you use <code class="docutils literal"><span class="pre">image_urls</span></code> for
the image URLs of an item and it will populate an <code class="docutils literal"><span class="pre">images</span></code> field for the information
about the downloaded images.</p>
<p>The advantage of using the <a class="reference internal" href="#scrapy.pipelines.images.ImagesPipeline" title="scrapy.pipelines.images.ImagesPipeline"><code class="xref py py-class docutils literal"><span class="pre">ImagesPipeline</span></code></a> for image files is that you
can configure some extra functions like generating thumbnails and filtering
the images based on their size.</p>
<p>The Images Pipeline uses <a class="reference external" href="https://github.com/python-pillow/Pillow">Pillow</a> for thumbnailing and normalizing images to
JPEG/RGB format, so you need to install this library in order to use it.
<a class="reference external" href="http://www.pythonware.com/products/pil/">Python Imaging Library</a> (PIL) should also work in most cases, but it is known
to cause troubles in some setups, so we recommend to use <a class="reference external" href="https://github.com/python-pillow/Pillow">Pillow</a> instead of
PIL.</p>
<p id="std:setting-FILES_STORE">To enable your media pipeline you must first add it to your project
<a class="reference internal" href="settings.html#std:setting-ITEM_PIPELINES"><code class="xref std std-setting docutils literal"><span class="pre">ITEM_PIPELINES</span></code></a> setting.</p>
<p>For Images Pipeline, use:</p>
<p>For Files Pipeline, use:</p>
<p class="first admonition-title">Note</p>
<p class="last">You can also use both the Files and Images Pipeline at the same time.</p>
<p>Then, configure the target storage setting to a valid value that will be used
for storing the downloaded images. Otherwise the pipeline will remain disabled,
even if you include it in the <a class="reference internal" href="settings.html#std:setting-ITEM_PIPELINES"><code class="xref std std-setting docutils literal"><span class="pre">ITEM_PIPELINES</span></code></a> setting.</p>
<p>For the Files Pipeline, set the <a class="reference internal" href="#std:setting-FILES_STORE"><code class="xref std std-setting docutils literal"><span class="pre">FILES_STORE</span></code></a> setting:</p>
<p>For the Images Pipeline, set the <a class="reference internal" href="#std:setting-IMAGES_STORE"><code class="xref std std-setting docutils literal"><span class="pre">IMAGES_STORE</span></code></a> setting:</p>
<p>File system is currently the only officially supported storage, but there is
also support for storing files in <a class="reference external" href="https://aws.amazon.com/s3/">Amazon S3</a>.</p>
<p>The files are stored using a <a class="reference external" href="https://en.wikipedia.org/wiki/SHA_hash_functions">SHA1 hash</a> of their URLs for the file names.</p>
<p>For example, the following image URL:</p>
<p>Whose <cite>SHA1 hash</cite> is:</p>
<p>Will be downloaded and stored in the following file:</p>
<p>Where:</p>
<p id="std:setting-IMAGES_STORE_S3_ACL"><a class="reference internal" href="#std:setting-FILES_STORE"><code class="xref std std-setting docutils literal"><span class="pre">FILES_STORE</span></code></a> and <a class="reference internal" href="#std:setting-IMAGES_STORE"><code class="xref std std-setting docutils literal"><span class="pre">IMAGES_STORE</span></code></a> can represent an Amazon S3
bucket. Scrapy will automatically upload the files to the bucket.</p>
<p>For example, this is a valid <a class="reference internal" href="#std:setting-IMAGES_STORE"><code class="xref std std-setting docutils literal"><span class="pre">IMAGES_STORE</span></code></a> value:</p>
<p>You can modify the Access Control List (ACL) policy used for the stored files,
which is defined by the <a class="reference internal" href="#std:setting-FILES_STORE_S3_ACL"><code class="xref std std-setting docutils literal"><span class="pre">FILES_STORE_S3_ACL</span></code></a> and
<a class="reference internal" href="#std:setting-IMAGES_STORE_S3_ACL"><code class="xref std std-setting docutils literal"><span class="pre">IMAGES_STORE_S3_ACL</span></code></a> settings. By default, the ACL is set to
<code class="docutils literal"><span class="pre">private</span></code>. To make the files publicly available use the <code class="docutils literal"><span class="pre">public-read</span></code>
policy:</p>
<p>For more information, see <a class="reference external" href="http://docs.aws.amazon.com/AmazonS3/latest/dev/acl-overview.html#canned-acl">canned ACLs</a> in the Amazon S3 Developer Guide.</p>
<p id="std:setting-IMAGES_RESULT_FIELD">In order to use a media pipeline first, <a class="reference internal" href="#topics-media-pipeline-enabling"><span class="std std-ref">enable it</span></a>.</p>
<p>Then, if a spider returns a dict with the URLs key (<code class="docutils literal"><span class="pre">file_urls</span></code> or
<code class="docutils literal"><span class="pre">image_urls</span></code>, for the Files or Images Pipeline respectively), the pipeline will
put the results under respective key (<code class="docutils literal"><span class="pre">files</span></code> or <code class="docutils literal"><span class="pre">images</span></code>).</p>
<p>If you prefer to use <a class="reference internal" href="items.html#scrapy.item.Item" title="scrapy.item.Item"><code class="xref py py-class docutils literal"><span class="pre">Item</span></code></a>, then define a custom item with the
necessary fields, like in this example for Images Pipeline:</p>
<p>If you want to use another field name for the URLs key or for the results key,
it is also possible to override it.</p>
<p>For the Files Pipeline, set <a class="reference internal" href="#std:setting-FILES_URLS_FIELD"><code class="xref std std-setting docutils literal"><span class="pre">FILES_URLS_FIELD</span></code></a> and/or
<a class="reference internal" href="#std:setting-FILES_RESULT_FIELD"><code class="xref std std-setting docutils literal"><span class="pre">FILES_RESULT_FIELD</span></code></a> settings:</p>
<p>For the Images Pipeline, set <a class="reference internal" href="#std:setting-IMAGES_URLS_FIELD"><code class="xref std std-setting docutils literal"><span class="pre">IMAGES_URLS_FIELD</span></code></a> and/or
<a class="reference internal" href="#std:setting-IMAGES_RESULT_FIELD"><code class="xref std std-setting docutils literal"><span class="pre">IMAGES_RESULT_FIELD</span></code></a> settings:</p>
<p>If you need something more complex and want to override the custom pipeline
behaviour, see <a class="reference internal" href="#topics-media-pipeline-override"><span class="std std-ref">Extending the Media Pipelines</span></a>.</p>
<p>If you have multiple image pipelines inheriting from ImagePipeline and you want
to have different settings in different pipelines you can set setting keys
preceded with uppercase name of your pipeline class. E.g. if your pipeline is
called MyPipeline and you want to have custom IMAGES_URLS_FIELD you define
setting MYPIPELINE_IMAGES_URLS_FIELD and your custom settings will be used.</p>
<p id="std:setting-FILES_EXPIRES">The Image Pipeline avoids downloading files that were downloaded recently. To
adjust this retention delay use the <a class="reference internal" href="#std:setting-FILES_EXPIRES"><code class="xref std std-setting docutils literal"><span class="pre">FILES_EXPIRES</span></code></a> setting (or
<a class="reference internal" href="#std:setting-IMAGES_EXPIRES"><code class="xref std std-setting docutils literal"><span class="pre">IMAGES_EXPIRES</span></code></a>, in case of Images Pipeline), which
specifies the delay in number of days:</p>
<p>The default value for both settings is 90 days.</p>
<p>If you have pipeline that subclasses FilesPipeline and you’d like to have
different setting for it you can set setting keys preceded by uppercase
class name. E.g. given pipeline class called MyPipeline you can set setting key:</p>
<p>and pipeline class MyPipeline will have expiration time set to 180.</p>
<p>The Images Pipeline can automatically create thumbnails of the downloaded
images.</p>
<p id="std:setting-IMAGES_THUMBS">In order use this feature, you must set <a class="reference internal" href="#std:setting-IMAGES_THUMBS"><code class="xref std std-setting docutils literal"><span class="pre">IMAGES_THUMBS</span></code></a> to a dictionary
where the keys are the thumbnail names and the values are their dimensions.</p>
<p>For example:</p>
<p>When you use this feature, the Images Pipeline will create thumbnails of the
each specified size with this format:</p>
<p>Where:</p>
<p>Example of image files stored using <code class="docutils literal"><span class="pre">small</span></code> and <code class="docutils literal"><span class="pre">big</span></code> thumbnail names:</p>
<p>The first one is the full image, as downloaded from the site.</p>
<p id="std:setting-IMAGES_MIN_WIDTH">When using the Images Pipeline, you can drop images which are too small, by
specifying the minimum allowed size in the <a class="reference internal" href="#std:setting-IMAGES_MIN_HEIGHT"><code class="xref std std-setting docutils literal"><span class="pre">IMAGES_MIN_HEIGHT</span></code></a> and
<a class="reference internal" href="#std:setting-IMAGES_MIN_WIDTH"><code class="xref std std-setting docutils literal"><span class="pre">IMAGES_MIN_WIDTH</span></code></a> settings.</p>
<p>For example:</p>
<p class="first admonition-title">Note</p>
<p class="last">The size constraints don’t affect thumbnail generation at all.</p>
<p>It is possible to set just one size constraint or both. When setting both of
them, only images that satisfy both minimum sizes will be saved. For the
above example, images of sizes (105 x 105) or (105 x 200) or (200 x 105) will
all be dropped because at least one dimension is shorter than the constraint.</p>
<p>By default, there are no size constraints, so all images are processed.</p>
<p id="std:setting-MEDIA_ALLOW_REDIRECTS">By default media pipelines ignore redirects, i.e. an HTTP redirection
to a media file URL request will mean the media download is considered failed.</p>
<p>To handle media redirections, set this setting to <code class="docutils literal"><span class="pre">True</span></code>:</p>
<p>See here the methods that you can override in your custom Files Pipeline:</p>
<p>As seen on the workflow, the pipeline will get the URLs of the images to
download from the item. In order to do this, you can override the
<a class="reference internal" href="#scrapy.pipelines.files.FilesPipeline.get_media_requests" title="scrapy.pipelines.files.FilesPipeline.get_media_requests"><code class="xref py py-meth docutils literal"><span class="pre">get_media_requests()</span></code></a> method and return a Request for each
file URL:</p>
<p>Those requests will be processed by the pipeline and, when they have finished
downloading, the results will be sent to the
<a class="reference internal" href="#scrapy.pipelines.files.FilesPipeline.item_completed" title="scrapy.pipelines.files.FilesPipeline.item_completed"><code class="xref py py-meth docutils literal"><span class="pre">item_completed()</span></code></a> method, as a list of 2-element tuples.
Each tuple will contain <code class="docutils literal"><span class="pre">(success,</span> <span class="pre">file_info_or_error)</span></code> where:</p>
<p>The list of tuples received by <a class="reference internal" href="#scrapy.pipelines.files.FilesPipeline.item_completed" title="scrapy.pipelines.files.FilesPipeline.item_completed"><code class="xref py py-meth docutils literal"><span class="pre">item_completed()</span></code></a> is
guaranteed to retain the same order of the requests returned from the
<a class="reference internal" href="#scrapy.pipelines.files.FilesPipeline.get_media_requests" title="scrapy.pipelines.files.FilesPipeline.get_media_requests"><code class="xref py py-meth docutils literal"><span class="pre">get_media_requests()</span></code></a> method.</p>
<p>Here’s a typical value of the <code class="docutils literal"><span class="pre">results</span></code> argument:</p>
<p>By default the <a class="reference internal" href="#scrapy.pipelines.files.FilesPipeline.get_media_requests" title="scrapy.pipelines.files.FilesPipeline.get_media_requests"><code class="xref py py-meth docutils literal"><span class="pre">get_media_requests()</span></code></a> method returns <code class="docutils literal"><span class="pre">None</span></code> which
means there are no files to download for the item.</p>
<p>The <a class="reference internal" href="#scrapy.pipelines.files.FilesPipeline.item_completed" title="scrapy.pipelines.files.FilesPipeline.item_completed"><code class="xref py py-meth docutils literal"><span class="pre">FilesPipeline.item_completed()</span></code></a> method called when all file
requests for a single item have completed (either finished downloading, or
failed for some reason).</p>
<p>The <a class="reference internal" href="#scrapy.pipelines.files.FilesPipeline.item_completed" title="scrapy.pipelines.files.FilesPipeline.item_completed"><code class="xref py py-meth docutils literal"><span class="pre">item_completed()</span></code></a> method must return the
output that will be sent to subsequent item pipeline stages, so you must
return (or drop) the item, as you would in any pipeline.</p>
<p>Here is an example of the <a class="reference internal" href="#scrapy.pipelines.files.FilesPipeline.item_completed" title="scrapy.pipelines.files.FilesPipeline.item_completed"><code class="xref py py-meth docutils literal"><span class="pre">item_completed()</span></code></a> method where we
store the downloaded file paths (passed in results) in the <code class="docutils literal"><span class="pre">file_paths</span></code>
item field, and we drop the item if it doesn’t contain any files:</p>
<p>By default, the <a class="reference internal" href="#scrapy.pipelines.files.FilesPipeline.item_completed" title="scrapy.pipelines.files.FilesPipeline.item_completed"><code class="xref py py-meth docutils literal"><span class="pre">item_completed()</span></code></a> method returns the item.</p>
<p>See here the methods that you can override in your custom Images Pipeline:</p>
<p>Works the same way as <code class="xref py py-meth docutils literal"><span class="pre">FilesPipeline.get_media_requests()</span></code> method,
but using a different field name for image urls.</p>
<p>Must return a Request for each image URL.</p>
<p>The <a class="reference internal" href="#scrapy.pipelines.images.ImagesPipeline.item_completed" title="scrapy.pipelines.images.ImagesPipeline.item_completed"><code class="xref py py-meth docutils literal"><span class="pre">ImagesPipeline.item_completed()</span></code></a> method is called when all image
requests for a single item have completed (either finished downloading, or
failed for some reason).</p>
<p>Works the same way as <code class="xref py py-meth docutils literal"><span class="pre">FilesPipeline.item_completed()</span></code> method,
but using a different field names for storing image downloading results.</p>
<p>By default, the <a class="reference internal" href="#scrapy.pipelines.images.ImagesPipeline.item_completed" title="scrapy.pipelines.images.ImagesPipeline.item_completed"><code class="xref py py-meth docutils literal"><span class="pre">item_completed()</span></code></a> method returns the item.</p>
<p>Here is a full example of the Images Pipeline whose methods are examplified
above:</p>
<p>
        © Copyright 2008-2016, Scrapy developers.
      
        <span class="commit">
          Revision <code>5aebdac4</code>.
        </span>
</p>
<p class="caption"><span class="caption-text">First steps</span></p>
<p class="caption"><span class="caption-text">Basic concepts</span></p>
<p class="caption"><span class="caption-text">Built-in services</span></p>
<p class="caption"><span class="caption-text">Solving specific problems</span></p>
<p class="caption"><span class="caption-text">Extending Scrapy</span></p>
<p class="caption"><span class="caption-text">All the rest</span></p>
<p>This section describes the different options you have for deploying your Scrapy
spiders to run them on a regular basis. Running Scrapy spiders in your local
machine is very convenient for the (early) development stage, but not so much
when you need to execute long-running spiders or move spiders to run in
production continuously. This is where the solutions for deploying Scrapy
spiders come in.</p>
<p>Popular choices for deploying Scrapy spiders are:</p>
<p><a class="reference external" href="https://github.com/scrapy/scrapyd">Scrapyd</a> is an open source application to run Scrapy spiders. It provides
a server with HTTP API, capable of running and monitoring Scrapy spiders.</p>
<p>To deploy spiders to Scrapyd, you can use the scrapyd-deploy tool provided by
the <a class="reference external" href="https://github.com/scrapy/scrapyd-client">scrapyd-client</a> package. Please refer to the <a class="reference external" href="https://scrapyd.readthedocs.io/en/latest/deploy.html">scrapyd-deploy
documentation</a> for more information.</p>
<p>Scrapyd is maintained by some of the Scrapy developers.</p>
<p><a class="reference external" href="http://scrapinghub.com/scrapy-cloud/">Scrapy Cloud</a> is a hosted, cloud-based service by <a class="reference external" href="http://scrapinghub.com/">Scrapinghub</a>,
the company behind Scrapy.</p>
<p>Scrapy Cloud removes the need to setup and monitor servers
and provides a nice UI to manage spiders and review scraped items,
logs and stats.</p>
<p>To deploy spiders to Scrapy Cloud you can use the <a class="reference external" href="http://doc.scrapinghub.com/shub.html">shub</a> command line tool.
Please refer to the <a class="reference external" href="http://doc.scrapinghub.com/scrapy-cloud.html">Scrapy Cloud documentation</a> for more information.</p>
<p>Scrapy Cloud is compatible with Scrapyd and one can switch between
them as needed - the configuration is read from the <code class="docutils literal"><span class="pre">scrapy.cfg</span></code> file
just like <code class="docutils literal"><span class="pre">scrapyd-deploy</span></code>.</p>
<p>
        © Copyright 2008-2016, Scrapy developers.
      
        <span class="commit">
          Revision <code>5aebdac4</code>.
        </span>
</p>
<p class="caption"><span class="caption-text">First steps</span></p>
<p class="caption"><span class="caption-text">Basic concepts</span></p>
<p class="caption"><span class="caption-text">Built-in services</span></p>
<p class="caption"><span class="caption-text">Solving specific problems</span></p>
<p class="caption"><span class="caption-text">Extending Scrapy</span></p>
<p class="caption"><span class="caption-text">All the rest</span></p>
<p>This is an extension for automatically throttling crawling speed based on load
of both the Scrapy server and the website you are crawling.</p>
<p>AutoThrottle extension adjusts download delays dynamically to make spider send
<a class="reference internal" href="#std:setting-AUTOTHROTTLE_TARGET_CONCURRENCY"><code class="xref std std-setting docutils literal"><span class="pre">AUTOTHROTTLE_TARGET_CONCURRENCY</span></code></a> concurrent requests on average
to each remote website.</p>
<p>It uses download latency to compute the delays. The main idea is the
following: if a server needs <code class="docutils literal"><span class="pre">latency</span></code> seconds to respond, a client
should send a request each <code class="docutils literal"><span class="pre">latency/N</span></code> seconds to have <code class="docutils literal"><span class="pre">N</span></code> requests
processed in parallel.</p>
<p>Instead of adjusting the delays one can just set a small fixed
download delay and impose hard limits on concurrency using
<a class="reference internal" href="settings.html#std:setting-CONCURRENT_REQUESTS_PER_DOMAIN"><code class="xref std std-setting docutils literal"><span class="pre">CONCURRENT_REQUESTS_PER_DOMAIN</span></code></a> or
<a class="reference internal" href="settings.html#std:setting-CONCURRENT_REQUESTS_PER_IP"><code class="xref std std-setting docutils literal"><span class="pre">CONCURRENT_REQUESTS_PER_IP</span></code></a> options. It will provide a similar
effect, but there are some important differences:</p>
<p>AutoThrottle doesn’t have these issues.</p>
<p>AutoThrottle algorithm adjusts download delays based on the following rules:</p>
<p class="first admonition-title">Note</p>
<p class="last">The AutoThrottle extension honours the standard Scrapy settings for
concurrency and delay. This means that it will respect
<a class="reference internal" href="settings.html#std:setting-CONCURRENT_REQUESTS_PER_DOMAIN"><code class="xref std std-setting docutils literal"><span class="pre">CONCURRENT_REQUESTS_PER_DOMAIN</span></code></a> and
<a class="reference internal" href="settings.html#std:setting-CONCURRENT_REQUESTS_PER_IP"><code class="xref std std-setting docutils literal"><span class="pre">CONCURRENT_REQUESTS_PER_IP</span></code></a> options and
never set a download delay lower than <a class="reference internal" href="settings.html#std:setting-DOWNLOAD_DELAY"><code class="xref std std-setting docutils literal"><span class="pre">DOWNLOAD_DELAY</span></code></a>.</p>
<p id="download-latency">In Scrapy, the download latency is measured as the time elapsed between
establishing the TCP connection and receiving the HTTP headers.</p>
<p>Note that these latencies are very hard to measure accurately in a cooperative
multitasking environment because Scrapy may be busy processing a spider
callback, for example, and unable to attend downloads. However, these latencies
should still give a reasonable estimate of how busy Scrapy (and ultimately, the
server) is, and this extension builds on that premise.</p>
<p>The settings used to control the AutoThrottle extension are:</p>
<p>For more information see <a class="reference internal" href="#autothrottle-algorithm"><span class="std std-ref">How it works</span></a>.</p>
<p>Default: <code class="docutils literal"><span class="pre">False</span></code></p>
<p>Enables the AutoThrottle extension.</p>
<p>Default: <code class="docutils literal"><span class="pre">5.0</span></code></p>
<p>The initial download delay (in seconds).</p>
<p>Default: <code class="docutils literal"><span class="pre">60.0</span></code></p>
<p>The maximum download delay (in seconds) to be set in case of high latencies.</p>
<p><span class="versionmodified">New in version 1.1.</span></p>
<p>Default: <code class="docutils literal"><span class="pre">1.0</span></code></p>
<p>Average number of requests Scrapy should be sending in parallel to remote
websites.</p>
<p>By default, AutoThrottle adjusts the delay to send a single
concurrent request to each of the remote websites. Set this option to
a higher value (e.g. <code class="docutils literal"><span class="pre">2.0</span></code>) to increase the throughput and the load on remote
servers. A lower <code class="docutils literal"><span class="pre">AUTOTHROTTLE_TARGET_CONCURRENCY</span></code> value
(e.g. <code class="docutils literal"><span class="pre">0.5</span></code>) makes the crawler more conservative and polite.</p>
<p>Note that <a class="reference internal" href="settings.html#std:setting-CONCURRENT_REQUESTS_PER_DOMAIN"><code class="xref std std-setting docutils literal"><span class="pre">CONCURRENT_REQUESTS_PER_DOMAIN</span></code></a>
and <a class="reference internal" href="settings.html#std:setting-CONCURRENT_REQUESTS_PER_IP"><code class="xref std std-setting docutils literal"><span class="pre">CONCURRENT_REQUESTS_PER_IP</span></code></a> options are still respected
when AutoThrottle extension is enabled. This means that if
<code class="docutils literal"><span class="pre">AUTOTHROTTLE_TARGET_CONCURRENCY</span></code> is set to a value higher than
<a class="reference internal" href="settings.html#std:setting-CONCURRENT_REQUESTS_PER_DOMAIN"><code class="xref std std-setting docutils literal"><span class="pre">CONCURRENT_REQUESTS_PER_DOMAIN</span></code></a> or
<a class="reference internal" href="settings.html#std:setting-CONCURRENT_REQUESTS_PER_IP"><code class="xref std std-setting docutils literal"><span class="pre">CONCURRENT_REQUESTS_PER_IP</span></code></a>, the crawler won’t reach this number
of concurrent requests.</p>
<p>At every given time point Scrapy can be sending more or less concurrent
requests than <code class="docutils literal"><span class="pre">AUTOTHROTTLE_TARGET_CONCURRENCY</span></code>; it is a suggested
value the crawler tries to approach, not a hard limit.</p>
<p>Default: <code class="docutils literal"><span class="pre">False</span></code></p>
<p>Enable AutoThrottle debug mode which will display stats on every response
received, so you can see how the throttling parameters are being adjusted in
real time.</p>
<p>
        © Copyright 2008-2016, Scrapy developers.
      
        <span class="commit">
          Revision <code>5aebdac4</code>.
        </span>
</p>
<p class="caption"><span class="caption-text">First steps</span></p>
<p class="caption"><span class="caption-text">Basic concepts</span></p>
<p class="caption"><span class="caption-text">Built-in services</span></p>
<p class="caption"><span class="caption-text">Solving specific problems</span></p>
<p class="caption"><span class="caption-text">Extending Scrapy</span></p>
<p class="caption"><span class="caption-text">All the rest</span></p>
<p><span class="versionmodified">New in version 0.17.</span></p>
<p>Scrapy comes with a simple benchmarking suite that spawns a local HTTP server
and crawls it at the maximum possible speed. The goal of this benchmarking is
to get an idea of how Scrapy performs in your hardware, in order to have a
common baseline for comparisons. It uses a simple spider that does nothing and
just follows links.</p>
<p>To run it use:</p>
<p>You should see an output like this:</p>
<p>That tells you that Scrapy is able to crawl about 3000 pages per minute in the
hardware where you run it. Note that this is a very simple spider intended to
follow links, any custom spider you write will probably do more stuff which
results in slower crawl rates. How slower depends on how much your spider does
and how well it’s written.</p>
<p>In the future, more cases will be added to the benchmarking suite to cover
other common scenarios.</p>
<p>
        © Copyright 2008-2016, Scrapy developers.
      
        <span class="commit">
          Revision <code>5aebdac4</code>.
        </span>
</p>
<p class="caption"><span class="caption-text">First steps</span></p>
<p class="caption"><span class="caption-text">Basic concepts</span></p>
<p class="caption"><span class="caption-text">Built-in services</span></p>
<p class="caption"><span class="caption-text">Solving specific problems</span></p>
<p class="caption"><span class="caption-text">Extending Scrapy</span></p>
<p class="caption"><span class="caption-text">All the rest</span></p>
<p>Sometimes, for big sites, it’s desirable to pause crawls and be able to resume
them later.</p>
<p>Scrapy supports this functionality out of the box by providing the following
facilities:</p>
<p>To enable persistence support you just need to define a <em>job directory</em> through
the <code class="docutils literal"><span class="pre">JOBDIR</span></code> setting. This directory will be for storing all required data to
keep the state of a single job (ie. a spider run).  It’s important to note that
this directory must not be shared by different spiders, or even different
jobs/runs of the same spider, as it’s meant to be used for storing the state of
a <em>single</em> job.</p>
<p>To start a spider with persistence supported enabled, run it like this:</p>
<p>Then, you can stop the spider safely at any time (by pressing Ctrl-C or sending
a signal), and resume it later by issuing the same command:</p>
<p>Sometimes you’ll want to keep some persistent spider state between pause/resume
batches. You can use the <code class="docutils literal"><span class="pre">spider.state</span></code> attribute for that, which should be a
dict. There’s a built-in extension that takes care of serializing, storing and
loading that attribute from the job directory, when the spider starts and
stops.</p>
<p>Here’s an example of a callback that uses the spider state (other spider code
is omitted for brevity):</p>
<p>There are a few things to keep in mind if you want to be able to use the Scrapy
persistence support:</p>
<p>Cookies may expire. So, if you don’t resume your spider quickly the requests
scheduled may no longer work. This won’t be an issue if you spider doesn’t rely
on cookies.</p>
<p>Requests must be serializable by the <cite>pickle</cite> module, in order for persistence
to work, so you should make sure that your requests are serializable.</p>
<p>The most common issue here is to use <code class="docutils literal"><span class="pre">lambda</span></code> functions on request callbacks that
can’t be persisted.</p>
<p>So, for example, this won’t work:</p>
<p>But this will:</p>
<p>If you wish to log the requests that couldn’t be serialized, you can set the
<a class="reference internal" href="settings.html#std:setting-SCHEDULER_DEBUG"><code class="xref std std-setting docutils literal"><span class="pre">SCHEDULER_DEBUG</span></code></a> setting to <code class="docutils literal"><span class="pre">True</span></code> in the project’s settings page.
It is <code class="docutils literal"><span class="pre">False</span></code> by default.</p>
<p>
        © Copyright 2008-2016, Scrapy developers.
      
        <span class="commit">
          Revision <code>5aebdac4</code>.
        </span>
</p>
<p class="caption"><span class="caption-text">First steps</span></p>
<p class="caption"><span class="caption-text">Basic concepts</span></p>
<p class="caption"><span class="caption-text">Built-in services</span></p>
<p class="caption"><span class="caption-text">Solving specific problems</span></p>
<p class="caption"><span class="caption-text">Extending Scrapy</span></p>
<p class="caption"><span class="caption-text">All the rest</span></p>
<p>This document describes the architecture of Scrapy and how its components
interact.</p>
<p>The following diagram shows an overview of the Scrapy architecture with its
components and an outline of the data flow that takes place inside the system
(shown by the red arrows). A brief description of the components is included
below with links for more detailed information about them. The data flow is
also described below.</p>
<p>The data flow in Scrapy is controlled by the execution engine, and goes like
this:</p>
<p>The engine is responsible for controlling the data flow between all components
of the system, and triggering events when certain actions occur. See the
<a class="reference internal" href="#data-flow"><span class="std std-ref">Data Flow</span></a> section above for more details.</p>
<p>The Scheduler receives requests from the engine and enqueues them for feeding
them later (also to the engine) when the engine requests them.</p>
<p>The Downloader is responsible for fetching web pages and feeding them to the
engine which, in turn, feeds them to the spiders.</p>
<p>Spiders are custom classes written by Scrapy users to parse responses and
extract items (aka scraped items) from them or additional requests to
follow. For more information see <a class="reference internal" href="spiders.html#topics-spiders"><span class="std std-ref">Spiders</span></a>.</p>
<p>The Item Pipeline is responsible for processing the items once they have been
extracted (or scraped) by the spiders. Typical tasks include cleansing,
validation and persistence (like storing the item in a database). For more
information see <a class="reference internal" href="item-pipeline.html#topics-item-pipeline"><span class="std std-ref">Item Pipeline</span></a>.</p>
<p>Downloader middlewares are specific hooks that sit between the Engine and the
Downloader and process requests when they pass from the Engine to the
Downloader, and responses that pass from Downloader to the Engine.</p>
<p>Use a Downloader middleware if you need to do one of the following:</p>
<p>For more information see <a class="reference internal" href="downloader-middleware.html#topics-downloader-middleware"><span class="std std-ref">Downloader Middleware</span></a>.</p>
<p>Spider middlewares are specific hooks that sit between the Engine and the
Spiders and are able to process spider input (responses) and output (items and
requests).</p>
<p>Use a Spider middleware if you need to</p>
<p>For more information see <a class="reference internal" href="spider-middleware.html#topics-spider-middleware"><span class="std std-ref">Spider Middleware</span></a>.</p>
<p>Scrapy is written with <a class="reference external" href="https://twistedmatrix.com/trac/">Twisted</a>, a popular event-driven networking framework
for Python. Thus, it’s implemented using a non-blocking (aka asynchronous) code
for concurrency.</p>
<p>For more information about asynchronous programming and Twisted see these
links:</p>
<p>
        © Copyright 2008-2016, Scrapy developers.
      
        <span class="commit">
          Revision <code>5aebdac4</code>.
        </span>
</p>
<p class="caption"><span class="caption-text">First steps</span></p>
<p class="caption"><span class="caption-text">Basic concepts</span></p>
<p class="caption"><span class="caption-text">Built-in services</span></p>
<p class="caption"><span class="caption-text">Solving specific problems</span></p>
<p class="caption"><span class="caption-text">Extending Scrapy</span></p>
<p class="caption"><span class="caption-text">All the rest</span></p>
<p>The downloader middleware is a framework of hooks into Scrapy’s
request/response processing.  It’s a light, low-level system for globally
altering Scrapy’s requests and responses.</p>
<p>To activate a downloader middleware component, add it to the
<a class="reference internal" href="settings.html#std:setting-DOWNLOADER_MIDDLEWARES"><code class="xref std std-setting docutils literal"><span class="pre">DOWNLOADER_MIDDLEWARES</span></code></a> setting, which is a dict whose keys are the
middleware class paths and their values are the middleware orders.</p>
<p>Here’s an example:</p>
<p>The <a class="reference internal" href="settings.html#std:setting-DOWNLOADER_MIDDLEWARES"><code class="xref std std-setting docutils literal"><span class="pre">DOWNLOADER_MIDDLEWARES</span></code></a> setting is merged with the
<a class="reference internal" href="settings.html#std:setting-DOWNLOADER_MIDDLEWARES_BASE"><code class="xref std std-setting docutils literal"><span class="pre">DOWNLOADER_MIDDLEWARES_BASE</span></code></a> setting defined in Scrapy (and not meant
to be overridden) and then sorted by order to get the final sorted list of
enabled middlewares: the first middleware is the one closer to the engine and
the last is the one closer to the downloader. In other words,
the <a class="reference internal" href="#scrapy.downloadermiddlewares.DownloaderMiddleware.process_request" title="scrapy.downloadermiddlewares.DownloaderMiddleware.process_request"><code class="xref py py-meth docutils literal"><span class="pre">process_request()</span></code></a>
method of each middleware will be invoked in increasing
middleware order (100, 200, 300, ...) and the <a class="reference internal" href="#scrapy.downloadermiddlewares.DownloaderMiddleware.process_response" title="scrapy.downloadermiddlewares.DownloaderMiddleware.process_response"><code class="xref py py-meth docutils literal"><span class="pre">process_response()</span></code></a> method
of each middleware will be invoked in decreasing order.</p>
<p>To decide which order to assign to your middleware see the
<a class="reference internal" href="settings.html#std:setting-DOWNLOADER_MIDDLEWARES_BASE"><code class="xref std std-setting docutils literal"><span class="pre">DOWNLOADER_MIDDLEWARES_BASE</span></code></a> setting and pick a value according to
where you want to insert the middleware. The order does matter because each
middleware performs a different action and your middleware could depend on some
previous (or subsequent) middleware being applied.</p>
<p>If you want to disable a built-in middleware (the ones defined in
<a class="reference internal" href="settings.html#std:setting-DOWNLOADER_MIDDLEWARES_BASE"><code class="xref std std-setting docutils literal"><span class="pre">DOWNLOADER_MIDDLEWARES_BASE</span></code></a> and enabled by default) you must define it
in your project’s <a class="reference internal" href="settings.html#std:setting-DOWNLOADER_MIDDLEWARES"><code class="xref std std-setting docutils literal"><span class="pre">DOWNLOADER_MIDDLEWARES</span></code></a> setting and assign <cite>None</cite>
as its value.  For example, if you want to disable the user-agent middleware:</p>
<p>Finally, keep in mind that some middlewares may need to be enabled through a
particular setting. See each middleware documentation for more info.</p>
<p>Each middleware component is a Python class that defines one or
more of the following methods:</p>
<p class="first admonition-title">Note</p>
<p class="last">Any of the downloader middleware methods may also return a deferred.</p>
<p>This method is called for each request that goes through the download
middleware.</p>
<p><a class="reference internal" href="#scrapy.downloadermiddlewares.DownloaderMiddleware.process_request" title="scrapy.downloadermiddlewares.DownloaderMiddleware.process_request"><code class="xref py py-meth docutils literal"><span class="pre">process_request()</span></code></a> should either: return <code class="docutils literal"><span class="pre">None</span></code>, return a
<a class="reference internal" href="request-response.html#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal"><span class="pre">Response</span></code></a> object, return a <a class="reference internal" href="request-response.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal"><span class="pre">Request</span></code></a>
object, or raise <a class="reference internal" href="exceptions.html#scrapy.exceptions.IgnoreRequest" title="scrapy.exceptions.IgnoreRequest"><code class="xref py py-exc docutils literal"><span class="pre">IgnoreRequest</span></code></a>.</p>
<p>If it returns <code class="docutils literal"><span class="pre">None</span></code>, Scrapy will continue processing this request, executing all
other middlewares until, finally, the appropriate downloader handler is called
the request performed (and its response downloaded).</p>
<p>If it returns a <a class="reference internal" href="request-response.html#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal"><span class="pre">Response</span></code></a> object, Scrapy won’t bother
calling <em>any</em> other <a class="reference internal" href="#scrapy.downloadermiddlewares.DownloaderMiddleware.process_request" title="scrapy.downloadermiddlewares.DownloaderMiddleware.process_request"><code class="xref py py-meth docutils literal"><span class="pre">process_request()</span></code></a> or <a class="reference internal" href="#scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception" title="scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception"><code class="xref py py-meth docutils literal"><span class="pre">process_exception()</span></code></a> methods,
or the appropriate download function; it’ll return that response. The <a class="reference internal" href="#scrapy.downloadermiddlewares.DownloaderMiddleware.process_response" title="scrapy.downloadermiddlewares.DownloaderMiddleware.process_response"><code class="xref py py-meth docutils literal"><span class="pre">process_response()</span></code></a>
methods of installed middleware is always called on every response.</p>
<p>If it returns a <a class="reference internal" href="request-response.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal"><span class="pre">Request</span></code></a> object, Scrapy will stop calling
process_request methods and reschedule the returned request. Once the newly returned
request is performed, the appropriate middleware chain will be called on
the downloaded response.</p>
<p>If it raises an <a class="reference internal" href="exceptions.html#scrapy.exceptions.IgnoreRequest" title="scrapy.exceptions.IgnoreRequest"><code class="xref py py-exc docutils literal"><span class="pre">IgnoreRequest</span></code></a> exception, the
<a class="reference internal" href="#scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception" title="scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception"><code class="xref py py-meth docutils literal"><span class="pre">process_exception()</span></code></a> methods of installed downloader middleware will be called.
If none of them handle the exception, the errback function of the request
(<code class="docutils literal"><span class="pre">Request.errback</span></code>) is called. If no code handles the raised exception, it is
ignored and not logged (unlike other exceptions).</p>
<p><a class="reference internal" href="#scrapy.downloadermiddlewares.DownloaderMiddleware.process_response" title="scrapy.downloadermiddlewares.DownloaderMiddleware.process_response"><code class="xref py py-meth docutils literal"><span class="pre">process_response()</span></code></a> should either: return a <a class="reference internal" href="request-response.html#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal"><span class="pre">Response</span></code></a>
object, return a <a class="reference internal" href="request-response.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal"><span class="pre">Request</span></code></a> object or
raise a <a class="reference internal" href="exceptions.html#scrapy.exceptions.IgnoreRequest" title="scrapy.exceptions.IgnoreRequest"><code class="xref py py-exc docutils literal"><span class="pre">IgnoreRequest</span></code></a> exception.</p>
<p>If it returns a <a class="reference internal" href="request-response.html#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal"><span class="pre">Response</span></code></a> (it could be the same given
response, or a brand-new one), that response will continue to be processed
with the <a class="reference internal" href="#scrapy.downloadermiddlewares.DownloaderMiddleware.process_response" title="scrapy.downloadermiddlewares.DownloaderMiddleware.process_response"><code class="xref py py-meth docutils literal"><span class="pre">process_response()</span></code></a> of the next middleware in the chain.</p>
<p>If it returns a <a class="reference internal" href="request-response.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal"><span class="pre">Request</span></code></a> object, the middleware chain is
halted and the returned request is rescheduled to be downloaded in the future.
This is the same behavior as if a request is returned from <a class="reference internal" href="#scrapy.downloadermiddlewares.DownloaderMiddleware.process_request" title="scrapy.downloadermiddlewares.DownloaderMiddleware.process_request"><code class="xref py py-meth docutils literal"><span class="pre">process_request()</span></code></a>.</p>
<p>If it raises an <a class="reference internal" href="exceptions.html#scrapy.exceptions.IgnoreRequest" title="scrapy.exceptions.IgnoreRequest"><code class="xref py py-exc docutils literal"><span class="pre">IgnoreRequest</span></code></a> exception, the errback
function of the request (<code class="docutils literal"><span class="pre">Request.errback</span></code>) is called. If no code handles the raised
exception, it is ignored and not logged (unlike other exceptions).</p>
<p>Scrapy calls <a class="reference internal" href="#scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception" title="scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception"><code class="xref py py-meth docutils literal"><span class="pre">process_exception()</span></code></a> when a download handler
or a <a class="reference internal" href="#scrapy.downloadermiddlewares.DownloaderMiddleware.process_request" title="scrapy.downloadermiddlewares.DownloaderMiddleware.process_request"><code class="xref py py-meth docutils literal"><span class="pre">process_request()</span></code></a> (from a downloader middleware) raises an
exception (including an <a class="reference internal" href="exceptions.html#scrapy.exceptions.IgnoreRequest" title="scrapy.exceptions.IgnoreRequest"><code class="xref py py-exc docutils literal"><span class="pre">IgnoreRequest</span></code></a> exception)</p>
<p><a class="reference internal" href="#scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception" title="scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception"><code class="xref py py-meth docutils literal"><span class="pre">process_exception()</span></code></a> should return: either <code class="docutils literal"><span class="pre">None</span></code>,
a <a class="reference internal" href="request-response.html#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal"><span class="pre">Response</span></code></a> object, or a <a class="reference internal" href="request-response.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal"><span class="pre">Request</span></code></a> object.</p>
<p>If it returns <code class="docutils literal"><span class="pre">None</span></code>, Scrapy will continue processing this exception,
executing any other <a class="reference internal" href="#scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception" title="scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception"><code class="xref py py-meth docutils literal"><span class="pre">process_exception()</span></code></a> methods of installed middleware,
until no middleware is left and the default exception handling kicks in.</p>
<p>If it returns a <a class="reference internal" href="request-response.html#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal"><span class="pre">Response</span></code></a> object, the <a class="reference internal" href="#scrapy.downloadermiddlewares.DownloaderMiddleware.process_response" title="scrapy.downloadermiddlewares.DownloaderMiddleware.process_response"><code class="xref py py-meth docutils literal"><span class="pre">process_response()</span></code></a>
method chain of installed middleware is started, and Scrapy won’t bother calling
any other <a class="reference internal" href="#scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception" title="scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception"><code class="xref py py-meth docutils literal"><span class="pre">process_exception()</span></code></a> methods of middleware.</p>
<p>If it returns a <a class="reference internal" href="request-response.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal"><span class="pre">Request</span></code></a> object, the returned request is
rescheduled to be downloaded in the future. This stops the execution of
<a class="reference internal" href="#scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception" title="scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception"><code class="xref py py-meth docutils literal"><span class="pre">process_exception()</span></code></a> methods of the middleware the same as returning a
response would.</p>
<p>This page describes all downloader middleware components that come with
Scrapy. For information on how to use them and how to write your own downloader
middleware, see the <a class="reference internal" href="#topics-downloader-middleware"><span class="std std-ref">downloader middleware usage guide</span></a>.</p>
<p>For a list of the components enabled by default (and their orders) see the
<a class="reference internal" href="settings.html#std:setting-DOWNLOADER_MIDDLEWARES_BASE"><code class="xref std std-setting docutils literal"><span class="pre">DOWNLOADER_MIDDLEWARES_BASE</span></code></a> setting.</p>
<p>This middleware enables working with sites that require cookies, such as
those that use sessions. It keeps track of cookies sent by web servers, and
send them back on subsequent requests (from that spider), just like web
browsers do.</p>
<p>The following settings can be used to configure the cookie middleware:</p>
<p><span class="versionmodified">New in version 0.15.</span></p>
<p>There is support for keeping multiple cookie sessions per spider by using the
<a class="reference internal" href="#std:reqmeta-cookiejar"><code class="xref std std-reqmeta docutils literal"><span class="pre">cookiejar</span></code></a> Request meta key. By default it uses a single cookie jar
(session), but you can pass an identifier to use different ones.</p>
<p>For example:</p>
<p>Keep in mind that the <a class="reference internal" href="#std:reqmeta-cookiejar"><code class="xref std std-reqmeta docutils literal"><span class="pre">cookiejar</span></code></a> meta key is not “sticky”. You need to keep
passing it along on subsequent requests. For example:</p>
<p>Default: <code class="docutils literal"><span class="pre">True</span></code></p>
<p>Whether to enable the cookies middleware. If disabled, no cookies will be sent
to web servers.</p>
<p>Default: <code class="docutils literal"><span class="pre">False</span></code></p>
<p>If enabled, Scrapy will log all cookies sent in requests (ie. <code class="docutils literal"><span class="pre">Cookie</span></code>
header) and all cookies received in responses (ie. <code class="docutils literal"><span class="pre">Set-Cookie</span></code> header).</p>
<p>Here’s an example of a log with <a class="reference internal" href="#std:setting-COOKIES_DEBUG"><code class="xref std std-setting docutils literal"><span class="pre">COOKIES_DEBUG</span></code></a> enabled:</p>
<p>This middleware sets all default requests headers specified in the
<a class="reference internal" href="settings.html#std:setting-DEFAULT_REQUEST_HEADERS"><code class="xref std std-setting docutils literal"><span class="pre">DEFAULT_REQUEST_HEADERS</span></code></a> setting.</p>
<p>This middleware sets the download timeout for requests specified in the
<a class="reference internal" href="settings.html#std:setting-DOWNLOAD_TIMEOUT"><code class="xref std std-setting docutils literal"><span class="pre">DOWNLOAD_TIMEOUT</span></code></a> setting or <code class="xref py py-attr docutils literal"><span class="pre">download_timeout</span></code>
spider attribute.</p>
<p class="first admonition-title">Note</p>
<p class="last">You can also set download timeout per-request using
<a class="reference internal" href="request-response.html#std:reqmeta-download_timeout"><code class="xref std std-reqmeta docutils literal"><span class="pre">download_timeout</span></code></a> Request.meta key; this is supported
even when DownloadTimeoutMiddleware is disabled.</p>
<p>This middleware authenticates all requests generated from certain spiders
using <a class="reference external" href="https://en.wikipedia.org/wiki/Basic_access_authentication">Basic access authentication</a> (aka. HTTP auth).</p>
<p>To enable HTTP authentication from certain spiders, set the <code class="docutils literal"><span class="pre">http_user</span></code>
and <code class="docutils literal"><span class="pre">http_pass</span></code> attributes of those spiders.</p>
<p>Example:</p>
<p>This middleware provides low-level cache to all HTTP requests and responses.
It has to be combined with a cache storage backend as well as a cache policy.</p>
<p>Scrapy ships with three HTTP cache storage backends:</p>
<p>You can change the HTTP cache storage backend with the <a class="reference internal" href="#std:setting-HTTPCACHE_STORAGE"><code class="xref std std-setting docutils literal"><span class="pre">HTTPCACHE_STORAGE</span></code></a>
setting. Or you can also implement your own storage backend.</p>
<p>Scrapy ships with two HTTP cache policies:</p>
<p>You can change the HTTP cache policy with the <a class="reference internal" href="#std:setting-HTTPCACHE_POLICY"><code class="xref std std-setting docutils literal"><span class="pre">HTTPCACHE_POLICY</span></code></a>
setting. Or you can also implement your own policy.</p>
<p id="std:reqmeta-dont_cache">You can also avoid caching a response on every policy using <a class="reference internal" href="#std:reqmeta-dont_cache"><code class="xref std std-reqmeta docutils literal"><span class="pre">dont_cache</span></code></a> meta key equals <cite>True</cite>.</p>
<p>This policy has no awareness of any HTTP Cache-Control directives.
Every request and its corresponding response are cached.  When the same
request is seen again, the response is returned without transferring
anything from the Internet.</p>
<p>The Dummy policy is useful for testing spiders faster (without having
to wait for downloads every time) and for trying your spider offline,
when an Internet connection is not available. The goal is to be able to
“replay” a spider run <em>exactly as it ran before</em>.</p>
<p>In order to use this policy, set:</p>
<p>This policy provides a RFC2616 compliant HTTP cache, i.e. with HTTP
Cache-Control awareness, aimed at production and used in continuous
runs to avoid downloading unmodified data (to save bandwidth and speed up crawls).</p>
<p>what is implemented:</p>
<p class="first">Do not attempt to store responses/requests with <cite>no-store</cite> cache-control directive set</p>
<p class="first">Do not serve responses from cache if <cite>no-cache</cite> cache-control directive is set even for fresh responses</p>
<p class="first">Compute freshness lifetime from <cite>max-age</cite> cache-control directive</p>
<p class="first">Compute freshness lifetime from <cite>Expires</cite> response header</p>
<p class="first">Compute freshness lifetime from <cite>Last-Modified</cite> response header (heuristic used by Firefox)</p>
<p class="first">Compute current age from <cite>Age</cite> response header</p>
<p class="first">Compute current age from <cite>Date</cite> header</p>
<p class="first">Revalidate stale responses based on <cite>Last-Modified</cite> response header</p>
<p class="first">Revalidate stale responses based on <cite>ETag</cite> response header</p>
<p class="first">Set <cite>Date</cite> header for any received response missing it</p>
<p class="first">Support <cite>max-stale</cite> cache-control directive in requests</p>
<p>This allows spiders to be configured with the full RFC2616 cache policy,
but avoid revalidation on a request-by-request basis, while remaining
conformant with the HTTP spec.</p>
<p>Example:</p>
<p>Add <cite>Cache-Control: max-stale=600</cite> to Request headers to accept responses that
have exceeded their expiration time by no more than 600 seconds.</p>
<p>See also: RFC2616, 14.9.3</p>
<p>what is missing:</p>
<p>In order to use this policy, set:</p>
<p>File system storage backend is available for the HTTP cache middleware.</p>
<p>In order to use this storage backend, set:</p>
<p>Each request/response pair is stored in a different directory containing
the following files:</p>
<p>The directory name is made from the request fingerprint (see
<code class="docutils literal"><span class="pre">scrapy.utils.request.fingerprint</span></code>), and one level of subdirectories is
used to avoid creating too many files into the same directory (which is
inefficient in many file systems). An example directory could be:</p>
<p><span class="versionmodified">New in version 0.13.</span></p>
<p>A <a class="reference external" href="https://en.wikipedia.org/wiki/Dbm">DBM</a> storage backend is also available for the HTTP cache middleware.</p>
<p>By default, it uses the <a class="reference external" href="https://docs.python.org/2/library/anydbm.html">anydbm</a> module, but you can change it with the
<a class="reference internal" href="#std:setting-HTTPCACHE_DBM_MODULE"><code class="xref std std-setting docutils literal"><span class="pre">HTTPCACHE_DBM_MODULE</span></code></a> setting.</p>
<p>In order to use this storage backend, set:</p>
<p><span class="versionmodified">New in version 0.23.</span></p>
<p>A <a class="reference external" href="https://github.com/google/leveldb">LevelDB</a> storage backend is also available for the HTTP cache middleware.</p>
<p>This backend is not recommended for development because only one process can
access LevelDB databases at the same time, so you can’t run a crawl and open
the scrapy shell in parallel for the same spider.</p>
<p>In order to use this storage backend:</p>
<p>The <a class="reference internal" href="#scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware" title="scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware"><code class="xref py py-class docutils literal"><span class="pre">HttpCacheMiddleware</span></code></a> can be configured through the following
settings:</p>
<p><span class="versionmodified">New in version 0.11.</span></p>
<p>Default: <code class="docutils literal"><span class="pre">False</span></code></p>
<p>Whether the HTTP cache will be enabled.</p>
<p><span class="versionmodified">Changed in version 0.11: </span>Before 0.11, <a class="reference internal" href="#std:setting-HTTPCACHE_DIR"><code class="xref std std-setting docutils literal"><span class="pre">HTTPCACHE_DIR</span></code></a> was used to enable cache.</p>
<p>Default: <code class="docutils literal"><span class="pre">0</span></code></p>
<p>Expiration time for cached requests, in seconds.</p>
<p>Cached requests older than this time will be re-downloaded. If zero, cached
requests will never expire.</p>
<p><span class="versionmodified">Changed in version 0.11: </span>Before 0.11, zero meant cached requests always expire.</p>
<p>Default: <code class="docutils literal"><span class="pre">'httpcache'</span></code></p>
<p>The directory to use for storing the (low-level) HTTP cache. If empty, the HTTP
cache will be disabled. If a relative path is given, is taken relative to the
project data dir. For more info see: <a class="reference internal" href="commands.html#topics-project-structure"><span class="std std-ref">Default structure of Scrapy projects</span></a>.</p>
<p><span class="versionmodified">New in version 0.10.</span></p>
<p>Default: <code class="docutils literal"><span class="pre">[]</span></code></p>
<p>Don’t cache response with these HTTP codes.</p>
<p>Default: <code class="docutils literal"><span class="pre">False</span></code></p>
<p>If enabled, requests not found in the cache will be ignored instead of downloaded.</p>
<p><span class="versionmodified">New in version 0.10.</span></p>
<p>Default: <code class="docutils literal"><span class="pre">['file']</span></code></p>
<p>Don’t cache responses with these URI schemes.</p>
<p>Default: <code class="docutils literal"><span class="pre">'scrapy.extensions.httpcache.FilesystemCacheStorage'</span></code></p>
<p>The class which implements the cache storage backend.</p>
<p><span class="versionmodified">New in version 0.13.</span></p>
<p>Default: <code class="docutils literal"><span class="pre">'anydbm'</span></code></p>
<p>The database module to use in the <a class="reference internal" href="#httpcache-storage-dbm"><span class="std std-ref">DBM storage backend</span></a>. This setting is specific to the DBM backend.</p>
<p><span class="versionmodified">New in version 0.18.</span></p>
<p>Default: <code class="docutils literal"><span class="pre">'scrapy.extensions.httpcache.DummyPolicy'</span></code></p>
<p>The class which implements the cache policy.</p>
<p><span class="versionmodified">New in version 1.0.</span></p>
<p>Default: <code class="docutils literal"><span class="pre">False</span></code></p>
<p>If enabled, will compress all cached data with gzip.
This setting is specific to the Filesystem backend.</p>
<p><span class="versionmodified">New in version 1.1.</span></p>
<p>Default: <code class="docutils literal"><span class="pre">False</span></code></p>
<p>If enabled, will cache pages unconditionally.</p>
<p>A spider may wish to have all responses available in the cache, for
future use with <cite>Cache-Control: max-stale</cite>, for instance. The
DummyPolicy caches all responses but never revalidates them, and
sometimes a more nuanced policy is desirable.</p>
<p>This setting still respects <cite>Cache-Control: no-store</cite> directives in responses.
If you don’t want that, filter <cite>no-store</cite> out of the Cache-Control headers in
responses you feedto the cache middleware.</p>
<p><span class="versionmodified">New in version 1.1.</span></p>
<p>Default: <code class="docutils literal"><span class="pre">[]</span></code></p>
<p>List of Cache-Control directives in responses to be ignored.</p>
<p>Sites often set “no-store”, “no-cache”, “must-revalidate”, etc., but get
upset at the traffic a spider can generate if it respects those
directives. This allows to selectively ignore Cache-Control directives
that are known to be unimportant for the sites being crawled.</p>
<p>We assume that the spider will not issue Cache-Control directives
in requests unless it actually needs them, so directives in requests are
not filtered.</p>
<p>This middleware allows compressed (gzip, deflate) traffic to be
sent/received from web sites.</p>
<p>This middleware also supports decoding <a class="reference external" href="https://www.ietf.org/rfc/rfc7932.txt">brotli-compressed</a> responses,
provided <a class="reference external" href="https://pypi.python.org/pypi/brotlipy">brotlipy</a> is installed.</p>
<p>Default: <code class="docutils literal"><span class="pre">True</span></code></p>
<p>Whether the Compression middleware will be enabled.</p>
<p><span class="versionmodified">New in version 0.8.</span></p>
<p>This middleware sets the HTTP proxy to use for requests, by setting the
<code class="docutils literal"><span class="pre">proxy</span></code> meta value for <a class="reference internal" href="request-response.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal"><span class="pre">Request</span></code></a> objects.</p>
<p>Like the Python standard library modules <a class="reference external" href="https://docs.python.org/2/library/urllib.html">urllib</a> and <a class="reference external" href="https://docs.python.org/2/library/urllib2.html">urllib2</a>, it obeys
the following environment variables:</p>
<p>You can also set the meta key <code class="docutils literal"><span class="pre">proxy</span></code> per-request, to a value like
<code class="docutils literal"><span class="pre">http://some_proxy_server:port</span></code> or <code class="docutils literal"><span class="pre">http://username:password@some_proxy_server:port</span></code>.
Keep in mind this value will take precedence over <code class="docutils literal"><span class="pre">http_proxy</span></code>/<code class="docutils literal"><span class="pre">https_proxy</span></code>
environment variables, and it will also ignore <code class="docutils literal"><span class="pre">no_proxy</span></code> environment variable.</p>
<p>This middleware handles redirection of requests based on response status.</p>
<p id="std:reqmeta-redirect_urls">The urls which the request goes through (while being redirected) can be found
in the <code class="docutils literal"><span class="pre">redirect_urls</span></code> <a class="reference internal" href="request-response.html#scrapy.http.Request.meta" title="scrapy.http.Request.meta"><code class="xref py py-attr docutils literal"><span class="pre">Request.meta</span></code></a> key.</p>
<p>The <a class="reference internal" href="#scrapy.downloadermiddlewares.redirect.RedirectMiddleware" title="scrapy.downloadermiddlewares.redirect.RedirectMiddleware"><code class="xref py py-class docutils literal"><span class="pre">RedirectMiddleware</span></code></a> can be configured through the following
settings (see the settings documentation for more info):</p>
<p id="std:reqmeta-dont_redirect">If <a class="reference internal" href="request-response.html#scrapy.http.Request.meta" title="scrapy.http.Request.meta"><code class="xref py py-attr docutils literal"><span class="pre">Request.meta</span></code></a> has <code class="docutils literal"><span class="pre">dont_redirect</span></code>
key set to True, the request will be ignored by this middleware.</p>
<p>If you want to handle some redirect status codes in your spider, you can
specify these in the <code class="docutils literal"><span class="pre">handle_httpstatus_list</span></code> spider attribute.</p>
<p>For example, if you want the redirect middleware to ignore 301 and 302
responses (and pass them through to your spider) you can do this:</p>
<p>The <code class="docutils literal"><span class="pre">handle_httpstatus_list</span></code> key of <a class="reference internal" href="request-response.html#scrapy.http.Request.meta" title="scrapy.http.Request.meta"><code class="xref py py-attr docutils literal"><span class="pre">Request.meta</span></code></a> can also be used to specify which response codes to
allow on a per-request basis. You can also set the meta key
<code class="docutils literal"><span class="pre">handle_httpstatus_all</span></code> to <code class="docutils literal"><span class="pre">True</span></code> if you want to allow any response code
for a request.</p>
<p><span class="versionmodified">New in version 0.13.</span></p>
<p>Default: <code class="docutils literal"><span class="pre">True</span></code></p>
<p>Whether the Redirect middleware will be enabled.</p>
<p>Default: <code class="docutils literal"><span class="pre">20</span></code></p>
<p>The maximum number of redirections that will be followed for a single request.</p>
<p>This middleware handles redirection of requests based on meta-refresh html tag.</p>
<p>The <a class="reference internal" href="#scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware" title="scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware"><code class="xref py py-class docutils literal"><span class="pre">MetaRefreshMiddleware</span></code></a> can be configured through the following
settings (see the settings documentation for more info):</p>
<p>This middleware obey <a class="reference internal" href="settings.html#std:setting-REDIRECT_MAX_TIMES"><code class="xref std std-setting docutils literal"><span class="pre">REDIRECT_MAX_TIMES</span></code></a> setting, <a class="reference internal" href="#std:reqmeta-dont_redirect"><code class="xref std std-reqmeta docutils literal"><span class="pre">dont_redirect</span></code></a>
and <a class="reference internal" href="#std:reqmeta-redirect_urls"><code class="xref std std-reqmeta docutils literal"><span class="pre">redirect_urls</span></code></a> request meta keys as described for <a class="reference internal" href="#scrapy.downloadermiddlewares.redirect.RedirectMiddleware" title="scrapy.downloadermiddlewares.redirect.RedirectMiddleware"><code class="xref py py-class docutils literal"><span class="pre">RedirectMiddleware</span></code></a></p>
<p><span class="versionmodified">New in version 0.17.</span></p>
<p>Default: <code class="docutils literal"><span class="pre">True</span></code></p>
<p>Whether the Meta Refresh middleware will be enabled.</p>
<p>Default: <code class="docutils literal"><span class="pre">100</span></code></p>
<p>The maximum meta-refresh delay (in seconds) to follow the redirection.
Some sites use meta-refresh for redirecting to a session expired page, so we
restrict automatic redirection to the maximum delay.</p>
<p>A middleware to retry failed requests that are potentially caused by
temporary problems such as a connection timeout or HTTP 500 error.</p>
<p>Failed pages are collected on the scraping process and rescheduled at the
end, once the spider has finished crawling all regular (non failed) pages.
Once there are no more failed pages to retry, this middleware sends a signal
(retry_complete), so other extensions could connect to that signal.</p>
<p>The <a class="reference internal" href="#scrapy.downloadermiddlewares.retry.RetryMiddleware" title="scrapy.downloadermiddlewares.retry.RetryMiddleware"><code class="xref py py-class docutils literal"><span class="pre">RetryMiddleware</span></code></a> can be configured through the following
settings (see the settings documentation for more info):</p>
<p id="std:reqmeta-dont_retry">If <a class="reference internal" href="request-response.html#scrapy.http.Request.meta" title="scrapy.http.Request.meta"><code class="xref py py-attr docutils literal"><span class="pre">Request.meta</span></code></a> has <code class="docutils literal"><span class="pre">dont_retry</span></code> key
set to True, the request will be ignored by this middleware.</p>
<p><span class="versionmodified">New in version 0.13.</span></p>
<p>Default: <code class="docutils literal"><span class="pre">True</span></code></p>
<p>Whether the Retry middleware will be enabled.</p>
<p>Default: <code class="docutils literal"><span class="pre">2</span></code></p>
<p>Maximum number of times to retry, in addition to the first download.</p>
<p>Maximum number of retries can also be specified per-request using
<a class="reference internal" href="request-response.html#std:reqmeta-max_retry_times"><code class="xref std std-reqmeta docutils literal"><span class="pre">max_retry_times</span></code></a> attribute of <a class="reference internal" href="request-response.html#scrapy.http.Request.meta" title="scrapy.http.Request.meta"><code class="xref py py-attr docutils literal"><span class="pre">Request.meta</span></code></a>.
When initialized, the <a class="reference internal" href="request-response.html#std:reqmeta-max_retry_times"><code class="xref std std-reqmeta docutils literal"><span class="pre">max_retry_times</span></code></a> meta key takes higher
precedence over the <a class="reference internal" href="#std:setting-RETRY_TIMES"><code class="xref std std-setting docutils literal"><span class="pre">RETRY_TIMES</span></code></a> setting.</p>
<p>Default: <code class="docutils literal"><span class="pre">[500,</span> <span class="pre">502,</span> <span class="pre">503,</span> <span class="pre">504,</span> <span class="pre">408]</span></code></p>
<p>Which HTTP response codes to retry. Other errors (DNS lookup issues,
connections lost, etc) are always retried.</p>
<p>In some cases you may want to add 400 to <a class="reference internal" href="#std:setting-RETRY_HTTP_CODES"><code class="xref std std-setting docutils literal"><span class="pre">RETRY_HTTP_CODES</span></code></a> because
it is a common code used to indicate server overload. It is not included by
default because HTTP specs say so.</p>
<p>This middleware filters out requests forbidden by the robots.txt exclusion
standard.</p>
<p>To make sure Scrapy respects robots.txt make sure the middleware is enabled
and the <a class="reference internal" href="settings.html#std:setting-ROBOTSTXT_OBEY"><code class="xref std std-setting docutils literal"><span class="pre">ROBOTSTXT_OBEY</span></code></a> setting is enabled.</p>
<p id="std:reqmeta-dont_obey_robotstxt">If <a class="reference internal" href="request-response.html#scrapy.http.Request.meta" title="scrapy.http.Request.meta"><code class="xref py py-attr docutils literal"><span class="pre">Request.meta</span></code></a> has
<code class="docutils literal"><span class="pre">dont_obey_robotstxt</span></code> key set to True
the request will be ignored by this middleware even if
<a class="reference internal" href="settings.html#std:setting-ROBOTSTXT_OBEY"><code class="xref std std-setting docutils literal"><span class="pre">ROBOTSTXT_OBEY</span></code></a> is enabled.</p>
<p>Middleware that stores stats of all requests, responses and exceptions that
pass through it.</p>
<p>To use this middleware you must enable the <a class="reference internal" href="settings.html#std:setting-DOWNLOADER_STATS"><code class="xref std std-setting docutils literal"><span class="pre">DOWNLOADER_STATS</span></code></a>
setting.</p>
<p>Middleware that allows spiders to override the default user agent.</p>
<p>In order for a spider to override the default user agent, its <cite>user_agent</cite>
attribute must be set.</p>
<p>Middleware that finds ‘AJAX crawlable’ page variants based
on meta-fragment html tag. See
<a class="reference external" href="https://developers.google.com/webmasters/ajax-crawling/docs/getting-started">https://developers.google.com/webmasters/ajax-crawling/docs/getting-started</a>
for more info.</p>
<p class="first admonition-title">Note</p>
<p class="last">Scrapy finds ‘AJAX crawlable’ pages for URLs like
<code class="docutils literal"><span class="pre">'http://example.com/!#foo=bar'</span></code> even without this middleware.
AjaxCrawlMiddleware is necessary when URL doesn’t contain <code class="docutils literal"><span class="pre">'!#'</span></code>.
This is often a case for ‘index’ or ‘main’ website pages.</p>
<p><span class="versionmodified">New in version 0.21.</span></p>
<p>Default: <code class="docutils literal"><span class="pre">False</span></code></p>
<p>Whether the AjaxCrawlMiddleware will be enabled. You may want to
enable it for <a class="reference internal" href="broad-crawls.html#topics-broad-crawls"><span class="std std-ref">broad crawls</span></a>.</p>
<p>Default: <code class="docutils literal"><span class="pre">True</span></code></p>
<p>Whether or not to enable the <code class="xref py py-class docutils literal"><span class="pre">HttpProxyMiddleware</span></code>.</p>
<p>Default: <code class="docutils literal"><span class="pre">"latin-1"</span></code></p>
<p>The default encoding for proxy authentication on <code class="xref py py-class docutils literal"><span class="pre">HttpProxyMiddleware</span></code>.</p>
<p>
        © Copyright 2008-2016, Scrapy developers.
      
        <span class="commit">
          Revision <code>5aebdac4</code>.
        </span>
</p>
<p class="caption"><span class="caption-text">First steps</span></p>
<p class="caption"><span class="caption-text">Basic concepts</span></p>
<p class="caption"><span class="caption-text">Built-in services</span></p>
<p class="caption"><span class="caption-text">Solving specific problems</span></p>
<p class="caption"><span class="caption-text">Extending Scrapy</span></p>
<p class="caption"><span class="caption-text">All the rest</span></p>
<p>The spider middleware is a framework of hooks into Scrapy’s spider processing
mechanism where you can plug custom functionality to process the responses that
are sent to <a class="reference internal" href="spiders.html#topics-spiders"><span class="std std-ref">Spiders</span></a> for processing and to process the requests
and items that are generated from spiders.</p>
<p>To activate a spider middleware component, add it to the
<a class="reference internal" href="settings.html#std:setting-SPIDER_MIDDLEWARES"><code class="xref std std-setting docutils literal"><span class="pre">SPIDER_MIDDLEWARES</span></code></a> setting, which is a dict whose keys are the
middleware class path and their values are the middleware orders.</p>
<p>Here’s an example:</p>
<p>The <a class="reference internal" href="settings.html#std:setting-SPIDER_MIDDLEWARES"><code class="xref std std-setting docutils literal"><span class="pre">SPIDER_MIDDLEWARES</span></code></a> setting is merged with the
<a class="reference internal" href="settings.html#std:setting-SPIDER_MIDDLEWARES_BASE"><code class="xref std std-setting docutils literal"><span class="pre">SPIDER_MIDDLEWARES_BASE</span></code></a> setting defined in Scrapy (and not meant to
be overridden) and then sorted by order to get the final sorted list of enabled
middlewares: the first middleware is the one closer to the engine and the last
is the one closer to the spider. In other words,
the <a class="reference internal" href="#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_input" title="scrapy.spidermiddlewares.SpiderMiddleware.process_spider_input"><code class="xref py py-meth docutils literal"><span class="pre">process_spider_input()</span></code></a>
method of each middleware will be invoked in increasing
middleware order (100, 200, 300, ...), and the
<a class="reference internal" href="#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output" title="scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output"><code class="xref py py-meth docutils literal"><span class="pre">process_spider_output()</span></code></a> method
of each middleware will be invoked in decreasing order.</p>
<p>To decide which order to assign to your middleware see the
<a class="reference internal" href="settings.html#std:setting-SPIDER_MIDDLEWARES_BASE"><code class="xref std std-setting docutils literal"><span class="pre">SPIDER_MIDDLEWARES_BASE</span></code></a> setting and pick a value according to where
you want to insert the middleware. The order does matter because each
middleware performs a different action and your middleware could depend on some
previous (or subsequent) middleware being applied.</p>
<p>If you want to disable a builtin middleware (the ones defined in
<a class="reference internal" href="settings.html#std:setting-SPIDER_MIDDLEWARES_BASE"><code class="xref std std-setting docutils literal"><span class="pre">SPIDER_MIDDLEWARES_BASE</span></code></a>, and enabled by default) you must define it
in your project <a class="reference internal" href="settings.html#std:setting-SPIDER_MIDDLEWARES"><code class="xref std std-setting docutils literal"><span class="pre">SPIDER_MIDDLEWARES</span></code></a> setting and assign <cite>None</cite> as its
value.  For example, if you want to disable the off-site middleware:</p>
<p>Finally, keep in mind that some middlewares may need to be enabled through a
particular setting. See each middleware documentation for more info.</p>
<p>Each middleware component is a Python class that defines one or more of the
following methods:</p>
<p>This method is called for each response that goes through the spider
middleware and into the spider, for processing.</p>
<p><a class="reference internal" href="#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_input" title="scrapy.spidermiddlewares.SpiderMiddleware.process_spider_input"><code class="xref py py-meth docutils literal"><span class="pre">process_spider_input()</span></code></a> should return <code class="docutils literal"><span class="pre">None</span></code> or raise an
exception.</p>
<p>If it returns <code class="docutils literal"><span class="pre">None</span></code>, Scrapy will continue processing this response,
executing all other middlewares until, finally, the response is handed
to the spider for processing.</p>
<p>If it raises an exception, Scrapy won’t bother calling any other spider
middleware <a class="reference internal" href="#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_input" title="scrapy.spidermiddlewares.SpiderMiddleware.process_spider_input"><code class="xref py py-meth docutils literal"><span class="pre">process_spider_input()</span></code></a> and will call the request
errback.  The output of the errback is chained back in the other
direction for <a class="reference internal" href="#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output" title="scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output"><code class="xref py py-meth docutils literal"><span class="pre">process_spider_output()</span></code></a> to process it, or
<a class="reference internal" href="#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_exception" title="scrapy.spidermiddlewares.SpiderMiddleware.process_spider_exception"><code class="xref py py-meth docutils literal"><span class="pre">process_spider_exception()</span></code></a> if it raised an exception.</p>
<p>This method is called with the results returned from the Spider, after
it has processed the response.</p>
<p><a class="reference internal" href="#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output" title="scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output"><code class="xref py py-meth docutils literal"><span class="pre">process_spider_output()</span></code></a> must return an iterable of
<a class="reference internal" href="request-response.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal"><span class="pre">Request</span></code></a>, dict or <a class="reference internal" href="items.html#scrapy.item.Item" title="scrapy.item.Item"><code class="xref py py-class docutils literal"><span class="pre">Item</span></code></a>
objects.</p>
<p>This method is called when a spider or <a class="reference internal" href="#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_input" title="scrapy.spidermiddlewares.SpiderMiddleware.process_spider_input"><code class="xref py py-meth docutils literal"><span class="pre">process_spider_input()</span></code></a>
method (from other spider middleware) raises an exception.</p>
<p><a class="reference internal" href="#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_exception" title="scrapy.spidermiddlewares.SpiderMiddleware.process_spider_exception"><code class="xref py py-meth docutils literal"><span class="pre">process_spider_exception()</span></code></a> should return either <code class="docutils literal"><span class="pre">None</span></code> or an
iterable of <a class="reference internal" href="request-response.html#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal"><span class="pre">Response</span></code></a>, dict or
<a class="reference internal" href="items.html#scrapy.item.Item" title="scrapy.item.Item"><code class="xref py py-class docutils literal"><span class="pre">Item</span></code></a> objects.</p>
<p>If it returns <code class="docutils literal"><span class="pre">None</span></code>, Scrapy will continue processing this exception,
executing any other <a class="reference internal" href="#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_exception" title="scrapy.spidermiddlewares.SpiderMiddleware.process_spider_exception"><code class="xref py py-meth docutils literal"><span class="pre">process_spider_exception()</span></code></a> in the following
middleware components, until no middleware components are left and the
exception reaches the engine (where it’s logged and discarded).</p>
<p>If it returns an iterable the <a class="reference internal" href="#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output" title="scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output"><code class="xref py py-meth docutils literal"><span class="pre">process_spider_output()</span></code></a> pipeline
kicks in, and no other <a class="reference internal" href="#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_exception" title="scrapy.spidermiddlewares.SpiderMiddleware.process_spider_exception"><code class="xref py py-meth docutils literal"><span class="pre">process_spider_exception()</span></code></a> will be called.</p>
<p><span class="versionmodified">New in version 0.15.</span></p>
<p>This method is called with the start requests of the spider, and works
similarly to the <a class="reference internal" href="#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output" title="scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output"><code class="xref py py-meth docutils literal"><span class="pre">process_spider_output()</span></code></a> method, except that it
doesn’t have a response associated and must return only requests (not
items).</p>
<p>It receives an iterable (in the <code class="docutils literal"><span class="pre">start_requests</span></code> parameter) and must
return another iterable of <a class="reference internal" href="request-response.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal"><span class="pre">Request</span></code></a> objects.</p>
<p class="first admonition-title">Note</p>
<p class="last">When implementing this method in your spider middleware, you
should always return an iterable (that follows the input one) and
not consume all <code class="docutils literal"><span class="pre">start_requests</span></code> iterator because it can be very
large (or even unbounded) and cause a memory overflow. The Scrapy
engine is designed to pull start requests while it has capacity to
process them, so the start requests iterator can be effectively
endless where there is some other condition for stopping the spider
(like a time limit or item/page count).</p>
<p>This page describes all spider middleware components that come with Scrapy. For
information on how to use them and how to write your own spider middleware, see
the <a class="reference internal" href="#topics-spider-middleware"><span class="std std-ref">spider middleware usage guide</span></a>.</p>
<p>For a list of the components enabled by default (and their orders) see the
<a class="reference internal" href="settings.html#std:setting-SPIDER_MIDDLEWARES_BASE"><code class="xref std std-setting docutils literal"><span class="pre">SPIDER_MIDDLEWARES_BASE</span></code></a> setting.</p>
<p>DepthMiddleware is a scrape middleware used for tracking the depth of each
Request inside the site being scraped. It can be used to limit the maximum
depth to scrape or things like that.</p>
<p>The <a class="reference internal" href="#scrapy.spidermiddlewares.depth.DepthMiddleware" title="scrapy.spidermiddlewares.depth.DepthMiddleware"><code class="xref py py-class docutils literal"><span class="pre">DepthMiddleware</span></code></a> can be configured through the following
settings (see the settings documentation for more info):</p>
<p>Filter out unsuccessful (erroneous) HTTP responses so that spiders don’t
have to deal with them, which (most of the time) imposes an overhead,
consumes more resources, and makes the spider logic more complex.</p>
<p>According to the <a class="reference external" href="https://www.w3.org/Protocols/rfc2616/rfc2616-sec10.html">HTTP standard</a>, successful responses are those whose
status codes are in the 200-300 range.</p>
<p>If you still want to process response codes outside that range, you can
specify which response codes the spider is able to handle using the
<code class="docutils literal"><span class="pre">handle_httpstatus_list</span></code> spider attribute or
<a class="reference internal" href="#std:setting-HTTPERROR_ALLOWED_CODES"><code class="xref std std-setting docutils literal"><span class="pre">HTTPERROR_ALLOWED_CODES</span></code></a> setting.</p>
<p>For example, if you want your spider to handle 404 responses you can do
this:</p>
<p id="std:reqmeta-handle_httpstatus_all">The <code class="docutils literal"><span class="pre">handle_httpstatus_list</span></code> key of <a class="reference internal" href="request-response.html#scrapy.http.Request.meta" title="scrapy.http.Request.meta"><code class="xref py py-attr docutils literal"><span class="pre">Request.meta</span></code></a> can also be used to specify which response codes to
allow on a per-request basis. You can also set the meta key <code class="docutils literal"><span class="pre">handle_httpstatus_all</span></code>
to <code class="docutils literal"><span class="pre">True</span></code> if you want to allow any response code for a request.</p>
<p>Keep in mind, however, that it’s usually a bad idea to handle non-200
responses, unless you really know what you’re doing.</p>
<p>For more information see: <a class="reference external" href="https://www.w3.org/Protocols/rfc2616/rfc2616-sec10.html">HTTP Status Code Definitions</a>.</p>
<p>Default: <code class="docutils literal"><span class="pre">[]</span></code></p>
<p>Pass all responses with non-200 status codes contained in this list.</p>
<p>Default: <code class="docutils literal"><span class="pre">False</span></code></p>
<p>Pass all responses, regardless of its status code.</p>
<p>Filters out Requests for URLs outside the domains covered by the spider.</p>
<p>This middleware filters out every request whose host names aren’t in the
spider’s <a class="reference internal" href="spiders.html#scrapy.spiders.Spider.allowed_domains" title="scrapy.spiders.Spider.allowed_domains"><code class="xref py py-attr docutils literal"><span class="pre">allowed_domains</span></code></a> attribute.
All subdomains of any domain in the list are also allowed.
E.g. the rule <code class="docutils literal"><span class="pre">www.example.org</span></code> will also allow <code class="docutils literal"><span class="pre">bob.www.example.org</span></code>
but not <code class="docutils literal"><span class="pre">www2.example.com</span></code> nor <code class="docutils literal"><span class="pre">example.com</span></code>.</p>
<p>When your spider returns a request for a domain not belonging to those
covered by the spider, this middleware will log a debug message similar to
this one:</p>
<p>To avoid filling the log with too much noise, it will only print one of
these messages for each new domain filtered. So, for example, if another
request for <code class="docutils literal"><span class="pre">www.othersite.com</span></code> is filtered, no log message will be
printed. But if a request for <code class="docutils literal"><span class="pre">someothersite.com</span></code> is filtered, a message
will be printed (but only for the first request filtered).</p>
<p>If the spider doesn’t define an
<a class="reference internal" href="spiders.html#scrapy.spiders.Spider.allowed_domains" title="scrapy.spiders.Spider.allowed_domains"><code class="xref py py-attr docutils literal"><span class="pre">allowed_domains</span></code></a> attribute, or the
attribute is empty, the offsite middleware will allow all requests.</p>
<p>If the request has the <code class="xref py py-attr docutils literal"><span class="pre">dont_filter</span></code> attribute
set, the offsite middleware will allow the request even if its domain is not
listed in allowed domains.</p>
<p>Populates Request <code class="docutils literal"><span class="pre">Referer</span></code> header, based on the URL of the Response which
generated it.</p>
<p><span class="versionmodified">New in version 0.15.</span></p>
<p>Default: <code class="docutils literal"><span class="pre">True</span></code></p>
<p>Whether to enable referer middleware.</p>
<p><span class="versionmodified">New in version 1.4.</span></p>
<p>Default: <code class="docutils literal"><span class="pre">'scrapy.spidermiddlewares.referer.DefaultReferrerPolicy'</span></code></p>
<p id="std:reqmeta-referrer_policy"><a class="reference external" href="https://www.w3.org/TR/referrer-policy">Referrer Policy</a> to apply when populating Request “Referer” header.</p>
<p class="first admonition-title">Note</p>
<p class="last">You can also set the Referrer Policy per request,
using the special <code class="docutils literal"><span class="pre">"referrer_policy"</span></code> <a class="reference internal" href="request-response.html#topics-request-meta"><span class="std std-ref">Request.meta</span></a> key,
with the same acceptable values as for the <code class="docutils literal"><span class="pre">REFERRER_POLICY</span></code> setting.</p>
<p>A variant of “no-referrer-when-downgrade”,
with the addition that “Referer” is not sent if the parent request was
using <code class="docutils literal"><span class="pre">file://</span></code> or <code class="docutils literal"><span class="pre">s3://</span></code> scheme.</p>
<p class="first admonition-title">Warning</p>
<p>Scrapy’s default referrer policy — just like <a class="reference external" href="https://www.w3.org/TR/referrer-policy/#referrer-policy-no-referrer-when-downgrade">“no-referrer-when-downgrade”</a>,
the W3C-recommended value for browsers — will send a non-empty
“Referer” header from any <code class="docutils literal"><span class="pre">http(s)://</span></code> to any <code class="docutils literal"><span class="pre">https://</span></code> URL,
even if the domain is different.</p>
<p class="last"><a class="reference external" href="https://www.w3.org/TR/referrer-policy/#referrer-policy-same-origin">“same-origin”</a> may be a better choice if you want to remove referrer
information for cross-domain requests.</p>
<p><a class="reference external" href="https://www.w3.org/TR/referrer-policy/#referrer-policy-no-referrer">https://www.w3.org/TR/referrer-policy/#referrer-policy-no-referrer</a></p>
<p>The simplest policy is “no-referrer”, which specifies that no referrer information
is to be sent along with requests made from a particular request client to any origin.
The header will be omitted entirely.</p>
<p><a class="reference external" href="https://www.w3.org/TR/referrer-policy/#referrer-policy-no-referrer-when-downgrade">https://www.w3.org/TR/referrer-policy/#referrer-policy-no-referrer-when-downgrade</a></p>
<p>The “no-referrer-when-downgrade” policy sends a full URL along with requests
from a TLS-protected environment settings object to a potentially trustworthy URL,
and requests from clients which are not TLS-protected to any origin.</p>
<p>Requests from TLS-protected clients to non-potentially trustworthy URLs,
on the other hand, will contain no referrer information.
A Referer HTTP header will not be sent.</p>
<p>This is a user agent’s default behavior, if no policy is otherwise specified.</p>
<p class="first admonition-title">Note</p>
<p>“no-referrer-when-downgrade” policy is the W3C-recommended default,
and is used by major web browsers.</p>
<p class="last">However, it is NOT Scrapy’s default referrer policy (see <a class="reference internal" href="#scrapy.spidermiddlewares.referer.DefaultReferrerPolicy" title="scrapy.spidermiddlewares.referer.DefaultReferrerPolicy"><code class="xref py py-class docutils literal"><span class="pre">DefaultReferrerPolicy</span></code></a>).</p>
<p><a class="reference external" href="https://www.w3.org/TR/referrer-policy/#referrer-policy-same-origin">https://www.w3.org/TR/referrer-policy/#referrer-policy-same-origin</a></p>
<p>The “same-origin” policy specifies that a full URL, stripped for use as a referrer,
is sent as referrer information when making same-origin requests from a particular request client.</p>
<p>Cross-origin requests, on the other hand, will contain no referrer information.
A Referer HTTP header will not be sent.</p>
<p><a class="reference external" href="https://www.w3.org/TR/referrer-policy/#referrer-policy-origin">https://www.w3.org/TR/referrer-policy/#referrer-policy-origin</a></p>
<p>The “origin” policy specifies that only the ASCII serialization
of the origin of the request client is sent as referrer information
when making both same-origin requests and cross-origin requests
from a particular request client.</p>
<p><a class="reference external" href="https://www.w3.org/TR/referrer-policy/#referrer-policy-strict-origin">https://www.w3.org/TR/referrer-policy/#referrer-policy-strict-origin</a></p>
<p>The “strict-origin” policy sends the ASCII serialization
of the origin of the request client when making requests:
- from a TLS-protected environment settings object to a potentially trustworthy URL, and
- from non-TLS-protected environment settings objects to any origin.</p>
<p>Requests from TLS-protected request clients to non- potentially trustworthy URLs,
on the other hand, will contain no referrer information.
A Referer HTTP header will not be sent.</p>
<p><a class="reference external" href="https://www.w3.org/TR/referrer-policy/#referrer-policy-origin-when-cross-origin">https://www.w3.org/TR/referrer-policy/#referrer-policy-origin-when-cross-origin</a></p>
<p>The “origin-when-cross-origin” policy specifies that a full URL,
stripped for use as a referrer, is sent as referrer information
when making same-origin requests from a particular request client,
and only the ASCII serialization of the origin of the request client
is sent as referrer information when making cross-origin requests
from a particular request client.</p>
<p><a class="reference external" href="https://www.w3.org/TR/referrer-policy/#referrer-policy-strict-origin-when-cross-origin">https://www.w3.org/TR/referrer-policy/#referrer-policy-strict-origin-when-cross-origin</a></p>
<p>The “strict-origin-when-cross-origin” policy specifies that a full URL,
stripped for use as a referrer, is sent as referrer information
when making same-origin requests from a particular request client,
and only the ASCII serialization of the origin of the request client
when making cross-origin requests:</p>
<p>Requests from TLS-protected clients to non- potentially trustworthy URLs,
on the other hand, will contain no referrer information.
A Referer HTTP header will not be sent.</p>
<p><a class="reference external" href="https://www.w3.org/TR/referrer-policy/#referrer-policy-unsafe-url">https://www.w3.org/TR/referrer-policy/#referrer-policy-unsafe-url</a></p>
<p>The “unsafe-url” policy specifies that a full URL, stripped for use as a referrer,
is sent along with both cross-origin requests
and same-origin requests made from a particular request client.</p>
<p>Note: The policy’s name doesn’t lie; it is unsafe.
This policy will leak origins and paths from TLS-protected resources
to insecure origins.
Carefully consider the impact of setting such a policy for potentially sensitive documents.</p>
<p class="first admonition-title">Warning</p>
<p class="last">“unsafe-url” policy is NOT recommended.</p>
<p>Filters out requests with URLs longer than URLLENGTH_LIMIT</p>
<p>The <a class="reference internal" href="#scrapy.spidermiddlewares.urllength.UrlLengthMiddleware" title="scrapy.spidermiddlewares.urllength.UrlLengthMiddleware"><code class="xref py py-class docutils literal"><span class="pre">UrlLengthMiddleware</span></code></a> can be configured through the following
settings (see the settings documentation for more info):</p>
<p>
        © Copyright 2008-2016, Scrapy developers.
      
        <span class="commit">
          Revision <code>5aebdac4</code>.
        </span>
</p>
<p class="caption"><span class="caption-text">First steps</span></p>
<p class="caption"><span class="caption-text">Basic concepts</span></p>
<p class="caption"><span class="caption-text">Built-in services</span></p>
<p class="caption"><span class="caption-text">Solving specific problems</span></p>
<p class="caption"><span class="caption-text">Extending Scrapy</span></p>
<p class="caption"><span class="caption-text">All the rest</span></p>
<p>The extensions framework provides a mechanism for inserting your own
custom functionality into Scrapy.</p>
<p>Extensions are just regular classes that are instantiated at Scrapy startup,
when extensions are initialized.</p>
<p>Extensions use the <a class="reference internal" href="settings.html#topics-settings"><span class="std std-ref">Scrapy settings</span></a> to manage their
settings, just like any other Scrapy code.</p>
<p>It is customary for extensions to prefix their settings with their own name, to
avoid collision with existing (and future) extensions. For example, a
hypothetic extension to handle <a class="reference external" href="https://en.wikipedia.org/wiki/Sitemaps">Google Sitemaps</a> would use settings like
<cite>GOOGLESITEMAP_ENABLED</cite>, <cite>GOOGLESITEMAP_DEPTH</cite>, and so on.</p>
<p>Extensions are loaded and activated at startup by instantiating a single
instance of the extension class. Therefore, all the extension initialization
code must be performed in the class constructor (<code class="docutils literal"><span class="pre">__init__</span></code> method).</p>
<p>To make an extension available, add it to the <a class="reference internal" href="settings.html#std:setting-EXTENSIONS"><code class="xref std std-setting docutils literal"><span class="pre">EXTENSIONS</span></code></a> setting in
your Scrapy settings. In <a class="reference internal" href="settings.html#std:setting-EXTENSIONS"><code class="xref std std-setting docutils literal"><span class="pre">EXTENSIONS</span></code></a>, each extension is represented
by a string: the full Python path to the extension’s class name. For example:</p>
<p>As you can see, the <a class="reference internal" href="settings.html#std:setting-EXTENSIONS"><code class="xref std std-setting docutils literal"><span class="pre">EXTENSIONS</span></code></a> setting is a dict where the keys are
the extension paths, and their values are the orders, which define the
extension <em>loading</em> order. The <a class="reference internal" href="settings.html#std:setting-EXTENSIONS"><code class="xref std std-setting docutils literal"><span class="pre">EXTENSIONS</span></code></a> setting is merged with the
<a class="reference internal" href="settings.html#std:setting-EXTENSIONS_BASE"><code class="xref std std-setting docutils literal"><span class="pre">EXTENSIONS_BASE</span></code></a> setting defined in Scrapy (and not meant to be
overridden) and then sorted by order to get the final sorted list of enabled
extensions.</p>
<p>As extensions typically do not depend on each other, their loading order is
irrelevant in most cases. This is why the <a class="reference internal" href="settings.html#std:setting-EXTENSIONS_BASE"><code class="xref std std-setting docutils literal"><span class="pre">EXTENSIONS_BASE</span></code></a> setting
defines all extensions with the same order (<code class="docutils literal"><span class="pre">0</span></code>). However, this feature can
be exploited if you need to add an extension which depends on other extensions
already loaded.</p>
<p>Not all available extensions will be enabled. Some of them usually depend on a
particular setting. For example, the HTTP Cache extension is available by default
but disabled unless the <a class="reference internal" href="downloader-middleware.html#std:setting-HTTPCACHE_ENABLED"><code class="xref std std-setting docutils literal"><span class="pre">HTTPCACHE_ENABLED</span></code></a> setting is set.</p>
<p>In order to disable an extension that comes enabled by default (ie. those
included in the <a class="reference internal" href="settings.html#std:setting-EXTENSIONS_BASE"><code class="xref std std-setting docutils literal"><span class="pre">EXTENSIONS_BASE</span></code></a> setting) you must set its order to
<code class="docutils literal"><span class="pre">None</span></code>. For example:</p>
<p>Each extension is a Python class. The main entry point for a Scrapy extension
(this also includes middlewares and pipelines) is the <code class="docutils literal"><span class="pre">from_crawler</span></code>
class method which receives a <code class="docutils literal"><span class="pre">Crawler</span></code> instance. Through the Crawler object
you can access settings, signals, stats, and also control the crawling behaviour.</p>
<p>Typically, extensions connect to <a class="reference internal" href="signals.html#topics-signals"><span class="std std-ref">signals</span></a> and perform
tasks triggered by them.</p>
<p>Finally, if the <code class="docutils literal"><span class="pre">from_crawler</span></code> method raises the
<a class="reference internal" href="exceptions.html#scrapy.exceptions.NotConfigured" title="scrapy.exceptions.NotConfigured"><code class="xref py py-exc docutils literal"><span class="pre">NotConfigured</span></code></a> exception, the extension will be
disabled. Otherwise, the extension will be enabled.</p>
<p>Here we will implement a simple extension to illustrate the concepts described
in the previous section. This extension will log a message every time:</p>
<p>The extension will be enabled through the <code class="docutils literal"><span class="pre">MYEXT_ENABLED</span></code> setting and the
number of items will be specified through the <code class="docutils literal"><span class="pre">MYEXT_ITEMCOUNT</span></code> setting.</p>
<p>Here is the code of such extension:</p>
<p>Log basic stats like crawled pages and scraped items.</p>
<p>Enable the collection of core statistics, provided the stats collection is
enabled (see <a class="reference internal" href="stats.html#topics-stats"><span class="std std-ref">Stats Collection</span></a>).</p>
<p>Provides a telnet console for getting into a Python interpreter inside the
currently running Scrapy process, which can be very useful for debugging.</p>
<p>The telnet console must be enabled by the <a class="reference internal" href="settings.html#std:setting-TELNETCONSOLE_ENABLED"><code class="xref std std-setting docutils literal"><span class="pre">TELNETCONSOLE_ENABLED</span></code></a>
setting, and the server will listen in the port specified in
<a class="reference internal" href="telnetconsole.html#std:setting-TELNETCONSOLE_PORT"><code class="xref std std-setting docutils literal"><span class="pre">TELNETCONSOLE_PORT</span></code></a>.</p>
<p class="first admonition-title">Note</p>
<p class="last">This extension does not work in Windows.</p>
<p>Monitors the memory used by the Scrapy process that runs the spider and:</p>
<p>The notification e-mails can be triggered when a certain warning value is
reached (<a class="reference internal" href="settings.html#std:setting-MEMUSAGE_WARNING_MB"><code class="xref std std-setting docutils literal"><span class="pre">MEMUSAGE_WARNING_MB</span></code></a>) and when the maximum value is reached
(<a class="reference internal" href="settings.html#std:setting-MEMUSAGE_LIMIT_MB"><code class="xref std std-setting docutils literal"><span class="pre">MEMUSAGE_LIMIT_MB</span></code></a>) which will also cause the spider to be closed
and the Scrapy process to be terminated.</p>
<p>This extension is enabled by the <a class="reference internal" href="settings.html#std:setting-MEMUSAGE_ENABLED"><code class="xref std std-setting docutils literal"><span class="pre">MEMUSAGE_ENABLED</span></code></a> setting and
can be configured with the following settings:</p>
<p>An extension for debugging memory usage. It collects information about:</p>
<p>To enable this extension, turn on the <a class="reference internal" href="settings.html#std:setting-MEMDEBUG_ENABLED"><code class="xref std std-setting docutils literal"><span class="pre">MEMDEBUG_ENABLED</span></code></a> setting. The
info will be stored in the stats.</p>
<p>Closes a spider automatically when some conditions are met, using a specific
closing reason for each condition.</p>
<p>The conditions for closing a spider can be configured through the following
settings:</p>
<p>Default: <code class="docutils literal"><span class="pre">0</span></code></p>
<p>An integer which specifies a number of seconds. If the spider remains open for
more than that number of second, it will be automatically closed with the
reason <code class="docutils literal"><span class="pre">closespider_timeout</span></code>. If zero (or non set), spiders won’t be closed by
timeout.</p>
<p>Default: <code class="docutils literal"><span class="pre">0</span></code></p>
<p>An integer which specifies a number of items. If the spider scrapes more than
that amount and those items are passed by the item pipeline, the
spider will be closed with the reason <code class="docutils literal"><span class="pre">closespider_itemcount</span></code>.
Requests which  are currently in the downloader queue (up to
<a class="reference internal" href="settings.html#std:setting-CONCURRENT_REQUESTS"><code class="xref std std-setting docutils literal"><span class="pre">CONCURRENT_REQUESTS</span></code></a> requests) are still processed.
If zero (or non set), spiders won’t be closed by number of passed items.</p>
<p><span class="versionmodified">New in version 0.11.</span></p>
<p>Default: <code class="docutils literal"><span class="pre">0</span></code></p>
<p>An integer which specifies the maximum number of responses to crawl. If the spider
crawls more than that, the spider will be closed with the reason
<code class="docutils literal"><span class="pre">closespider_pagecount</span></code>. If zero (or non set), spiders won’t be closed by
number of crawled responses.</p>
<p><span class="versionmodified">New in version 0.11.</span></p>
<p>Default: <code class="docutils literal"><span class="pre">0</span></code></p>
<p>An integer which specifies the maximum number of errors to receive before
closing the spider. If the spider generates more than that number of errors,
it will be closed with the reason <code class="docutils literal"><span class="pre">closespider_errorcount</span></code>. If zero (or non
set), spiders won’t be closed by number of errors.</p>
<p>This simple extension can be used to send a notification e-mail every time a
domain has finished scraping, including the Scrapy stats collected. The email
will be sent to all recipients specified in the <a class="reference internal" href="settings.html#std:setting-STATSMAILER_RCPTS"><code class="xref std std-setting docutils literal"><span class="pre">STATSMAILER_RCPTS</span></code></a>
setting.</p>
<p>Dumps information about the running process when a <a class="reference external" href="https://en.wikipedia.org/wiki/SIGQUIT">SIGQUIT</a> or <a class="reference external" href="https://en.wikipedia.org/wiki/SIGUSR1_and_SIGUSR2">SIGUSR2</a>
signal is received. The information dumped is the following:</p>
<p>After the stack trace and engine status is dumped, the Scrapy process continues
running normally.</p>
<p>This extension only works on POSIX-compliant platforms (ie. not Windows),
because the <a class="reference external" href="https://en.wikipedia.org/wiki/SIGQUIT">SIGQUIT</a> and <a class="reference external" href="https://en.wikipedia.org/wiki/SIGUSR1_and_SIGUSR2">SIGUSR2</a> signals are not available on Windows.</p>
<p>There are at least two ways to send Scrapy the <a class="reference external" href="https://en.wikipedia.org/wiki/SIGQUIT">SIGQUIT</a> signal:</p>
<p class="first">By pressing Ctrl-while a Scrapy process is running (Linux only?)</p>
<p class="first">By running this command (assuming <code class="docutils literal"><span class="pre">&lt;pid&gt;</span></code> is the process id of the Scrapy
process):</p>
<p>Invokes a <a class="reference external" href="https://docs.python.org/2/library/pdb.html">Python debugger</a> inside a running Scrapy process when a <a class="reference external" href="https://en.wikipedia.org/wiki/SIGUSR1_and_SIGUSR2">SIGUSR2</a>
signal is received. After the debugger is exited, the Scrapy process continues
running normally.</p>
<p>For more info see <cite>Debugging in Python</cite>.</p>
<p>This extension only works on POSIX-compliant platforms (ie. not Windows).</p>
<p>
        © Copyright 2008-2016, Scrapy developers.
      
        <span class="commit">
          Revision <code>5aebdac4</code>.
        </span>
</p>
<p class="caption"><span class="caption-text">First steps</span></p>
<p class="caption"><span class="caption-text">Basic concepts</span></p>
<p class="caption"><span class="caption-text">Built-in services</span></p>
<p class="caption"><span class="caption-text">Solving specific problems</span></p>
<p class="caption"><span class="caption-text">Extending Scrapy</span></p>
<p class="caption"><span class="caption-text">All the rest</span></p>
<p><span class="versionmodified">New in version 0.15.</span></p>
<p>This section documents the Scrapy core API, and it’s intended for developers of
extensions and middlewares.</p>
<p>The main entry point to Scrapy API is the <a class="reference internal" href="#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler"><code class="xref py py-class docutils literal"><span class="pre">Crawler</span></code></a>
object, passed to extensions through the <code class="docutils literal"><span class="pre">from_crawler</span></code> class method. This
object provides access to all Scrapy core components, and it’s the only way for
extensions to access them and hook their functionality into Scrapy.</p>
<p>The Extension Manager is responsible for loading and keeping track of installed
extensions and it’s configured through the <a class="reference internal" href="settings.html#std:setting-EXTENSIONS"><code class="xref std std-setting docutils literal"><span class="pre">EXTENSIONS</span></code></a> setting which
contains a dictionary of all available extensions and their order similar to
how you <a class="reference internal" href="downloader-middleware.html#topics-downloader-middleware-setting"><span class="std std-ref">configure the downloader middlewares</span></a>.</p>
<p>The Crawler object must be instantiated with a
<a class="reference internal" href="spiders.html#scrapy.spiders.Spider" title="scrapy.spiders.Spider"><code class="xref py py-class docutils literal"><span class="pre">scrapy.spiders.Spider</span></code></a> subclass and a
<a class="reference internal" href="#scrapy.settings.Settings" title="scrapy.settings.Settings"><code class="xref py py-class docutils literal"><span class="pre">scrapy.settings.Settings</span></code></a> object.</p>
<p>The settings manager of this crawler.</p>
<p>This is used by extensions &amp; middlewares to access the Scrapy settings
of this crawler.</p>
<p>For an introduction on Scrapy settings see <a class="reference internal" href="settings.html#topics-settings"><span class="std std-ref">Settings</span></a>.</p>
<p>For the API see <a class="reference internal" href="#scrapy.settings.Settings" title="scrapy.settings.Settings"><code class="xref py py-class docutils literal"><span class="pre">Settings</span></code></a> class.</p>
<p>The signals manager of this crawler.</p>
<p>This is used by extensions &amp; middlewares to hook themselves into Scrapy
functionality.</p>
<p>For an introduction on signals see <a class="reference internal" href="signals.html#topics-signals"><span class="std std-ref">Signals</span></a>.</p>
<p>For the API see <a class="reference internal" href="#scrapy.signalmanager.SignalManager" title="scrapy.signalmanager.SignalManager"><code class="xref py py-class docutils literal"><span class="pre">SignalManager</span></code></a> class.</p>
<p>The stats collector of this crawler.</p>
<p>This is used from extensions &amp; middlewares to record stats of their
behaviour, or access stats collected by other extensions.</p>
<p>For an introduction on stats collection see <a class="reference internal" href="stats.html#topics-stats"><span class="std std-ref">Stats Collection</span></a>.</p>
<p>For the API see <a class="reference internal" href="#scrapy.statscollectors.StatsCollector" title="scrapy.statscollectors.StatsCollector"><code class="xref py py-class docutils literal"><span class="pre">StatsCollector</span></code></a> class.</p>
<p>The extension manager that keeps track of enabled extensions.</p>
<p>Most extensions won’t need to access this attribute.</p>
<p>For an introduction on extensions and a list of available extensions on
Scrapy see <a class="reference internal" href="extensions.html#topics-extensions"><span class="std std-ref">Extensions</span></a>.</p>
<p>The execution engine, which coordinates the core crawling logic
between the scheduler, downloader and spiders.</p>
<p>Some extension may want to access the Scrapy engine, to inspect  or
modify the downloader and scheduler behaviour, although this is an
advanced use and this API is not yet stable.</p>
<p>Spider currently being crawled. This is an instance of the spider class
provided while constructing the crawler, and it is created after the
arguments given in the <a class="reference internal" href="#scrapy.crawler.Crawler.crawl" title="scrapy.crawler.Crawler.crawl"><code class="xref py py-meth docutils literal"><span class="pre">crawl()</span></code></a> method.</p>
<p>Starts the crawler by instantiating its spider class with the given
<cite>args</cite> and <cite>kwargs</cite> arguments, while setting the execution engine in
motion.</p>
<p>Returns a deferred that is fired when the crawl is finished.</p>
<p>This is a convenient helper class that keeps track of, manages and runs
crawlers inside an already setup Twisted <a class="reference external" href="https://twistedmatrix.com/documents/current/core/howto/reactor-basics.html">reactor</a>.</p>
<p>The CrawlerRunner object must be instantiated with a
<a class="reference internal" href="#scrapy.settings.Settings" title="scrapy.settings.Settings"><code class="xref py py-class docutils literal"><span class="pre">Settings</span></code></a> object.</p>
<p>This class shouldn’t be needed (since Scrapy is responsible of using it
accordingly) unless writing scripts that manually handle the crawling
process. See <a class="reference internal" href="practices.html#run-from-script"><span class="std std-ref">Run Scrapy from a script</span></a> for an example.</p>
<p>Run a crawler with the provided arguments.</p>
<p>It will call the given Crawler’s <a class="reference internal" href="#scrapy.crawler.Crawler.crawl" title="scrapy.crawler.Crawler.crawl"><code class="xref py py-meth docutils literal"><span class="pre">crawl()</span></code></a> method, while
keeping track of it so it can be stopped later.</p>
<p>If <cite>crawler_or_spidercls</cite> isn’t a <a class="reference internal" href="#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler"><code class="xref py py-class docutils literal"><span class="pre">Crawler</span></code></a>
instance, this method will try to create one using this parameter as
the spider class given to it.</p>
<p>Returns a deferred that is fired when the crawling is finished.</p>
<p>Set of <a class="reference internal" href="#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler"><code class="xref py py-class docutils literal"><span class="pre">crawlers</span></code></a> started by <a class="reference internal" href="#scrapy.crawler.CrawlerRunner.crawl" title="scrapy.crawler.CrawlerRunner.crawl"><code class="xref py py-meth docutils literal"><span class="pre">crawl()</span></code></a> and managed by this class.</p>
<p>Return a <a class="reference internal" href="#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler"><code class="xref py py-class docutils literal"><span class="pre">Crawler</span></code></a> object.</p>
<p>Returns a deferred that is fired when all managed <a class="reference internal" href="#scrapy.crawler.CrawlerRunner.crawlers" title="scrapy.crawler.CrawlerRunner.crawlers"><code class="xref py py-attr docutils literal"><span class="pre">crawlers</span></code></a> have
completed their executions.</p>
<p>Stops simultaneously all the crawling jobs taking place.</p>
<p>Returns a deferred that is fired when they all have ended.</p>
<p>Bases: <a class="reference internal" href="#scrapy.crawler.CrawlerRunner" title="scrapy.crawler.CrawlerRunner"><code class="xref py py-class docutils literal"><span class="pre">scrapy.crawler.CrawlerRunner</span></code></a></p>
<p>A class to run multiple scrapy crawlers in a process simultaneously.</p>
<p>This class extends <a class="reference internal" href="#scrapy.crawler.CrawlerRunner" title="scrapy.crawler.CrawlerRunner"><code class="xref py py-class docutils literal"><span class="pre">CrawlerRunner</span></code></a> by adding support
for starting a Twisted <a class="reference external" href="https://twistedmatrix.com/documents/current/core/howto/reactor-basics.html">reactor</a> and handling shutdown signals, like the
keyboard interrupt command Ctrl-C. It also configures top-level logging.</p>
<p>This utility should be a better fit than
<a class="reference internal" href="#scrapy.crawler.CrawlerRunner" title="scrapy.crawler.CrawlerRunner"><code class="xref py py-class docutils literal"><span class="pre">CrawlerRunner</span></code></a> if you aren’t running another
Twisted <a class="reference external" href="https://twistedmatrix.com/documents/current/core/howto/reactor-basics.html">reactor</a> within your application.</p>
<p>The CrawlerProcess object must be instantiated with a
<a class="reference internal" href="#scrapy.settings.Settings" title="scrapy.settings.Settings"><code class="xref py py-class docutils literal"><span class="pre">Settings</span></code></a> object.</p>
<p>This class shouldn’t be needed (since Scrapy is responsible of using it
accordingly) unless writing scripts that manually handle the crawling
process. See <a class="reference internal" href="practices.html#run-from-script"><span class="std std-ref">Run Scrapy from a script</span></a> for an example.</p>
<p>Run a crawler with the provided arguments.</p>
<p>It will call the given Crawler’s <a class="reference internal" href="#scrapy.crawler.Crawler.crawl" title="scrapy.crawler.Crawler.crawl"><code class="xref py py-meth docutils literal"><span class="pre">crawl()</span></code></a> method, while
keeping track of it so it can be stopped later.</p>
<p>If <cite>crawler_or_spidercls</cite> isn’t a <a class="reference internal" href="#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler"><code class="xref py py-class docutils literal"><span class="pre">Crawler</span></code></a>
instance, this method will try to create one using this parameter as
the spider class given to it.</p>
<p>Returns a deferred that is fired when the crawling is finished.</p>
<p>Set of <a class="reference internal" href="#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler"><code class="xref py py-class docutils literal"><span class="pre">crawlers</span></code></a> started by <a class="reference internal" href="#scrapy.crawler.CrawlerProcess.crawl" title="scrapy.crawler.CrawlerProcess.crawl"><code class="xref py py-meth docutils literal"><span class="pre">crawl()</span></code></a> and managed by this class.</p>
<p>Return a <a class="reference internal" href="#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler"><code class="xref py py-class docutils literal"><span class="pre">Crawler</span></code></a> object.</p>
<p>Returns a deferred that is fired when all managed <a class="reference internal" href="#scrapy.crawler.CrawlerProcess.crawlers" title="scrapy.crawler.CrawlerProcess.crawlers"><code class="xref py py-attr docutils literal"><span class="pre">crawlers</span></code></a> have
completed their executions.</p>
<p>This method starts a Twisted <a class="reference external" href="https://twistedmatrix.com/documents/current/core/howto/reactor-basics.html">reactor</a>, adjusts its pool size to
<a class="reference internal" href="settings.html#std:setting-REACTOR_THREADPOOL_MAXSIZE"><code class="xref std std-setting docutils literal"><span class="pre">REACTOR_THREADPOOL_MAXSIZE</span></code></a>, and installs a DNS cache based
on <a class="reference internal" href="settings.html#std:setting-DNSCACHE_ENABLED"><code class="xref std std-setting docutils literal"><span class="pre">DNSCACHE_ENABLED</span></code></a> and <a class="reference internal" href="settings.html#std:setting-DNSCACHE_SIZE"><code class="xref std std-setting docutils literal"><span class="pre">DNSCACHE_SIZE</span></code></a>.</p>
<p>If <cite>stop_after_crawl</cite> is True, the reactor will be stopped after all
crawlers have finished, using <a class="reference internal" href="#scrapy.crawler.CrawlerProcess.join" title="scrapy.crawler.CrawlerProcess.join"><code class="xref py py-meth docutils literal"><span class="pre">join()</span></code></a>.</p>
<p>Stops simultaneously all the crawling jobs taking place.</p>
<p>Returns a deferred that is fired when they all have ended.</p>
<p>Dictionary that sets the key name and priority level of the default
settings priorities used in Scrapy.</p>
<p>Each item defines a settings entry point, giving it a code name for
identification and an integer priority. Greater priorities take more
precedence over lesser ones when setting and retrieving values in the
<a class="reference internal" href="#scrapy.settings.Settings" title="scrapy.settings.Settings"><code class="xref py py-class docutils literal"><span class="pre">Settings</span></code></a> class.</p>
<p>For a detailed explanation on each settings sources, see:
<a class="reference internal" href="settings.html#topics-settings"><span class="std std-ref">Settings</span></a>.</p>
<p>Small helper function that looks up a given string priority in the
<a class="reference internal" href="#scrapy.settings.SETTINGS_PRIORITIES" title="scrapy.settings.SETTINGS_PRIORITIES"><code class="xref py py-attr docutils literal"><span class="pre">SETTINGS_PRIORITIES</span></code></a> dictionary and returns its
numerical value, or directly returns a given numerical priority.</p>
<p>Bases: <a class="reference internal" href="#scrapy.settings.BaseSettings" title="scrapy.settings.BaseSettings"><code class="xref py py-class docutils literal"><span class="pre">scrapy.settings.BaseSettings</span></code></a></p>
<p>This object stores Scrapy settings for the configuration of internal
components, and can be used for any further customization.</p>
<p>It is a direct subclass and supports all methods of
<a class="reference internal" href="#scrapy.settings.BaseSettings" title="scrapy.settings.BaseSettings"><code class="xref py py-class docutils literal"><span class="pre">BaseSettings</span></code></a>. Additionally, after instantiation
of this class, the new object will have the global default settings
described on <a class="reference internal" href="settings.html#topics-settings-ref"><span class="std std-ref">Built-in settings reference</span></a> already populated.</p>
<p>Instances of this class behave like dictionaries, but store priorities
along with their <code class="docutils literal"><span class="pre">(key,</span> <span class="pre">value)</span></code> pairs, and can be frozen (i.e. marked
immutable).</p>
<p>Key-value entries can be passed on initialization with the <code class="docutils literal"><span class="pre">values</span></code>
argument, and they would take the <code class="docutils literal"><span class="pre">priority</span></code> level (unless <code class="docutils literal"><span class="pre">values</span></code> is
already an instance of <a class="reference internal" href="#scrapy.settings.BaseSettings" title="scrapy.settings.BaseSettings"><code class="xref py py-class docutils literal"><span class="pre">BaseSettings</span></code></a>, in which
case the existing priority levels will be kept).  If the <code class="docutils literal"><span class="pre">priority</span></code>
argument is a string, the priority name will be looked up in
<a class="reference internal" href="#scrapy.settings.SETTINGS_PRIORITIES" title="scrapy.settings.SETTINGS_PRIORITIES"><code class="xref py py-attr docutils literal"><span class="pre">SETTINGS_PRIORITIES</span></code></a>. Otherwise, a specific integer
should be provided.</p>
<p>Once the object is created, new settings can be loaded or updated with the
<a class="reference internal" href="#scrapy.settings.BaseSettings.set" title="scrapy.settings.BaseSettings.set"><code class="xref py py-meth docutils literal"><span class="pre">set()</span></code></a> method, and can be accessed with
the square bracket notation of dictionaries, or with the
<a class="reference internal" href="#scrapy.settings.BaseSettings.get" title="scrapy.settings.BaseSettings.get"><code class="xref py py-meth docutils literal"><span class="pre">get()</span></code></a> method of the instance and its
value conversion variants. When requesting a stored key, the value with the
highest priority will be retrieved.</p>
<p>Make a deep copy of current settings.</p>
<p>This method returns a new instance of the <a class="reference internal" href="#scrapy.settings.Settings" title="scrapy.settings.Settings"><code class="xref py py-class docutils literal"><span class="pre">Settings</span></code></a> class,
populated with the same values and their priorities.</p>
<p>Modifications to the new object won’t be reflected on the original
settings.</p>
<p>Make a copy of current settings and convert to a dict.</p>
<p>This method returns a new dict populated with the same values
and their priorities as the current settings.</p>
<p>Modifications to the returned dict won’t be reflected on the original
settings.</p>
<p>This method can be useful for example for printing settings
in Scrapy shell.</p>
<p>Disable further changes to the current settings.</p>
<p>After calling this method, the present state of the settings will become
immutable. Trying to change values through the <a class="reference internal" href="#scrapy.settings.BaseSettings.set" title="scrapy.settings.BaseSettings.set"><code class="xref py py-meth docutils literal"><span class="pre">set()</span></code></a> method and
its variants won’t be possible and will be alerted.</p>
<p>Return an immutable copy of the current settings.</p>
<p>Alias for a <a class="reference internal" href="#scrapy.settings.BaseSettings.freeze" title="scrapy.settings.BaseSettings.freeze"><code class="xref py py-meth docutils literal"><span class="pre">freeze()</span></code></a> call in the object returned by <a class="reference internal" href="#scrapy.settings.BaseSettings.copy" title="scrapy.settings.BaseSettings.copy"><code class="xref py py-meth docutils literal"><span class="pre">copy()</span></code></a>.</p>
<p>Get a setting value without affecting its original type.</p>
<p>Get a setting value as a boolean.</p>
<p><code class="docutils literal"><span class="pre">1</span></code>, <code class="docutils literal"><span class="pre">'1'</span></code>, <cite>True`</cite> and <code class="docutils literal"><span class="pre">'True'</span></code> return <code class="docutils literal"><span class="pre">True</span></code>,
while <code class="docutils literal"><span class="pre">0</span></code>, <code class="docutils literal"><span class="pre">'0'</span></code>, <code class="docutils literal"><span class="pre">False</span></code>, <code class="docutils literal"><span class="pre">'False'</span></code> and <code class="docutils literal"><span class="pre">None</span></code> return <code class="docutils literal"><span class="pre">False</span></code>.</p>
<p>For example, settings populated through environment variables set to
<code class="docutils literal"><span class="pre">'0'</span></code> will return <code class="docutils literal"><span class="pre">False</span></code> when using this method.</p>
<p>Get a setting value as a dictionary. If the setting original type is a
dictionary, a copy of it will be returned. If it is a string it will be
evaluated as a JSON dictionary. In the case that it is a
<a class="reference internal" href="#scrapy.settings.BaseSettings" title="scrapy.settings.BaseSettings"><code class="xref py py-class docutils literal"><span class="pre">BaseSettings</span></code></a> instance itself, it will be
converted to a dictionary, containing all its current settings values
as they would be returned by <a class="reference internal" href="#scrapy.settings.BaseSettings.get" title="scrapy.settings.BaseSettings.get"><code class="xref py py-meth docutils literal"><span class="pre">get()</span></code></a>,
and losing all information about priority and mutability.</p>
<p>Get a setting value as a float.</p>
<p>Get a setting value as an int.</p>
<p>Get a setting value as a list. If the setting original type is a list, a
copy of it will be returned. If it’s a string it will be split by ”,”.</p>
<p>For example, settings populated through environment variables set to
<code class="docutils literal"><span class="pre">'one,two'</span></code> will return a list [‘one’, ‘two’] when using this method.</p>
<p>Return the current numerical priority value of a setting, or <code class="docutils literal"><span class="pre">None</span></code> if
the given <code class="docutils literal"><span class="pre">name</span></code> does not exist.</p>
<p>Get a composition of a dictionary-like setting and its <cite>_BASE</cite>
counterpart.</p>
<p>Return the numerical value of the highest priority present throughout
all settings, or the numerical value for <code class="docutils literal"><span class="pre">default</span></code> from
<a class="reference internal" href="#scrapy.settings.SETTINGS_PRIORITIES" title="scrapy.settings.SETTINGS_PRIORITIES"><code class="xref py py-attr docutils literal"><span class="pre">SETTINGS_PRIORITIES</span></code></a> if there are no settings
stored.</p>
<p>Store a key/value attribute with a given priority.</p>
<p>Settings should be populated <em>before</em> configuring the Crawler object
(through the <code class="xref py py-meth docutils literal"><span class="pre">configure()</span></code> method),
otherwise they won’t have any effect.</p>
<p>Store settings from a module with a given priority.</p>
<p>This is a helper function that calls
<a class="reference internal" href="#scrapy.settings.BaseSettings.set" title="scrapy.settings.BaseSettings.set"><code class="xref py py-meth docutils literal"><span class="pre">set()</span></code></a> for every globally declared
uppercase variable of <code class="docutils literal"><span class="pre">module</span></code> with the provided <code class="docutils literal"><span class="pre">priority</span></code>.</p>
<p>Store key/value pairs with a given priority.</p>
<p>This is a helper function that calls
<a class="reference internal" href="#scrapy.settings.BaseSettings.set" title="scrapy.settings.BaseSettings.set"><code class="xref py py-meth docutils literal"><span class="pre">set()</span></code></a> for every item of <code class="docutils literal"><span class="pre">values</span></code>
with the provided <code class="docutils literal"><span class="pre">priority</span></code>.</p>
<p>If <code class="docutils literal"><span class="pre">values</span></code> is a string, it is assumed to be JSON-encoded and parsed
into a dict with <code class="docutils literal"><span class="pre">json.loads()</span></code> first. If it is a
<a class="reference internal" href="#scrapy.settings.BaseSettings" title="scrapy.settings.BaseSettings"><code class="xref py py-class docutils literal"><span class="pre">BaseSettings</span></code></a> instance, the per-key priorities
will be used and the <code class="docutils literal"><span class="pre">priority</span></code> parameter ignored. This allows
inserting/updating settings with different priorities with a single
command.</p>
<p>This class is in charge of retrieving and handling the spider classes
defined across the project.</p>
<p>Custom spider loaders can be employed by specifying their path in the
<a class="reference internal" href="settings.html#std:setting-SPIDER_LOADER_CLASS"><code class="xref std std-setting docutils literal"><span class="pre">SPIDER_LOADER_CLASS</span></code></a> project setting. They must fully implement
the <code class="xref py py-class docutils literal"><span class="pre">scrapy.interfaces.ISpiderLoader</span></code> interface to guarantee an
errorless execution.</p>
<p>This class method is used by Scrapy to create an instance of the class.
It’s called with the current project settings, and it loads the spiders
found recursively in the modules of the <a class="reference internal" href="settings.html#std:setting-SPIDER_MODULES"><code class="xref std std-setting docutils literal"><span class="pre">SPIDER_MODULES</span></code></a>
setting.</p>
<p>Get the Spider class with the given name. It’ll look into the previously
loaded spiders for a spider class with name <cite>spider_name</cite> and will raise
a KeyError if not found.</p>
<p>Get the names of the available spiders in the project.</p>
<p>List the spiders’ names that can handle the given request. Will try to
match the request’s url against the domains of the spiders.</p>
<p>Connect a receiver function to a signal.</p>
<p>The signal can be any object, although Scrapy comes with some
predefined signals that are documented in the <a class="reference internal" href="signals.html#topics-signals"><span class="std std-ref">Signals</span></a>
section.</p>
<p>Disconnect a receiver function from a signal. This has the
opposite effect of the <a class="reference internal" href="#scrapy.signalmanager.SignalManager.connect" title="scrapy.signalmanager.SignalManager.connect"><code class="xref py py-meth docutils literal"><span class="pre">connect()</span></code></a> method, and the arguments
are the same.</p>
<p>Disconnect all receivers from the given signal.</p>
<p>Send a signal, catch exceptions and log them.</p>
<p>The keyword arguments are passed to the signal handlers (connected
through the <a class="reference internal" href="#scrapy.signalmanager.SignalManager.connect" title="scrapy.signalmanager.SignalManager.connect"><code class="xref py py-meth docutils literal"><span class="pre">connect()</span></code></a> method).</p>
<p>Like <a class="reference internal" href="#scrapy.signalmanager.SignalManager.send_catch_log" title="scrapy.signalmanager.SignalManager.send_catch_log"><code class="xref py py-meth docutils literal"><span class="pre">send_catch_log()</span></code></a> but supports returning <a class="reference external" href="http://twistedmatrix.com/documents/current/core/howto/defer.html">deferreds</a> from
signal handlers.</p>
<p>Returns a Deferred that gets fired once all signal handlers
deferreds were fired. Send a signal, catch exceptions and log them.</p>
<p>The keyword arguments are passed to the signal handlers (connected
through the <a class="reference internal" href="#scrapy.signalmanager.SignalManager.connect" title="scrapy.signalmanager.SignalManager.connect"><code class="xref py py-meth docutils literal"><span class="pre">connect()</span></code></a> method).</p>
<p>There are several Stats Collectors available under the
<a class="reference internal" href="stats.html#module-scrapy.statscollectors" title="scrapy.statscollectors: Stats Collectors"><code class="xref py py-mod docutils literal"><span class="pre">scrapy.statscollectors</span></code></a> module and they all implement the Stats
Collector API defined by the <a class="reference internal" href="#scrapy.statscollectors.StatsCollector" title="scrapy.statscollectors.StatsCollector"><code class="xref py py-class docutils literal"><span class="pre">StatsCollector</span></code></a>
class (which they all inherit from).</p>
<p>Return the value for the given stats key or default if it doesn’t exist.</p>
<p>Get all stats from the currently running spider as a dict.</p>
<p>Set the given value for the given stats key.</p>
<p>Override the current stats with the dict passed in <code class="docutils literal"><span class="pre">stats</span></code> argument.</p>
<p>Increment the value of the given stats key, by the given count,
assuming the start value given (when it’s not set).</p>
<p>Set the given value for the given key only if current value for the
same key is lower than value. If there is no current value for the
given key, the value is always set.</p>
<p>Set the given value for the given key only if current value for the
same key is greater than value. If there is no current value for the
given key, the value is always set.</p>
<p>Clear all stats.</p>
<p>The following methods are not part of the stats collection api but instead
used when implementing custom stats collectors:</p>
<p>Open the given spider for stats collection.</p>
<p>Close the given spider. After this is called, no more specific stats
can be accessed or collected.</p>
<p>
        © Copyright 2008-2016, Scrapy developers.
      
        <span class="commit">
          Revision <code>5aebdac4</code>.
        </span>
</p>
<p class="caption"><span class="caption-text">First steps</span></p>
<p class="caption"><span class="caption-text">Basic concepts</span></p>
<p class="caption"><span class="caption-text">Built-in services</span></p>
<p class="caption"><span class="caption-text">Solving specific problems</span></p>
<p class="caption"><span class="caption-text">Extending Scrapy</span></p>
<p class="caption"><span class="caption-text">All the rest</span></p>
<p>Scrapy uses signals extensively to notify when certain events occur. You can
catch some of those signals in your Scrapy project (using an <a class="reference internal" href="extensions.html#topics-extensions"><span class="std std-ref">extension</span></a>, for example) to perform additional tasks or extend Scrapy
to add functionality not provided out of the box.</p>
<p>Even though signals provide several arguments, the handlers that catch them
don’t need to accept all of them - the signal dispatching mechanism will only
deliver the arguments that the handler receives.</p>
<p>You can connect to signals (or send your own) through the
<a class="reference internal" href="api.html#topics-api-signals"><span class="std std-ref">Signals API</span></a>.</p>
<p>Here is a simple example showing how you can catch signals and perform some action:</p>
<p>Some signals support returning <a class="reference external" href="https://twistedmatrix.com/documents/current/core/howto/defer.html">Twisted deferreds</a> from their handlers, see
the <a class="reference internal" href="#topics-signals-ref"><span class="std std-ref">Built-in signals reference</span></a> below to know which ones.</p>
<p>Here’s the list of Scrapy built-in signals and their meaning.</p>
<p>Sent when the Scrapy engine has started crawling.</p>
<p>This signal supports returning deferreds from their handlers.</p>
<p class="first admonition-title">Note</p>
<p class="last">This signal may be fired <em>after</em> the <a class="reference internal" href="#std:signal-spider_opened"><code class="xref std std-signal docutils literal"><span class="pre">spider_opened</span></code></a> signal,
depending on how the spider was started. So <strong>don’t</strong> rely on this signal
getting fired before <a class="reference internal" href="#std:signal-spider_opened"><code class="xref std std-signal docutils literal"><span class="pre">spider_opened</span></code></a>.</p>
<p>Sent when the Scrapy engine is stopped (for example, when a crawling
process has finished).</p>
<p>This signal supports returning deferreds from their handlers.</p>
<p>Sent when an item has been scraped, after it has passed all the
<a class="reference internal" href="item-pipeline.html#topics-item-pipeline"><span class="std std-ref">Item Pipeline</span></a> stages (without being dropped).</p>
<p>This signal supports returning deferreds from their handlers.</p>
<p>Sent after an item has been dropped from the <a class="reference internal" href="item-pipeline.html#topics-item-pipeline"><span class="std std-ref">Item Pipeline</span></a>
when some stage raised a <a class="reference internal" href="exceptions.html#scrapy.exceptions.DropItem" title="scrapy.exceptions.DropItem"><code class="xref py py-exc docutils literal"><span class="pre">DropItem</span></code></a> exception.</p>
<p>This signal supports returning deferreds from their handlers.</p>
<p>Sent after a spider has been closed. This can be used to release per-spider
resources reserved on <a class="reference internal" href="#std:signal-spider_opened"><code class="xref std std-signal docutils literal"><span class="pre">spider_opened</span></code></a>.</p>
<p>This signal supports returning deferreds from their handlers.</p>
<p>Sent after a spider has been opened for crawling. This is typically used to
reserve per-spider resources, but can be used for any task that needs to be
performed when a spider is opened.</p>
<p>This signal supports returning deferreds from their handlers.</p>
<p>Sent when a spider has gone idle, which means the spider has no further:</p>
<p>If the idle state persists after all handlers of this signal have finished,
the engine starts closing the spider. After the spider has finished
closing, the <a class="reference internal" href="#std:signal-spider_closed"><code class="xref std std-signal docutils literal"><span class="pre">spider_closed</span></code></a> signal is sent.</p>
<p>You can, for example, schedule some requests in your <a class="reference internal" href="#std:signal-spider_idle"><code class="xref std std-signal docutils literal"><span class="pre">spider_idle</span></code></a>
handler to prevent the spider from being closed.</p>
<p>This signal does not support returning deferreds from their handlers.</p>
<p>Sent when a spider callback generates an error (ie. raises an exception).</p>
<p>This signal does not support returning deferreds from their handlers.</p>
<p>Sent when the engine schedules a <a class="reference internal" href="request-response.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal"><span class="pre">Request</span></code></a>, to be
downloaded later.</p>
<p>The signal does not support returning deferreds from their handlers.</p>
<p>Sent when a <a class="reference internal" href="request-response.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal"><span class="pre">Request</span></code></a>, scheduled by the engine to be
downloaded later, is rejected by the scheduler.</p>
<p>The signal does not support returning deferreds from their handlers.</p>
<p>Sent when the engine receives a new <a class="reference internal" href="request-response.html#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal"><span class="pre">Response</span></code></a> from the
downloader.</p>
<p>This signal does not support returning deferreds from their handlers.</p>
<p>Sent by the downloader right after a <code class="docutils literal"><span class="pre">HTTPResponse</span></code> is downloaded.</p>
<p>This signal does not support returning deferreds from their handlers.</p>
<p>
        © Copyright 2008-2016, Scrapy developers.
      
        <span class="commit">
          Revision <code>5aebdac4</code>.
        </span>
</p>
<p class="caption"><span class="caption-text">First steps</span></p>
<p class="caption"><span class="caption-text">Basic concepts</span></p>
<p class="caption"><span class="caption-text">Built-in services</span></p>
<p class="caption"><span class="caption-text">Solving specific problems</span></p>
<p class="caption"><span class="caption-text">Extending Scrapy</span></p>
<p class="caption"><span class="caption-text">All the rest</span></p>
<p>Once you have scraped your items, you often want to persist or export those
items, to use the data in some other application. That is, after all, the whole
purpose of the scraping process.</p>
<p>For this purpose Scrapy provides a collection of Item Exporters for different
output formats, such as XML, CSV or JSON.</p>
<p>If you are in a hurry, and just want to use an Item Exporter to output scraped
data see the <a class="reference internal" href="feed-exports.html#topics-feed-exports"><span class="std std-ref">Feed exports</span></a>. Otherwise, if you want to know how
Item Exporters work or need more custom functionality (not covered by the
default exports), continue reading below.</p>
<p>In order to use an Item Exporter, you  must instantiate it with its required
args. Each Item Exporter requires different arguments, so check each exporter
documentation to be sure, in <a class="reference internal" href="#topics-exporters-reference"><span class="std std-ref">Built-in Item Exporters reference</span></a>. After you have
instantiated your exporter, you have to:</p>
<p>1. call the method <a class="reference internal" href="#scrapy.exporters.BaseItemExporter.start_exporting" title="scrapy.exporters.BaseItemExporter.start_exporting"><code class="xref py py-meth docutils literal"><span class="pre">start_exporting()</span></code></a> in order to
signal the beginning of the exporting process</p>
<p>2. call the <a class="reference internal" href="#scrapy.exporters.BaseItemExporter.export_item" title="scrapy.exporters.BaseItemExporter.export_item"><code class="xref py py-meth docutils literal"><span class="pre">export_item()</span></code></a> method for each item you want
to export</p>
<p>3. and finally call the <a class="reference internal" href="#scrapy.exporters.BaseItemExporter.finish_exporting" title="scrapy.exporters.BaseItemExporter.finish_exporting"><code class="xref py py-meth docutils literal"><span class="pre">finish_exporting()</span></code></a> to signal
the end of the exporting process</p>
<p>Here you can see an <a class="reference internal" href="item-pipeline.html"><span class="doc">Item Pipeline</span></a> which uses an Item
Exporter to export scraped items to different files, one per spider:</p>
<p>By default, the field values are passed unmodified to the underlying
serialization library, and the decision of how to serialize them is delegated
to each particular serialization library.</p>
<p>However, you can customize how each field value is serialized <em>before it is
passed to the serialization library</em>.</p>
<p>There are two ways to customize how a field will be serialized, which are
described next.</p>
<p>If you use <a class="reference internal" href="items.html#scrapy.item.Item" title="scrapy.item.Item"><code class="xref py py-class docutils literal"><span class="pre">Item</span></code></a> you can declare a serializer in the
<a class="reference internal" href="items.html#topics-items-fields"><span class="std std-ref">field metadata</span></a>. The serializer must be
a callable which receives a value and returns its serialized form.</p>
<p>Example:</p>
<p>You can also override the <a class="reference internal" href="#scrapy.exporters.BaseItemExporter.serialize_field" title="scrapy.exporters.BaseItemExporter.serialize_field"><code class="xref py py-meth docutils literal"><span class="pre">serialize_field()</span></code></a> method to
customize how your field value will be exported.</p>
<p>Make sure you call the base class <a class="reference internal" href="#scrapy.exporters.BaseItemExporter.serialize_field" title="scrapy.exporters.BaseItemExporter.serialize_field"><code class="xref py py-meth docutils literal"><span class="pre">serialize_field()</span></code></a> method
after your custom code.</p>
<p>Example:</p>
<p>Here is a list of the Item Exporters bundled with Scrapy. Some of them contain
output examples, which assume you’re exporting these two items:</p>
<p>This is the (abstract) base class for all Item Exporters. It provides
support for common features used by all (concrete) Item Exporters, such as
defining what fields to export, whether to export empty fields, or which
encoding to use.</p>
<p>These features can be configured through the constructor arguments which
populate their respective instance attributes: <a class="reference internal" href="#scrapy.exporters.BaseItemExporter.fields_to_export" title="scrapy.exporters.BaseItemExporter.fields_to_export"><code class="xref py py-attr docutils literal"><span class="pre">fields_to_export</span></code></a>,
<a class="reference internal" href="#scrapy.exporters.BaseItemExporter.export_empty_fields" title="scrapy.exporters.BaseItemExporter.export_empty_fields"><code class="xref py py-attr docutils literal"><span class="pre">export_empty_fields</span></code></a>, <a class="reference internal" href="#scrapy.exporters.BaseItemExporter.encoding" title="scrapy.exporters.BaseItemExporter.encoding"><code class="xref py py-attr docutils literal"><span class="pre">encoding</span></code></a>, <a class="reference internal" href="#scrapy.exporters.BaseItemExporter.indent" title="scrapy.exporters.BaseItemExporter.indent"><code class="xref py py-attr docutils literal"><span class="pre">indent</span></code></a>.</p>
<p>Exports the given item. This method must be implemented in subclasses.</p>
<p>Return the serialized value for the given field. You can override this
method (in your custom Item Exporters) if you want to control how a
particular field or value will be serialized/exported.</p>
<p>By default, this method looks for a serializer <a class="reference internal" href="#topics-exporters-serializers"><span class="std std-ref">declared in the item
field</span></a> and returns the result of applying
that serializer to the value. If no serializer is found, it returns the
value unchanged except for <code class="docutils literal"><span class="pre">unicode</span></code> values which are encoded to
<code class="docutils literal"><span class="pre">str</span></code> using the encoding declared in the <a class="reference internal" href="#scrapy.exporters.BaseItemExporter.encoding" title="scrapy.exporters.BaseItemExporter.encoding"><code class="xref py py-attr docutils literal"><span class="pre">encoding</span></code></a> attribute.</p>
<p>Signal the beginning of the exporting process. Some exporters may use
this to generate some required header (for example, the
<a class="reference internal" href="#scrapy.exporters.XmlItemExporter" title="scrapy.exporters.XmlItemExporter"><code class="xref py py-class docutils literal"><span class="pre">XmlItemExporter</span></code></a>). You must call this method before exporting any
items.</p>
<p>Signal the end of the exporting process. Some exporters may use this to
generate some required footer (for example, the
<a class="reference internal" href="#scrapy.exporters.XmlItemExporter" title="scrapy.exporters.XmlItemExporter"><code class="xref py py-class docutils literal"><span class="pre">XmlItemExporter</span></code></a>). You must always call this method after you
have no more items to export.</p>
<p>A list with the name of the fields that will be exported, or None if you
want to export all fields. Defaults to None.</p>
<p>Some exporters (like <a class="reference internal" href="#scrapy.exporters.CsvItemExporter" title="scrapy.exporters.CsvItemExporter"><code class="xref py py-class docutils literal"><span class="pre">CsvItemExporter</span></code></a>) respect the order of the
fields defined in this attribute.</p>
<p>Some exporters may require fields_to_export list in order to export the
data properly when spiders return dicts (not <code class="xref py py-class docutils literal"><span class="pre">Item</span></code> instances).</p>
<p>Whether to include empty/unpopulated item fields in the exported data.
Defaults to <code class="docutils literal"><span class="pre">False</span></code>. Some exporters (like <a class="reference internal" href="#scrapy.exporters.CsvItemExporter" title="scrapy.exporters.CsvItemExporter"><code class="xref py py-class docutils literal"><span class="pre">CsvItemExporter</span></code></a>)
ignore this attribute and always export all empty fields.</p>
<p>This option is ignored for dict items.</p>
<p>The encoding that will be used to encode unicode values. This only
affects unicode values (which are always serialized to str using this
encoding). Other value types are passed unchanged to the specific
serialization library.</p>
<p>Amount of spaces used to indent the output on each level. Defaults to <code class="docutils literal"><span class="pre">0</span></code>.</p>
<p>Exports Items in XML format to the specified file object.</p>
<p>The additional keyword arguments of this constructor are passed to the
<a class="reference internal" href="#scrapy.exporters.BaseItemExporter" title="scrapy.exporters.BaseItemExporter"><code class="xref py py-class docutils literal"><span class="pre">BaseItemExporter</span></code></a> constructor.</p>
<p>A typical output of this exporter would be:</p>
<p>Unless overridden in the <code class="xref py py-meth docutils literal"><span class="pre">serialize_field()</span></code> method, multi-valued fields are
exported by serializing each value inside a <code class="docutils literal"><span class="pre">&lt;value&gt;</span></code> element. This is for
convenience, as multi-valued fields are very common.</p>
<p>For example, the item:</p>
<p>Would be serialized as:</p>
<p>Exports Items in CSV format to the given file-like object. If the
<code class="xref py py-attr docutils literal"><span class="pre">fields_to_export</span></code> attribute is set, it will be used to define the
CSV columns and their order. The <code class="xref py py-attr docutils literal"><span class="pre">export_empty_fields</span></code> attribute has
no effect on this exporter.</p>
<p>The additional keyword arguments of this constructor are passed to the
<a class="reference internal" href="#scrapy.exporters.BaseItemExporter" title="scrapy.exporters.BaseItemExporter"><code class="xref py py-class docutils literal"><span class="pre">BaseItemExporter</span></code></a> constructor, and the leftover arguments to the
<a class="reference external" href="https://docs.python.org/2/library/csv.html#csv.writer">csv.writer</a> constructor, so you can use any <cite>csv.writer</cite> constructor
argument to customize this exporter.</p>
<p>A typical output of this exporter would be:</p>
<p>Exports Items in pickle format to the given file-like object.</p>
<p>For more information, refer to the <a class="reference external" href="https://docs.python.org/2/library/pickle.html">pickle module documentation</a>.</p>
<p>The additional keyword arguments of this constructor are passed to the
<a class="reference internal" href="#scrapy.exporters.BaseItemExporter" title="scrapy.exporters.BaseItemExporter"><code class="xref py py-class docutils literal"><span class="pre">BaseItemExporter</span></code></a> constructor.</p>
<p>Pickle isn’t a human readable format, so no output examples are provided.</p>
<p>Exports Items in pretty print format to the specified file object.</p>
<p>The additional keyword arguments of this constructor are passed to the
<a class="reference internal" href="#scrapy.exporters.BaseItemExporter" title="scrapy.exporters.BaseItemExporter"><code class="xref py py-class docutils literal"><span class="pre">BaseItemExporter</span></code></a> constructor.</p>
<p>A typical output of this exporter would be:</p>
<p>Longer lines (when present) are pretty-formatted.</p>
<p>Exports Items in JSON format to the specified file-like object, writing all
objects as a list of objects. The additional constructor arguments are
passed to the <a class="reference internal" href="#scrapy.exporters.BaseItemExporter" title="scrapy.exporters.BaseItemExporter"><code class="xref py py-class docutils literal"><span class="pre">BaseItemExporter</span></code></a> constructor, and the leftover
arguments to the <a class="reference external" href="https://docs.python.org/2/library/json.html#json.JSONEncoder">JSONEncoder</a> constructor, so you can use any
<a class="reference external" href="https://docs.python.org/2/library/json.html#json.JSONEncoder">JSONEncoder</a> constructor argument to customize this exporter.</p>
<p>A typical output of this exporter would be:</p>
<p class="first admonition-title">Warning</p>
<p class="last">JSON is very simple and flexible serialization format, but it
doesn’t scale well for large amounts of data since incremental (aka.
stream-mode) parsing is not well supported (if at all) among JSON parsers
(on any language), and most of them just parse the entire object in
memory. If you want the power and simplicity of JSON with a more
stream-friendly format, consider using <a class="reference internal" href="#scrapy.exporters.JsonLinesItemExporter" title="scrapy.exporters.JsonLinesItemExporter"><code class="xref py py-class docutils literal"><span class="pre">JsonLinesItemExporter</span></code></a>
instead, or splitting the output in multiple chunks.</p>
<p>Exports Items in JSON format to the specified file-like object, writing one
JSON-encoded item per line. The additional constructor arguments are passed
to the <a class="reference internal" href="#scrapy.exporters.BaseItemExporter" title="scrapy.exporters.BaseItemExporter"><code class="xref py py-class docutils literal"><span class="pre">BaseItemExporter</span></code></a> constructor, and the leftover arguments to
the <a class="reference external" href="https://docs.python.org/2/library/json.html#json.JSONEncoder">JSONEncoder</a> constructor, so you can use any <a class="reference external" href="https://docs.python.org/2/library/json.html#json.JSONEncoder">JSONEncoder</a>
constructor argument to customize this exporter.</p>
<p>A typical output of this exporter would be:</p>
<p>Unlike the one produced by <a class="reference internal" href="#scrapy.exporters.JsonItemExporter" title="scrapy.exporters.JsonItemExporter"><code class="xref py py-class docutils literal"><span class="pre">JsonItemExporter</span></code></a>, the format produced by
this exporter is well suited for serializing large amounts of data.</p>
<p>
        © Copyright 2008-2016, Scrapy developers.
      
        <span class="commit">
          Revision <code>5aebdac4</code>.
        </span>
</p>
<p class="caption"><span class="caption-text">First steps</span></p>
<p class="caption"><span class="caption-text">Basic concepts</span></p>
<p class="caption"><span class="caption-text">Built-in services</span></p>
<p class="caption"><span class="caption-text">Solving specific problems</span></p>
<p class="caption"><span class="caption-text">Extending Scrapy</span></p>
<p class="caption"><span class="caption-text">All the rest</span></p>
<p>Scrapy 1.4 does not bring that many breathtaking new features
but quite a few handy improvements nonetheless.</p>
<p>Scrapy now supports anonymous FTP sessions with customizable user and
password via the new <a class="reference internal" href="topics/settings.html#std:setting-FTP_USER"><code class="xref std std-setting docutils literal"><span class="pre">FTP_USER</span></code></a> and <a class="reference internal" href="topics/settings.html#std:setting-FTP_PASSWORD"><code class="xref std std-setting docutils literal"><span class="pre">FTP_PASSWORD</span></code></a> settings.
And if you’re using Twisted version 17.1.0 or above, FTP is now available
with Python 3.</p>
<p>There’s a new <a class="reference internal" href="topics/request-response.html#scrapy.http.TextResponse.follow" title="scrapy.http.TextResponse.follow"><code class="xref py py-meth docutils literal"><span class="pre">response.follow</span></code></a> method
for creating requests; <strong>it is now a recommended way to create Requests
in Scrapy spiders</strong>. This method makes it easier to write correct
spiders; <code class="docutils literal"><span class="pre">response.follow</span></code> has several advantages over creating
<code class="docutils literal"><span class="pre">scrapy.Request</span></code> objects directly:</p>
<p>For example, instead of this:</p>
<p>One can now write this:</p>
<p>Link extractors are also improved. They work similarly to what a regular
modern browser would do: leading and trailing whitespace are removed
from attributes (think <code class="docutils literal"><span class="pre">href="</span>   <span class="pre">http://example.com"</span></code>) when building
<code class="docutils literal"><span class="pre">Link</span></code> objects. This whitespace-stripping also happens for <code class="docutils literal"><span class="pre">action</span></code>
attributes with <code class="docutils literal"><span class="pre">FormRequest</span></code>.</p>
<p><strong>Please also note that link extractors do not canonicalize URLs by default
anymore.</strong> This was puzzling users every now and then, and it’s not what
browsers do in fact, so we removed that extra transformation on extractred
links.</p>
<p>For those of you wanting more control on the <code class="docutils literal"><span class="pre">Referer:</span></code> header that Scrapy
sends when following links, you can set your own <code class="docutils literal"><span class="pre">Referrer</span> <span class="pre">Policy</span></code>.
Prior to Scrapy 1.4, the default <code class="docutils literal"><span class="pre">RefererMiddleware</span></code> would simply and
blindly set it to the URL of the response that generated the HTTP request
(which could leak information on your URL seeds).
By default, Scrapy now behaves much like your regular browser does.
And this policy is fully customizable with W3C standard values
(or with something really custom of your own if you wish).
See <a class="reference internal" href="topics/spider-middleware.html#std:setting-REFERRER_POLICY"><code class="xref std std-setting docutils literal"><span class="pre">REFERRER_POLICY</span></code></a> for details.</p>
<p>To make Scrapy spiders easier to debug, Scrapy logs more stats by default
in 1.4: memory usage stats, detailed retry stats, detailed HTTP error code
stats. A similar change is that HTTP cache path is also visible in logs now.</p>
<p>Last but not least, Scrapy now has the option to make JSON and XML items
more human-readable, with newlines between items and even custom indenting
offset, using the new <a class="reference internal" href="topics/feed-exports.html#std:setting-FEED_EXPORT_INDENT"><code class="xref std std-setting docutils literal"><span class="pre">FEED_EXPORT_INDENT</span></code></a> setting.</p>
<p>Enjoy! (Or read on for the rest of changes in this release.)</p>
<p>This release comes rather soon after 1.2.2 for one main reason:
it was found out that releases since 0.18 up to 1.2.2 (included) use
some backported code from Twisted (<code class="docutils literal"><span class="pre">scrapy.xlib.tx.*</span></code>),
even if newer Twisted modules are available.
Scrapy now uses <code class="docutils literal"><span class="pre">twisted.web.client</span></code> and <code class="docutils literal"><span class="pre">twisted.internet.endpoints</span></code> directly.
(See also cleanups below.)</p>
<p>As it is a major change, we wanted to get the bug fix out quickly
while not breaking any projects using the 1.2 series.</p>
<p>Scrapy’s new requirements baseline is Debian 8 “Jessie”. It was previously
Ubuntu 12.04 Precise.
What this means in practice is that we run continuous integration tests
with these (main) packages versions at a minimum:
Twisted 14.0, pyOpenSSL 0.14, lxml 3.4.</p>
<p>Scrapy may very well work with older versions of these packages
(the code base still has switches for older Twisted versions for example)
but it is not guaranteed (because it’s not tested anymore).</p>
<p>This 1.1 release brings a lot of interesting features and bug fixes:</p>
<p>Keep reading for more details on other improvements and bug fixes.</p>
<p>We have been <a class="reference external" href="https://github.com/scrapy/scrapy/wiki/Python-3-Porting">hard at work to make Scrapy run on Python 3</a>. As a result, now
you can run spiders on Python 3.3, 3.4 and 3.5 (Twisted &gt;= 15.5 required). Some
features are still missing (and some may never be ported).</p>
<p>Almost all builtin extensions/middlewares are expected to work.
However, we are aware of some limitations in Python 3:</p>
<p>You will find a lot of new features and bugfixes in this major release.  Make
sure to check our updated <a class="reference internal" href="intro/overview.html#intro-overview"><span class="std std-ref">overview</span></a> to get a glance of
some of the changes, along with our brushed <a class="reference internal" href="intro/tutorial.html#intro-tutorial"><span class="std std-ref">tutorial</span></a>.</p>
<p>Declaring and returning Scrapy Items is no longer necessary to collect the
scraped data from your spider, you can now return explicit dictionaries
instead.</p>
<p><em>Classic version</em></p>
<p><em>New version</em></p>
<p>Last Google Summer of Code project accomplished an important redesign of the
mechanism used for populating settings, introducing explicit priorities to
override any given setting. As an extension of that goal, we included a new
level of priority for settings that act exclusively for a single spider,
allowing them to redefine project settings.</p>
<p>Start using it by defining a <a class="reference internal" href="topics/spiders.html#scrapy.spiders.Spider.custom_settings" title="scrapy.spiders.Spider.custom_settings"><code class="xref py py-attr docutils literal"><span class="pre">custom_settings</span></code></a>
class variable in your spider:</p>
<p>Read more about settings population: <a class="reference internal" href="topics/settings.html#topics-settings"><span class="std std-ref">Settings</span></a></p>
<p>Scrapy 1.0 has moved away from Twisted logging to support Python built in’s
as default logging system. We’re maintaining backward compatibility for most
of the old custom interface to call logging functions, but you’ll get
warnings to switch to the Python logging API entirely.</p>
<p><em>Old version</em></p>
<p><em>New version</em></p>
<p>Logging with spiders remains the same, but on top of the
<a class="reference internal" href="topics/spiders.html#scrapy.spiders.Spider.log" title="scrapy.spiders.Spider.log"><code class="xref py py-meth docutils literal"><span class="pre">log()</span></code></a> method you’ll have access to a custom
<a class="reference internal" href="topics/spiders.html#scrapy.spiders.Spider.logger" title="scrapy.spiders.Spider.logger"><code class="xref py py-attr docutils literal"><span class="pre">logger</span></code></a> created for the spider to issue log
events:</p>
<p>Read more in the logging documentation: <a class="reference internal" href="topics/logging.html#topics-logging"><span class="std std-ref">Logging</span></a></p>
<p>Another milestone for last Google Summer of Code was a refactoring of the
internal API, seeking a simpler and easier usage. Check new core interface
in: <a class="reference internal" href="topics/api.html#topics-api"><span class="std std-ref">Core API</span></a></p>
<p>A common situation where you will face these changes is while running Scrapy
from scripts. Here’s a quick example of how to run a Spider manually with the
new API:</p>
<p>Bear in mind this feature is still under development and its API may change
until it reaches a stable status.</p>
<p>See more examples for scripts running Scrapy: <a class="reference internal" href="topics/practices.html#topics-practices"><span class="std std-ref">Common Practices</span></a></p>
<p>There’s been a large rearrangement of modules trying to improve the general
structure of Scrapy. Main changes were separating various subpackages into
new projects and dissolving both <cite>scrapy.contrib</cite> and <cite>scrapy.contrib_exp</cite>
into top level packages. Backward compatibility was kept among internal
relocations, while importing deprecated modules expect warnings indicating
their new place.</p>
<p>Outsourced packages</p>
<p class="first admonition-title">Note</p>
<p class="last">These extensions went through some minor changes, e.g. some setting names
were changed. Please check the documentation in each new repository to
get familiar with the new usage.</p>
<p><cite>scrapy.contrib_exp</cite> and <cite>scrapy.contrib</cite> dissolutions</p>
<p>Plural renames and Modules unification</p>
<p>Class renames</p>
<p>Settings renames</p>
<p>New Features and Enhancements</p>
<p>Deprecations and Removals</p>
<p>Relocations</p>
<p>Documentation</p>
<p>Bugfixes</p>
<p>Python 3 In Progress Support</p>
<p>Tests</p>
<p>Code refactoring</p>
<p>Thanks to everyone who contribute to this release!</p>
<p>List of contributors sorted by number of commits:</p>
<p>Thanks to everyone who contribute to this release. Here is a list of
contributors sorted by number of commits:</p>
<p>Scrapy changes:</p>
<p>The numbers like #NNN reference tickets in the old issue tracker (Trac) which is no longer available.</p>
<p>The numbers like #NNN reference tickets in the old issue tracker (Trac) which is no longer available.</p>
<p>The numbers like #NNN reference tickets in the old issue tracker (Trac) which is no longer available.</p>
<p>The numbers like #NNN reference tickets in the old issue tracker (Trac) which is no longer available.</p>
<p>First release of Scrapy.</p>
<p>
        © Copyright 2008-2016, Scrapy developers.
      
        <span class="commit">
          Revision <code>5aebdac4</code>.
        </span>
</p>
<p class="caption"><span class="caption-text">First steps</span></p>
<p class="caption"><span class="caption-text">Basic concepts</span></p>
<p class="caption"><span class="caption-text">Built-in services</span></p>
<p class="caption"><span class="caption-text">Solving specific problems</span></p>
<p class="caption"><span class="caption-text">Extending Scrapy</span></p>
<p class="caption"><span class="caption-text">All the rest</span></p>
<p class="first admonition-title">Important</p>
<p class="last">Double check you are reading the most recent version of this document at
<a class="reference external" href="http://doc.scrapy.org/en/master/contributing.html">http://doc.scrapy.org/en/master/contributing.html</a></p>
<p>There are many ways to contribute to Scrapy. Here are some of them:</p>
<p class="first admonition-title">Note</p>
<p class="last">Please report security issues <strong>only</strong> to
<a class="reference external" href="mailto:scrapy-security%40googlegroups.com">scrapy-security<span>@</span>googlegroups<span>.</span>com</a>. This is a private list only open to
trusted Scrapy developers, and its archives are not public.</p>
<p>Well-written bug reports are very helpful, so keep in mind the following
guidelines when reporting a new bug.</p>
<p>The better written a patch is, the higher chance that it’ll get accepted and
the sooner that will be merged.</p>
<p>Well-written patches should:</p>
<p>The best way to submit a patch is to issue a <a class="reference external" href="https://help.github.com/send-pull-requests/">pull request</a> on GitHub,
optionally creating a new issue first.</p>
<p>Remember to explain what was fixed or the new functionality (what it is, why
it’s needed, etc). The more info you include, the easier will be for core
developers to understand and accept your patch.</p>
<p>You can also discuss the new functionality (or bug fix) before creating the
patch, but it’s always good to have a patch ready to illustrate your arguments
and show that you have put some additional thought into the subject. A good
starting point is to send a pull request on GitHub. It can be simple enough to
illustrate your idea, and leave documentation/tests for later, after the idea
has been validated and proven useful. Alternatively, you can start a
conversation in the <a class="reference external" href="http://reddit.com/r/scrapy">Scrapy subreddit</a> to discuss your idea first.
When writing GitHub pull requests, try to keep titles short but descriptive.
E.g. For bug #411: “Scrapy hangs if an exception raises in start_requests”
prefer “Fix hanging when exception occurs in start_requests (#411)”
instead of “Fix for #411”.
Complete titles make it easy to skim through the issue tracker.</p>
<p>Finally, try to keep aesthetic changes (<span class="target" id="index-0"></span><a class="pep reference external" href="https://www.python.org/dev/peps/pep-0008"><strong>PEP 8</strong></a> compliance, unused imports
removal, etc) in separate commits than functional changes. This will make pull
requests easier to review and more likely to get merged.</p>
<p>Please follow these coding conventions when writing code for inclusion in
Scrapy:</p>
<p>Tests are implemented using the <a class="reference external" href="https://twistedmatrix.com/documents/current/core/development/policy/test-standard.html">Twisted unit-testing framework</a>, running
tests requires <a class="reference external" href="https://pypi.python.org/pypi/tox">tox</a>.</p>
<p>Make sure you have a recent enough <a class="reference external" href="https://pypi.python.org/pypi/tox">tox</a> installation:</p>
<p>If your version is older than 1.7.0, please update it first:</p>
<p>To run all tests go to the root directory of Scrapy source code and run:</p>
<p>To run a specific test (say <code class="docutils literal"><span class="pre">tests/test_loader.py</span></code>) use:</p>
<p>To see coverage report install <a class="reference external" href="https://pypi.python.org/pypi/coverage">coverage</a> (<code class="docutils literal"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">coverage</span></code>) and run:</p>
<p>see output of <code class="docutils literal"><span class="pre">coverage</span> <span class="pre">--help</span></code> for more options like html or xml report.</p>
<p>All functionality (including new features and bug fixes) must include a test
case to check that it works as expected, so please include tests for your
patches if you want them to get accepted sooner.</p>
<p>Scrapy uses unit-tests, which are located in the <a class="reference external" href="https://github.com/scrapy/scrapy/tree/master/tests">tests/</a> directory.
Their module name typically resembles the full path of the module they’re
testing. For example, the item loaders code is in:</p>
<p>And their unit-tests are in:</p>
<p>
        © Copyright 2008-2016, Scrapy developers.
      
        <span class="commit">
          Revision <code>5aebdac4</code>.
        </span>
</p>
<p class="caption"><span class="caption-text">First steps</span></p>
<p class="caption"><span class="caption-text">Basic concepts</span></p>
<p class="caption"><span class="caption-text">Built-in services</span></p>
<p class="caption"><span class="caption-text">Solving specific problems</span></p>
<p class="caption"><span class="caption-text">Extending Scrapy</span></p>
<p class="caption"><span class="caption-text">All the rest</span></p>
<p>There are 3 numbers in a Scrapy version: <em>A.B.C</em></p>
<p>Backward-incompatibilities are explicitly mentioned in the <a class="reference internal" href="news.html#news"><span class="std std-ref">release notes</span></a>,
and may require special attention before upgrading.</p>
<p>Development releases do not follow 3-numbers version and are generally
released as <code class="docutils literal"><span class="pre">dev</span></code> suffixed versions, e.g. <code class="docutils literal"><span class="pre">1.3dev</span></code>.</p>
<p class="first admonition-title">Note</p>
<p>With Scrapy 0.* series, Scrapy used <a class="reference external" href="https://en.wikipedia.org/wiki/Software_versioning#Odd-numbered_versions_for_development_releases">odd-numbered versions for development releases</a>.
This is not the case anymore from Scrapy 1.0 onwards.</p>
<p class="last">Starting with Scrapy 1.0, all releases should be considered production-ready.</p>
<p>For example:</p>
<p>API stability was one of the major goals for the <em>1.0</em> release.</p>
<p>Methods or functions that start with a single dash (<code class="docutils literal"><span class="pre">_</span></code>) are private and
should never be relied as stable.</p>
<p>Also, keep in mind that stable doesn’t mean complete: stable APIs could grow
new methods or functionality but the existing methods should keep working the
same way.</p>
<p>
        © Copyright 2008-2016, Scrapy developers.
      
        <span class="commit">
          Revision <code>5aebdac4</code>.
        </span>
</p>
